# Preparación y transformación de datos

En este capítulo vamos a aprender a importar y exportar todo tipo de archivos, ademas de pasar de una base de datos no especialmente amigable, a una base de datos `tidy`, esto es, siguiendo algunas reglas bien sencillas que harán más fácil trabajar con los datos.

---  

#### Paquetes para este capítulo {-}

```{r paquetes-02}

if (!require("tidyverse")) install.packages("tidyverse"); library("tidyverse")
if (!require("readxl")) install.packages("readxl"); library("readxl")
if (!require("haven")) install.packages("haven"); library("haven")
if (!require("here")) install.packages("here"); library("here")
if (!require("readODS")) install.packages("readODS"); library("readODS")
if (!require("writexl")) install.packages("writexl"); library("writexl")
if (!require("DT")) install.packages("DT"); library("DT")
if (!require("gsheet")) install.packages("gsheet"); library("gsheet")
if (!require("janitor")) install.packages("janitor"); library("janitor")

if (!require('caret')) install.packages("caret", dependencies = c("Depends", "Suggests")); library('caret')
if (!require('FFTrees')) install.packages('FFTrees'); library('FFTrees')


```  

---  

## Importar y exportar datos


Podemos ver las funciones de esta sección y como usarlas en la [Cheatsheet importar datos](https://github.com/rstudio/cheatsheets/raw/master/data-import.pdf)  


### Importar un solo archivo

Vamos a ver con más detalle los archivos CSV (*comma separated values*). Las funciones para importar archivos excel, Libreoffice, SPSS, etc. tienen parámetros muy similares.    


#### Archivos CSV

Usaremos las siguientes funciones del paquete `readr`:    

* `readr::read_csv()`: valores separados por coma (",")  
* `readr::read_csv2()`: valores separados por punto y coma (";")  
* `readr::read_delim( , delim = "|")`: valores separados por un delimitador arbitrario  



```{r read-csv-01, eval=FALSE}

# Cargamos libreria
if (!require("readr")) install.packages("readr"); library("readr")

# Version simple
DF_name = read_csv("data/files/02-read-csv.csv")

```


```{r read-csv-02}
  
# Version avanzada
name_of_file = here::here("data", "files", "02-read-csv.csv")
DF_name = read_csv(name_of_file)

DF_name

```


#### Otros tipos de archivos

##### Archivos excel

```{r read-others-xls}

if (!require("readxl")) install.packages("readxl"); library("readxl")
name_of_file = here::here("data", "files", "02-read-xlsx.xlsx")
readxl::read_excel(name_of_file)

```


##### Archivos SPSS

```{r read-others-spss}

if (!require("haven")) install.packages("haven"); library("haven")
name_of_file = here::here("data", "files", "02-read-sav.sav")
haven::read_sav(name_of_file)

```


##### Archivos Libreoffice

```{r read-others-libreoffice}

if (!require("readODS")) install.packages("readODS"); library("readODS")
name_of_file = here::here("data", "files", "02-read-ods.ods")
df_ODS = readODS::read_ods(name_of_file)

# Vemos las primeras filas
head(df_ODS)

```

##### Google sheets

Para poder leer una gsheet debemos antes crear un enlace para compartirla: `"Share" -> "Get shareable link"`  

```{r google-sheets}

if (!require("gsheet")) install.packages("gsheet"); library("gsheet")
name_of_sheet = "1jjb91j2X13_JKDAeIwrKIdNv0rcseMdteSqb0ZMVOig/edit#gid=807114896"
gsheet::gsheet2tbl(name_of_sheet)   

```




### Ejercicios - Importar datos {.ejercicio} 


1. En el repositorio [R para preparación y visualización de datos - DNSC - UAI](https://osf.io/jdn37/) de la Open Science Foundation podrás ver una carpeta llamada `Capitulo 2`. Importa los archivos que ahí aparecen:  

+ 02-extralines-1.xlsx
+ 02-extralines-2.xlsx
+ 02-extralines-3.xlsx
+ 02-spanish.csv


```{r ejercicios-importar-datos, eval=FALSE, include=FALSE}

  read_excel("data/files/OSF_files/02-extralines-1.xlsx")
  read_excel("data/files/OSF_files/02-extralines-2.xlsx", skip = 2)
  read_excel("data/files/OSF_files/02-extralines-3.xlsx", skip = 2, sheet = 2)
  read_csv2("data/files/OSF_files/02-spanish.csv")

```





### Importar múltiples archivos

En ocasiones tenemos múltiples archivos en una carpeta (e.g. uno por participante) y queremos combinarlos todos en un solo DF.  


```{r importar-multiples-archivos-paquetes}

if (!require("purrr")) install.packages("purrr"); library("purrr")
if (!require("readr")) install.packages("readr"); library("readr")
if (!require("readxl")) install.packages("readxl"); library("readxl")

```


Importamos los archivos que están en la carpeta `data/files/02-CSVs`  


```{r importar-multiples-archivos}

# Directorio donde se encuentran los archivos
name_of_folder = here::here("data", "files", "02-CSVs")

# Listamos los archivos a leer
files <- list.files(name_of_folder, full.names = TRUE)

# Leemos todos los archivos, combinandolos en un dataframe
full <- map_df(files, read_csv)
full

```


#### Incluir nombres de archivos

Incluimos nombres de archivo en una columna:  

```{r importar-multiples-archivos-nombres}

name_of_folder = here::here("data", "files", "02-CSVs")
files <- list.files(name_of_folder, full.names = TRUE) %>% 
  set_names(basename(.))
full2 <- map_df(files, read_csv, .id = "file")
full2

```


#### Con parametros

Añadimos parametros a la funcion de lectura. En este caso, definimos el tipo de columna esperado con la función `col_types()`. Con esto nos aseguraremos que si alguno de los archivos tiene el tipo de datos "incorrecto", aparecerán warnings en la importación:  

```{r importar-multiples-archivos-parametros}

name_of_folder = here::here("data", "files", "02-CSVs")
files <- list.files(name_of_folder, full.names = TRUE)
full <- map_df(files, read_csv, 
               col_types = cols(
                 Sex = col_factor(),
                 Priming = col_character(),
                 trialN = col_integer(),
                 Block = col_character(),
                 Adjective = col_character(),
                 Valence = col_factor(),
                 Answer = col_character(),
                 Arrow = col_character(),
                 rT = col_double()))

full

```


### Ejercicios - Importar múltiples archivos {.ejercicio}


1. Cuando más arriba importamos los archivos que están en la carpeta `data/files/02-CSVs`, ¿qué archivos importamos exáctamente? ¿Ves algún problema en lo que hicimos? El resultado final deberia ser así:      


```{r ejercicios-importar-multiples-1, echo=FALSE}

# Directorio donde se encuentran los archivos
name_of_folder = here::here("data", "files", "02-CSVs")

# Listamos los archivos a leer
files <- list.files(name_of_folder, pattern = "csv", full.names = TRUE)

# Leemos todos los archivos, combinandolos en un dataframe
full <- map_df(files, read_csv)
full

```


2. Leed los archivos .xlsx de la carpeta `data/files/02-XLSs`, combinándolos en un único DF. El resultado final debería ser como se ve a continuación:  

```{r ejercicios-importar-multiples-2, echo=FALSE}

name_of_folder = here::here("data", "files", "02-XLSs")

# Listamos los archivos a leer
files <- list.files(name_of_folder, pattern = "xls", full.names = TRUE)
map_df(files, read_xlsx, sheet = 2, skip = 5)

```



### Exportar datos

#### Archivos CSV

```{r write-csv, eval = FALSE}

# Versión simple
write_csv(DF_name, "data/files/02-write-csv.csv")

# Versión avanzada
name_of_file = here::here("data", "files", "02-write-csv.csv")
write_csv(DF_name, name_of_file)


```

#### Otros Archivos

```{r write-otros, eval = FALSE}

if (!require("writexl")) install.packages("writexl"); library("writexl")
name_of_file = here::here("data", "files", "02-write-xlsx.xlsx")
writexl::write_xlsx(DF_name, name_of_file)


if (!require("haven")) install.packages("haven"); library("haven")
name_of_file = here::here("data", "files", "02-write-sav.sav")
haven::write_sav(DF_name, name_of_file)


if (!require("readODS")) install.packages("readODS"); library("readODS")
name_of_file = here::here("data", "files", "02-write-ods.ods")
readODS::write_ods(DF_name, name_of_file)

```



## Preparación y transformación de datos

Para la preparación y transformación de datos usaremos fundamentalmente `dplyr`. Hay otros paquetes [más rápidos](https://h2oai.github.io/db-benchmark/) como `data.table`. Si trabajas con datos gigantescos (millones de filas), sin duda notarás la diferencia. La desventaja es que la sintaxis es (habitualmente) algo más difícil.    


### Tidy data

Existen tres sencillas reglas que definen la *Tidy data*:

1. Cada variable tiene su columna propia
2. Cada observacion tiene su fila propia
3. Cada valor tiene su celda propia
    
    
Las ventajas fundamentales son:

* Uso de una manera consistente de trabajar, que se alinea con el tidyverse  
* Facilidad para trabajar con la logica vectorizada  

---  

Por ejemplo. De manera muy sencilla y rápida podemos crear una nueva columna realizando algún cómputo arbitrario con los valores de otra columna.  

```{r tidy_vector-1}

if (!require("tidyverse")) install.packages("tidyverse"); library("tidyverse")

# Compute rate per 100,000
table1 %>% 
  mutate(rate_per_100K = cases / population * 100000)

```

O contar el número de casos por valor de una variable.  

```{r tidy_vector-2}

# Compute cases per year
table1 %>% 
  count(year, wt = cases)

```

Y, como no, `ggplot` funciona con datos `tidy`, en formato long.  

```{r tidy_vector-3}

# Visualise changes over time
if (!require("tidyverse")) install.packages("tidyverse"); library("tidyverse")
ggplot(table1, aes(as.factor(year), cases)) + 
  geom_line(aes(group = country), colour = "grey50") + 
  geom_point(aes(colour = country))

```
    
    
### Verbos dplyr

Usaremos [{dplyr}](https://dplyr.tidyverse.org/), un paquete muy potente para la manipulación de datos. Su sintaxis, además, es bastante intuitiva (¡son verbos en inglés!). Usando pipes **%>%** (CONTROL + SHIFT + M) podemos enlazar operaciones de transformación de datos de manera muy sencilla (una vez nos aprendamos los verbos).  

Verbos esenciales:  

###### {.parameters -}    

* filter(): filtrar filas  
* arrange(): ordenar filas  
* select(): seleccionar columnas  
* rename(): renombrar columnas  
* mutate(): crear columnas, modificar columnas, etc.   


Podemos ver mas detalle y ejemplos en la [Cheatsheet de dplyr](https://github.com/rstudio/cheatsheets/raw/master/data-transformation.pdf).  


#### Tabla resumen dplyr {-}  

```{r tabla-dp, echo=FALSE}

DT::datatable(data.frame(
  tarea = c("Filtrar", "Ordenar", "Seleccionar/eliminar variables", "Renombrar variables", "Separar contenidos de variable", 
            "Extraer valores únicos", "Crear/modificar variables", "Omitir NAs", "Wide to long", "Long to wide", "Combinar bases de datos", "Recodificar valores", "Recodificar valores"),
  funcion = c("filter()", "arrange()", "select()", "rename()", "separate()", "distinct()", "mutate()", "drop_na()", "gather()", "spread()", "left_join()", "ifelse()", "case_when()"),
  ejemplo = c("datos %>% filter(Sexo == 1)", "datos %>% arrange(Sexo)", "datos %>% select(-Sexo)", "datos %>% rename(Genero = Sexo)", 
              "datos %>% separate(var_name, c('First', 'Second'), sep = '_')", "datos %>% distinct(Edad, .keep_all = T)", "datos %>% mutate(Viejo = Edad > 30)", 
              "datos %>% drop_na(Sexo)", "datos %>% gather(Condition, VD, 4:6)", "datos %>% spread(Condition, VD)", "left_join(datos1, datos2, by = 'ID')", 
              "datos %>%  mutate(Edad = ifelse(Edad > 30, 'Viejo', 'Joven'))", "datos %>%  mutate(Edad = case_when(.$Edad > 30 ~ 'Viejo', 'Joven')")))

```

#### Filtrar y ordenar filas

```{r filtrando-ordenando}

# DF original
name_of_file = here::here("data", "files", "02-read-csv.csv")
DF_name = read_csv(name_of_file)
DF_name

# Filtrar
DF_name %>% 
  filter(Educacion > 8)

# Ordenar
DF_name %>% 
  arrange(Educacion, desc(Genero))

```

#### Seleccionar, ordenar y renombrar columnas

```{r seleccionando-ordenando-columnas}

# Seleccionar columnas
DF_name %>% 
  select(Genero, Edad)

# Eliminar columnas  
DF_name %>% 
  select(-X1)

# Ordenar y eliminar columnas
DF_name %>% 
  select(ID, Edad, Genero, everything(), -X1) 


```


```{r renombrando}

# Renombrar columnas
DF_name %>% 
  rename(Identificador = ID,
         Sexo = Genero)

# Renombrar usando la posicion (DANGER!)
DF_name %>% 
  rename(Identificador = 2)

# Renombrar usando vectores
oldnames = c("ID","Genero")
newnames = c("Identificador","Sexo")
DF_name %>% rename_at(vars(oldnames), ~ newnames)

```



##### Selección avanzada con select_helpers()  

El `everything()` que usamos dentro de `select()` más arriba es uno de los `select_helpers()` existentes. Estos permiten realizar operaciones de selección de variables de manera más sencilla.  

###### select_helpers() {.parameters -}   

<!-- **tidyselect::select_helpers()**    -->

* starts_with(): Empieza con un prefijo (e.g. starts_with("))  
* ends_with(): Ends with a suffix   
* contains(): Contains a literal string  
* matches(): Matches a regular expression   
* num_range(): Matches a numerical range like x01, x02, x03  
* one_of(): Matches variable names in a character vector  
* everything(): Matches all variables  
* last_col(): Select last variable, possibly with an offset  


###### {-}  


Trabajaremos con los datos del paper [Cognitive and Socio-affective Predictors of Social Adaptation](https://osf.io/egxy5/), de Neely et al. Estos se pueden encontrar en un repositorio público de la OSF. Empezaremos con la base RAW en formato wide.    

```{r seleccion-avanzada}

  # DF original  
  df_wide = read_csv("https://raw.githubusercontent.com/gorkang/cognitive-and-socio-affective-predictors-of-social-adaptation/master/data-raw/sa-raw-anonymised.csv")  
  cat(names(df_wide))

  # Seleccionamos variables que contienen la cadena de texto "dem"
  df_wide %>% 
    select(contains("dem"))

  # Seleccionamos variables que acacan con la cadena de texto "cod"
  df_wide %>% 
    select(ID, ends_with("cod"))

  # Lo mismo, pero usando expresiones regulares
  df_wide %>% 
    select(ID, matches("cod$"))
  
```


#### Modificar y añadir variables


```{r modificar-anadir-variables}

# DF original
DF_name

# Modificar variable reemplazando valor
DF_name %>% 
  mutate(PPV_DECLARED = PPV_DECLARED/100)
  
# Añadir variable
DF_name %>% 
  mutate(PPV_DECLARED_PCT = PPV_DECLARED/100)

# Añadir variable destruyendo el resto del DF
DF_name %>% 
  transmute(PPV_DECLARED_PCT = PPV_DECLARED/100)

# Limpiar nombres
if (!require("janitor")) install.packages("janitor"); library("janitor")
DF_name %>% 
  janitor::clean_names()

```


#### Resúmenes agrupados

La combinación de verbos `group_by()` y `summarise()` es una de las más usadas. Con esta podemos calcular promedios, medianas, etc. por condición de manera sencilla.  

```{r resumenes-agrupados}

# Resumen
DF_name %>% 
  summarise(Promedio_PPV = mean(PPV_DECLARED), 
            N = n())

# Resumen agrupado
DF_name %>% 
  group_by(Genero) %>% 
  summarise(Promedio_PPV = mean(PPV_DECLARED), 
            N = n())

# Resumen agrupando por multiples variables, y calculando varias cosas  
DF_name %>% 
  group_by(Genero, condition) %>% 
  summarise(promedio_PPV = mean(PPV_DECLARED),
            mediana_PPV = median(PPV_DECLARED),
            SD = sd(PPV_DECLARED),
            N = n())

```


### Ejercicios - verbos dplyr {.ejercicio} 


1. Usando la base df_wide, haz las siguientes cosas, una a una:  

* Filtra el DF para quedarnos solo con edades entre 18 y 50 años  
* Ordena los datos por genero y edad, esta última decreciente  
* Selecciona las columnas para quedarnos solo con ID, variables demograficas, y respuestas crudas (raw)  
* Crea una nueva variable que sea niv_edu_porc, en la que calcules cual es el porcentaje de nivel educativo al que han llegado relativo al máximo de la base de datos (nivel educativo persona / nivel educativo maximo; en porcentaje)  

2. Ahora combina el resultado de todas las operaciones anteriores en un DF  

3. Calcula el promedio y desviación típica de edad para cada género  

```{r ejercicios-dplyr-1}

  df_wide = read_csv("https://raw.githubusercontent.com/gorkang/cognitive-and-socio-affective-predictors-of-social-adaptation/master/data-raw/sa-raw-anonymised.csv")  


```

```{r ejercicios-dplyr-soluciones, eval=FALSE, include=FALSE}

# 1. Usando la base df_wide, haz las siguientes cosas, una a una:  
# 
# * Filtra el DF para quedarnos solo con edades entre 18 y 50 años  
# * Ordena los datos por genero y edad, esta última decreciente  
# * Selecciona las columnas para quedarnos solo con ID, variables demograficas, y respuestas crudas (raw)  
# * Crea una nueva variable que sea niv_edu_porc, en la que calcules cual es el porcentaje de nivel educativo al que han llegado relativo al máximo de la base de datos (nivel educativo persona / nivel educativo maximo; en porcentaje)  
  
  df_wide %>% 
    filter(dem_edad >= 18 & dem_edad <= 50)
  
  df_wide %>% 
    arrange(dem_genero, desc(dem_edad))
  
  df_wide %>% 
    select(ID, contains("dem_"), contains("raw"))
  
  df_wide %>% 
    mutate(niv_edu_porc = dem_nivedu / max(dem_nivedu) * 100)


# 2. Ahora combina el resultado de todas las operaciones anteriores en un DF  

  
  df_final = df_wide %>% 
    filter(dem_edad >= 18 & dem_edad <= 50) %>%
    arrange(dem_genero, desc(dem_edad)) %>%
    select(ID, contains("dem_"), contains("raw")) %>%
    mutate(niv_edu_porc = dem_nivedu / max(dem_nivedu) * 100)


# 3. Calcula el promedio y desviación típica de edad para cada género  
  
  df_final %>%
    group_by(dem_genero) %>%
    summarize(edad_media = mean(dem_edad), SD = sd(dem_edad))

```



### Verbos avanzados y otras criaturas indómitas


#### Wide to long

Empecemos con un ejemplo muy sencillo. 3 participantes, 2 items.  

```{r wide-to-long}

# Creamos un DF
df_simple_wide = data.frame(ID = c("Participante1", "Participante2", "Participante3"),
           Item1 = c(22, 33, 44),
           Item2 = c(88, 99, 77))

df_simple_wide

# Wide to long
df_simple_long = df_simple_wide %>% 
  gather(Item, Response, Item1:Item2)

df_simple_long

```


Ahora pasemos a un ejemplo mas complejo. Tenemos las puntuaciones a los 11 items de la lipkus numeracy scale de 232 participantes, ademas de datos demográficos.  


```{r wide-to-long2}

# Leemos documento en formato WIDE
df_wide = read_csv("https://raw.githubusercontent.com/gorkang/cognitive-and-socio-affective-predictors-of-social-adaptation/master/data-raw/sa-raw-anonymised.csv") %>% 
  # Seleccionamos solo algunas de las filas
  select(ID, dem_genero, dem_edad, dem_nivedu, matches("lkns_[0-9]{2}_raw"))

df_wide  


# Wide to long
df_wide %>% 
  gather(Item, Response, lkns_01_raw:lkns_11_raw)

df_wide %>% 
  gather(Item, Response, 5:15)
  
df_long = df_wide %>% 
  gather(Item, Response, matches("lkns"))


DT::datatable(df_long)

```


#### Long to wide

Retomamos el ejemplo simple de antes:  

```{r long-to-wide}

# Long to wide simple
df_simple_long %>% spread(Item, Response)

```

Y lo mismo con el ejemplo mas complejo:  

```{r long-to-wide-2}

# Long to wide
df_long %>%  
  spread(Item, Response)

```


##### ¿Para que sirve tener los datos en formato long?  

```{r long-wide-plots}

DF_plot = df_long %>% 
  mutate(Response_num = as.numeric(Response))

DF_plot %>% 
  drop_na(Response_num) %>% 
  ggplot(aes(Item, Response_num, color = Item)) +
  geom_jitter(alpha = .5) +
  geom_violin(alpha = .4) +
  scale_y_log10() +
  coord_flip()
  
DF_plot %>% 
  filter(is.na(Response_num)) %>% 
  ggplot(aes(Item, Response, color = Item)) +
  geom_jitter(alpha = .5, height = .2) +
  facet_wrap(~ Item, scales = "free")

```


#### Separate, omit, ifelse, case_when, tipos de variables...

```{r separate-omit-ifelse-casewhen}

# Base original
name_of_file = here::here("data", "files", "02-read-csv.csv")
DF_name = read_csv(name_of_file)

# Separate
DF_name %>% 
  separate(condition, c("primer_chunk", "segundo_chunk"), sep = "_")

# Separate in rows
DF_name %>% 
  separate_rows(condition, sep = "_")

# Drop NAs
DF_name %>%
  drop_na(PPV_DECLARED)

# If else
DF_name %>%
  mutate(Genero = ifelse(Genero == 1, "Hombre", "Mujer"))

# Case when
DF_name %>%
  mutate(Genero = 
           case_when(
             Genero == 1 ~ "Hombre",
             Genero == 2 ~ "Mujer",
             TRUE ~ "Otros"))



# Unite
DF_separated = DF_name %>% 
  separate(condition, c("primer_chunk", "segundo_chunk"), sep = "_")

DF_separated %>% 
  unite(condition, c(primer_chunk, segundo_chunk), sep = "_")


# Pull
DF_name %>% pull(PPV_DECLARED) %>% mean(.)



```


#### Regular expressions

[Basic Regular Expressions Cheatsheet](https://www.rstudio.com/wp-content/uploads/2016/09/RegExCheatsheet.pdf)  

![SOURCE: https://xkcd.com/208/](`r here::here("data", "images", "regular_expressions.png")`)


```{r regular-expressions}

DF_regexp = DF_name %>% select(-X1); DF_regexp

# Regexp
DF_regexp %>% 
  mutate(condition = gsub("PPV_", "", condition)) %>% 
  mutate(condition_N = gsub(".*([[:digit:]]$)", "\\1", condition))

read_csv("https://raw.githubusercontent.com/gorkang/cognitive-and-socio-affective-predictors-of-social-adaptation/master/data-raw/sa-raw-anonymised.csv") %>% 
  # Seleccionamos solo algunas de las filas
  select(ID, dem_genero, dem_edad, dem_nivedu, matches("lkns_[0-9]{2}_raw"))

```

Una aplicación Shiny para ayudar a construir Regular Expressions:  

```{r regular-expressions-gadget, eval=FALSE}

devtools::install_github("gadenbuie/regexplain")

regexplain::regex_gadget()


```


### Ejercicios - verbos avanzados dplyr {.ejercicio} 

Trabajaremos con los datos *procesados* del paper [Cognitive and Socio-affective Predictors of Social Adaptation](https://osf.io/egxy5/), de Neely et al. Estos se pueden encontrar en un repositorio público de la OSF. Empezaremos con la base final en formato wide (archivo: `/outputs/data/sa-prepared.csv`).  

1. Cambia el orden de las variables para que ID sea la primera columna.  

2. Transforma la base a formato long.  


```{r ejercicios-dplyr-avanzado-1-2, include=FALSE}

if (!require('tidyverse')) install.packages('tidyverse'); library('tidyverse')

DF_wide = read.csv("https://raw.githubusercontent.com/gorkang/cognitive-and-socio-affective-predictors-of-social-adaptation/master/outputs/data/sa-prepared.csv") %>% 
  mutate(Anxious.Attachment = round(Anxious.Attachment, 2))


  # 1. Cambia el orden de las variables para que ID sea la primera columna.  
  
    DF_wide = DF_wide %>% select(ID, everything())
  
  
  # 2. Transforma la base a formato long (eso sí, mantén las variables demográficas en formato wide).   
  
    DF_long = DF_wide %>% gather(Variable, Value, Fluid.Intelligence:Working.Memory)
    
```


3. Crea un nuevo DF (`DF_split`) donde crees una variable llamada `social_adaptation_split` con la median split para la variable Social.Adaptation. La mitad superior se llamará `high_social_adaptation` y la mitad inferior `low_social_adaptation`. Asegúrate que no hay valores NA. El resultado final debería ser:    

```{r ejercicios-dplyr-avanzado-3, echo=FALSE}

    median_social_adaptation = DF_wide %>% pull(Social.Adaptation) %>% median(., na.rm = TRUE)
    DF_split = DF_wide %>% 
      mutate(social_adaptation_split = 
               as.factor(
                 case_when(
                   Social.Adaptation >= median_social_adaptation ~ "high_social_adaptation",
                   Social.Adaptation < median_social_adaptation ~ "low_social_adaptation",
                   TRUE ~ NA_character_))) %>% 
      select(ID, social_adaptation_split) %>% 
      drop_na(social_adaptation_split)

    DT::datatable(DF_split)
    
    # DF_split %>% summary(.)      
  
```

---


Ahora volvemos a usar con los datos brutos (`sa-raw-anonymised.csv`) del paper [Cognitive and Socio-affective Predictors of Social Adaptation](https://osf.io/egxy5/), de Neely et al.      

4. En estos datos se muestran las puntuaciones crudas (e.g. WMAT_01_raw) y ya codificadas/corregidas (WMAT_01_cod) para los ítems de varias pruebas. Con los datos de los ítems de cada prueba, necesitamos calcular el puntaje para cada participante. Empezaremos con la prueba de Matrices de WAIS (WMAT_). Extrae la suma para cada participante de los ítems WMAT_*_cod.
  
  Hay al menos dos estrategias posibles:
  
  A) Selecciona las columnas relevantes y haz la suma de columnas  
  
  B) Convierte a long, filtra para quedarte con las filas correspondientes a la prueba relevante, y haz una suma agrupada  
  
  
```{r ejercicios-dplyr-avanzado-4datos}

if (!require('tidyverse')) install.packages('tidyverse'); library('tidyverse')

df_wide_raw = read_csv("https://raw.githubusercontent.com/gorkang/cognitive-and-socio-affective-predictors-of-social-adaptation/master/data-raw/sa-raw-anonymised.csv") 

```


```{r ejercicios-dplyr-avanzado-4solucion, eval=FALSE, include=FALSE}

# Estrategia A - wide
DF_A = df_wide_raw %>% 
  select(ID, matches("WMAT_[0-9]{2}_cod")) %>% 
  transmute(ID = ID,
            WMAT_sum = rowSums(.[,2:27]))

# Estrategia B - long
DF_B = df_wide_raw %>% 
  gather(Variable, Value, WVOC_01_cod:bayes_text_quantitative_accuracy) %>% 
  filter(grepl("WMAT_[0-9]{2}_cod", Variable)) %>% 
  mutate(Value = as.numeric(Value)) %>%  
  group_by(ID) %>% 
  summarise(WMAT_sum = sum(Value, na.rm = TRUE))
   

# Estrategia B2 - long2
DF_C = df_wide_raw %>% 
  select(ID, matches("WMAT_[0-9]{2}_cod")) %>% 
  gather(Variable, Value, starts_with("W")) %>% 
  group_by(ID) %>% 
  summarise(WMAT_sum = sum(Value, na.rm = TRUE))

 all.equal(DF_A, DF_C) 
 all.equal(DF_B, DF_C) 
 
```


## Combinar bases de datos


### Bind

```{r bind}

# Importar CSVs
DF1 = read_csv(here::here("data", "files", "02-CSVs", "01.csv"))
DF2 = read_csv(here::here("data", "files", "02-CSVs", "02.csv"))


# Bind DFs añadiendo las *filas* de DF2 a DF1
DF1 %>% 
   bind_rows(DF2)


# Bind DFs añadiendo las *columnas* de DF2 a DF1
  # bind_cols renombra automaticamente los nombres de las columnas para que no haya coincidencias
DF1 %>% 
  bind_cols(DF2) 

```

### Joins

El paquete {dplyr} tiene funciones que permiten trabajar combinando, filtrando, etc. distintos dataframes. Podéis ver más detalle y algunas ilustraciones fantásticas (como la de abajo; **inner_join()**) en el capítulo [relational data de r4ds](https://r4ds.had.co.nz/relational-data.html).

![SOURCE: https://r4ds.had.co.nz/relational-data.html#mutating-joins](`r here::here("data", "images", "join-inner.png")`)  

---  


#### Tipos de Join {.parameters -}  

Estas operaciones tendrán la forma: `DF_x %>% WHATEVER_join(DF_y)`  


* **Mutating joins**:  
    + inner_join(): preserva pares de observaciones de de `DF_x` y de `DF_y` con claves iguales   
    + left_join(): preserva las observaciones de `DF_x`, añadiendo las de `DF_y` con claves iguales    
    + right_join(): preserva las observaciones de `DF_y`, añadiendo las de `DF_x` con claves iguales  
    + full_join(): preserva todas las observaciones de `DF_x` y `DF_y`, alineándolas cuando tengan claves iguales  
    
* **Filtering joins**:  
    + semi_join(): preserva solo aquellas observaciones de `DF_x` cuyas claves aparezcan en `DF_y`   
    + anti_join(): preserva solo aquellas observaciones de `DF_x` cuyas claves NO aparezcan en `DF_y`  

* **Nesting joins**:  
    + nest_join(): preserva las observaciones de `DF_x`, añadiendo las de `DF_y` con claves iguales  


#### Mutating joins  


**Importamos datos**

Tenemos los siguientes dataframes:  

* DF_IDs: Variables demográficas de participantes  
* DF_results: Resultados en variables de interés de participantes    
* DF_BAD: Grupo de participantes "selectos"  



```{r joins-data}

# Importar CSVs para los joins  
DF_IDs = read_csv(here::here("data", "files", "02-join-IDs.csv"))
DF_results = read_csv(here::here("data", "files", "02-join-results.csv"))
DF_BAD = read_csv(here::here("data", "files", "02-join-BAD.csv"))

# DT::datatable(DF_IDs)
# DT::datatable(DF_results)
# DT::datatable(DF_BAD)

```


```{r joins-mutating}

# inner_join
DF_inner_joined = DF_IDs %>% 
  inner_join(DF_results)

#nrow(DF_inner_joined)

DT::datatable(DF_inner_joined)


# left_join
DF_left_joined = DF_IDs %>% 
   left_join(DF_results)

#nrow(DF_left_joined)

DT::datatable(DF_left_joined)


# full_join
DF_full_joined = DF_IDs %>% 
   full_join(DF_results)

#nrow(DF_full_joined)

DT::datatable(DF_full_joined)

```


#### Filtering joins  


```{r joins-filtering}

# anti_join
# AVOID the people present in DF_BAD
DF_anti_joined = DF_IDs %>% 
  anti_join(DF_BAD, by = "ID") %>% 
  left_join(DF_results)

DT::datatable(DF_anti_joined)


# semi_join
# INCLUDE ONLY the people present in DF_BAD
DF_semi_joined = DF_IDs %>% 
  semi_join(DF_BAD, by = "ID") %>% 
  left_join(DF_results)

DT::datatable(DF_semi_joined)

```

#### Nesting joins  


```{r joins-nesting}

DF_nest_joined = DF_IDs %>% 
  nest_join(DF_results, by = "ID")

DT::datatable(DF_nest_joined)

```


### Ejercicios JOINS {.ejercicio}

Con los DFs de abajo, haz las siguientes operaciones:

```{r ejercicios-joins-dfs}

DF_IDs = read_csv(here::here("data", "files", "02-join-IDs2.csv"))
DF_results = read_csv(here::here("data", "files", "02-join-results.csv"))
DF_BAD = read_csv(here::here("data", "files", "02-join-BAD.csv"))

```


1. Une los datos demográficos con los resultados  

2. A la base resultante, quítale los sujetos descartados de `DF_BAD`  

3. Crea una nueva base con datos demográficos y resultados para los sujetos descartados  

4. Comprueba si el promedio para `Crystallized Intelligence` de los participantes descartados difiere de la de los no descartados  

5. Haz una gráfica donde se puedan ver las diferencias  


```{r ejercicios-joins-responses, eval=FALSE, include=FALSE}
# 1. Une los datos demográficos con los resultados  

  # Solucion 1  
  DF_joined = DF_IDs %>% 
    left_join(DF_results, by = c("Identificador" = "ID"))
  
  # Solucion 2
  DF_joined2 = DF_IDs %>% 
    rename(ID = Identificador) %>% 
    left_join(DF_results, by = "ID")
  
  # Comprobamos que son iguales
  all.equal(DF_joined, DF_joined2)

  
  
# 2. A la base resultante, quítale los sujetos descartados de `DF_BAD`  

  DF_joined_clean = DF_joined2 %>% 
    anti_join(DF_BAD, by = "ID")
  
  

# 3. Crea una nueva base con datos demográficos y resultados para los sujetos descartados  

  # Solucion 1
  DF_joined_discarded = DF_IDs %>% 
    rename(ID = Identificador) %>% 
    left_join(DF_results, by = "ID") %>% 
    semi_join(DF_BAD, by = "ID")

  # Solucion 2
  DF_joined_discarded2 = DF_BAD %>% 
    left_join(DF_results, by = "ID")
  
  # Comprobamos que son iguales
  all.equal(DF_joined_discarded, DF_joined_discarded2)

  
  
# 4. Comprueba si el promedio para `Crystallized Intelligence` de los participantes descartados difiere de la de los no descartados  
  
  # Solucion basica
  DF_joined_clean %>% 
    summarise(mean_CI = mean(`Crystallized Intelligence`, na.rm = TRUE))

  DF_joined_discarded %>% 
      summarise(mean_CI = mean(`Crystallized Intelligence`, na.rm = TRUE))
  
  # Solucion avanzada
  DF_all = DF_joined_clean %>% mutate(type = "clean") %>% 
    bind_rows(DF_joined_discarded %>% mutate(type = "discarded")) 
  
  DF_all %>% 
    group_by(type) %>% 
    summarise(mean_CI = mean(`Crystallized Intelligence`, na.rm = TRUE))
  

  
# 5. Haz una gráfica donde se puedan ver las diferencias  
  
  DF_all %>% 
    ggplot(aes(type, `Crystallized Intelligence`, color = type)) +
    geom_jitter(width = .2, height = 0) +
    stat_summary(fun.y = mean,
                 geom = "point",
                 size = 3, 
                 color = "black",
                 alpha = .5)
    
```

---  

6. En el ejercicio 3 de verbos avanzados creaste un DF llamado `DF_split` con la median split a partir de la variable `Social.Adaptation`. Uno ese DF al `DF_long` que habías creado en el ejercicio 2 de la misma sección. El DF final se vera así: 

```{r ejercicios-joins-responses-5, echo=FALSE}

    DF_final = DF_long %>% 
      right_join(DF_split, by = "ID")
  
    DT::datatable(DF_final)
    
```

7. Haz un plot donde se vea la distribución para todas las variables de resultados de los dos niveles de `social_adaptation_split`.

```{r ejercicios-joins-responses-6, echo=FALSE}
    
  DF_final %>% 
    ggplot(aes(Value, color = social_adaptation_split, fill = social_adaptation_split)) +
    geom_density(alpha = .2) +
    theme(legend.position = "bottom") +
    facet_wrap(~ Variable, scales = "free", nrow = 3)

  # DF_final %>% 
  #   ggplot(aes(Value, Variable, color = social_adaptation_split, fill = social_adaptation_split)) +
  #   ggridges::geom_density_ridges(alpha = .2) +
  #   theme(legend.position = "bottom")

```

---  

8. El plot que vimos en el tema anterior tiene el problema de que los datos de tuberculosis son en números absolutos.  Serias capaz de convertir estos a % de la población, como se ve en el plot de abajo?  

```{r ejercicios-joins-responses-7, echo=FALSE, fig.height=12, fig.width=8}

df_plot = table1 %>% 
  mutate(relative = cases/population)


plot1 = ggplot(table1, aes(as.factor(year), cases)) + 
  geom_line(aes(group = country), colour = "grey50") + 
  geom_point(aes(colour = country), size = 2) +
  hrbrthemes::scale_y_comma() +  
  labs(title = "Casos de Tuberculosis por año",
       x = "year",
       caption = "SOURCE: http://www.who.int/tb/country/data/download/en/") +
  theme(legend.position = "bottom", 
        legend.title = element_blank())


plot2 = ggplot(df_plot, aes(as.factor(year), relative)) + 
  geom_line(aes(group = country), colour = "grey50") + 
  geom_point(aes(colour = country), size = 2) +
  scale_y_continuous(labels = scales::percent) +
  labs(title = "Casos de Tuberculosis por año",
       x = "year",
       caption = "SOURCE: http://www.who.int/tb/country/data/download/en/") +
  theme(legend.position = "bottom", 
        legend.title = element_blank())

cowplot::plot_grid(plot1, plot2, rows = 2)

```

## Más allá de la manipulación 


### Fast and Frugal trees

Hay un paquete fantástico de Nathaniel Phillips llamado [{FFTrees}](https://github.com/ndphillips/FFTrees). Este permite [crear, visualizar y evaluar fast-and-frugal decision trees](http://journal.sjdm.org/17/17217/jdm17217.pdf)  
```{r mas-alla}

if (!require('tidyverse')) install.packages('tidyverse'); library('tidyverse')
if (!require('FFTrees')) install.packages('FFTrees'); library('FFTrees')

DF_wide = read.csv("https://raw.githubusercontent.com/gorkang/cognitive-and-socio-affective-predictors-of-social-adaptation/master/outputs/data/sa-prepared.csv") %>% 
  mutate(Anxious.Attachment = round(Anxious.Attachment, 2))


median_social_adaptation = DF_wide %>% pull(Social.Adaptation) %>% median(., na.rm = TRUE)
    DF_split = DF_wide %>% 
      mutate(high_social_adaptation = 
                 case_when(
                   Social.Adaptation >= median_social_adaptation ~ TRUE,
                   Social.Adaptation < median_social_adaptation ~ FALSE)) %>% 
      select(-ID, -Social.Adaptation) %>% 
      drop_na()
    
sa.fft <- FFTrees(formula = high_social_adaptation ~.,
                     data = DF_split)

# Plotear el arbol de decision
plot(sa.fft,
     main = "Social adaptation",
     decision.labels = c("Low", "High"))

# Describe el algoritmo
sa.fft$inwords
    
```

### Machine learning

Con el paquete [{caret}](http://topepo.github.io/caret/) podemos usar alguno de los [238 distintos métodos](http://topepo.github.io/caret/available-models.html) de machine learning. 

Por ejemplo, Backprop:  

```{r caret}

# ensure the results are repeatable
set.seed(7)

# load the library
if (!require('caret')) install.packages("caret", dependencies = c("Depends", "Suggests")); library('caret')
# if (!require('mlbench')) install.packages("mlbench"); library('mlbench')

DF_wide = read_csv("https://raw.githubusercontent.com/gorkang/cognitive-and-socio-affective-predictors-of-social-adaptation/master/outputs/data/sa-prepared.csv") %>% 
  janitor::clean_names() %>% as.data.frame()

DB_x = DF_wide %>% 
  mutate(high_social_adaptation = 
           as.factor(
             case_when(
               social_adaptation >= median_social_adaptation ~ "high",
               social_adaptation < median_social_adaptation ~ "low"))) %>% 
  select(-id, -social_adaptation) %>% 
  drop_na() %>% 
  select(high_social_adaptation, everything()) 


# calculate correlation matrix
correlationMatrix <- cor(DB_x[,2:ncol(DB_x)])
# print(correlationMatrix)

# find attributes that are highly corrected (ideally >0.75)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff = 0.25)
# print(highlyCorrelated)

DB_final = DB_x %>% select(highlyCorrelated) %>% select(high_social_adaptation, everything()) %>% as.data.frame()


# prepare training scheme
control <- trainControl(method="repeatedcv", number=10, repeats=3)
# train the model
model <- train(high_social_adaptation ~ ., data = DB_final, method="lvq", preProcess="scale", trControl=control)

# estimate variable importance
importance <- varImp(model, scale=FALSE)
# print(importance)

# plot importance
plot(importance)


# define the control using a random forest selection function
control <- rfeControl(functions=rfFuncs, method="cv", number=10)

# run the RFE algorithm
results <- rfe(DB_final[,2:ncol(DB_final)], DB_final[,1], sizes=c(1:ncol(DB_final)), rfeControl=control)

# summarize the results
print(results)

# list the chosen features
predictors(results)

# plot the results
plot(results, type=c("g", "o"))

```


## Datasets interesantes  

En los siguientes repositorios podréis encontrar datasets interesantes para jugar.  

* [fivethirtyeight](https://github.com/fivethirtyeight/data)  

* [Our World in Data](https://github.com/owid/owid-datasets)  

* [TidyTuesday](https://github.com/rfordatascience/tidytuesday)  


---  



## Bibliografía {.bibliografia -}

[Cheatsheets RStudio](https://github.com/rstudio/cheatsheets)

[Cheatsheet dplyr](https://github.com/rstudio/cheatsheets/raw/master/data-transformation.pdf)

[data-carpentry-week lesson_joins](https://mikoontz.github.io/data-carpentry-week/lesson_joins.html)

[R4ds - Joins](https://r4ds.had.co.nz/relational-data.html#mutating-joins)
