[["index.html", "R para preparación y visualización de datos Doctorado en Neurociencia Social y Cognición, UAI Introducción Objetivos Como empezar Bibliografía", " R para preparación y visualización de datos Doctorado en Neurociencia Social y Cognición, UAI Gorka Navarrete Introducción El objetivo de este seminario es aprender a usar R para preparar y visualizar datos, además de generar reportes reproducibles. Está pensado para alumnos de postgrado con conocimientos básicos de programación. R es un lenguaje de programación abierto, con una gran comunidad orientada al trabajo, visualización y modelado de datos en contextos científicos y técnicos. Nos introduciremos de manera práctica a R, resolviendo problemas que encontramos habitualmente durante el quehacer científico, focalizándonos en el trabajo abierto, colaborativo y reproducible. Objetivos Dar las herramientas básicas a los alumnos para que puedan trabajar de manera autónoma con R y RStudio para el proceso de importación, transformación, visualización y reporte de datos. Al finalizar el curso deberíamos ser capaces de: Importar archivos de datos, transformar los datos, crear nuevas variables. Realizar análisis de datos exploratorios, visualizar distribuciones y comparar grupos. Generar reportes reproducibles con RMarkdown. Como empezar Si ya has completado los pasos indicados en preparando nuestro sistema, puedes lanzar el siguiente código en tu ordenador para descargar los materiales del curso: if (!require(&#39;usethis&#39;)) install.packages(&#39;usethis&#39;); library(&#39;usethis&#39;) usethis::use_course(&quot;gorkang/R_preparacion_visualizacion_datos&quot;) Sigue las instrucciones que aparecen en la Consola para tener un nuevo proyecto de RStudio con todos los materiales del curso. El código anterior creará una carpeta llamada R_preparacion_visualizacion_datos-master. Dentro de esa carpeta tendrás un archivo llamado R_preparacion_visualizacion_datos.Rproj que te permitirá abrir el proyecto de RStudio del workshop. La carpeta R_preparacion_visualizacion_datos-master contiene varias cosas. Las mas importantes son: Carpeta docs: puedes abrir docs/index.html en tu navegador para ver el “libro” de este curso. Alternativamente, puedes consultar una version online del libro. Carpeta Rmd: En esa carpeta esta el código fuente de los capítulos del libro Carpeta data: Cuando usemos archivos de datos, vendrán de aquí En ocasiones encontraras una bombilla como esta: Si haces click sobre ella aparecerá una pista sobre como resolver el ejercicio. ¡No hagas click en mi sin antes haber intentado resolver el ejercicio sin ayuda! Bibliografía Bryan, J., &amp; Hester, J. What They Forgot to Teach You About R. https://whattheyforgot.org/ Wickham, H., &amp; Grolemund, G. (2016). R for data science: import, tidy, transform, visualize, and model data. O’Reilly Media, Inc. https://r4ds.had.co.nz/ Wickham, H. (2014). Advanced r. Chapman and Hall/CRC. https://adv-r.hadley.nz/ Xie, Y., Allaire, J. J., &amp; Grolemund, G. (2018). R Markdown: The Definitive Guide. CRC Press. https://bookdown.org/yihui/rmarkdown/ Yihui Xie (2018). bookdown: Authoring Books and Technical Documents with R Markdown https://bookdown.org/yihui/bookdown/markdown-syntax.html "],["preparando-sistema.html", "Preparando nuestro sistema Empezando en A-B-C Algo más sobre la instalación de paquetes Bibliografía", " Preparando nuestro sistema Empezando en A-B-C Para poder iniciar el workshop necesitamos tener R y RStudio instalados, además de algunas librerías. Para tener un sistema funcional, completa los pasos A, B y C. Si ya tienes R y Rstudio instalados (recientemente), puedes pasar directamente al paso (C). (A) Instalar R R, es un lenguaje de programación especializado en la computación estadística y visualización de datos. Es recomendable tener instalada la última versión de R. Puedes usar uno de los enlaces siguientes: Windows: Descargar e instalar R para Windows Mac: Descargar e instalar R para Mac Ubuntu Linux: más detalles en la web de R. En un terminal:     sudo apt-get install r-base (B) Instalar RStudio RStudio es un entorno integrado de desarrollo (IDE) para la programación R. Descargar e instalar RStudio. Una vez descargado e instalado, abre RStudio. Deberías ver algo parecido a lo siguiente: (C) Paquetes para el workshop Usaremos un buen numero de paquetes en el workshop. Hay algunos meta-paquetes que simplifican la instalación de múltiples paquetes (e.g. pacman, pak, …), pero en este caso vamos a usar una versión casera. Copia y pega el código de abajo y ejecútalo [tecla ENTER] en la consola de RStudio. El proceso de instalación requiere Internet y tardará un buen rato (en algunos sistemas puede ser facilmente 1 hora). if (!require(&#39;parallel&#39;)) install.packages(&#39;parallel&#39;) options(Ncpus = parallel::detectCores() - 2) list_of_packages = c(&quot;afex&quot;, &quot;caret&quot;, &quot;correlation&quot;, &quot;corrr&quot;, &quot;cowplot&quot;, &quot;dplyr&quot;, &quot;DT&quot;, &quot;esquisse&quot;, &quot;gapminder&quot;, &quot;ggplot2&quot;, &quot;ggraph&quot;, &quot;ggridges&quot;, &quot;ggthemes&quot;, &quot;gtsummary&quot;, &quot;haven&quot;, &quot;here&quot;, &quot;hexbin&quot;, &quot;inspectdf&quot;, &quot;janitor&quot;, &quot;knitr&quot;, &quot;parameters&quot;, &quot;plotly&quot;, &quot;purrr&quot;, &quot;readODS&quot;, &quot;readr&quot;, &quot;readxl&quot;, &quot;remotes&quot;, &quot;renv&quot;, &quot;rticles&quot;, &quot;see&quot;, &quot;sjPlot&quot;, &quot;stargazer&quot;, &quot;tidyr&quot;, &quot;usethis&quot;, &quot;writexl&quot;) new_packages &lt;- list_of_packages[!(list_of_packages %in% installed.packages()[,&quot;Package&quot;])] if (length(new_packages)) install.packages(new_packages, dependencies = TRUE) Otros paquetes que usaremos. if (!require(&#39;FFTrees&#39;)) remotes::install_github(&quot;ndphillips/FFTrees&quot;); library(&#39;FFTrees&#39;) # if (!require(&#39;grateful&#39;)) remotes::install_github(&quot;Pakillo/grateful&quot;); library(&#39;grateful&#39;) if (!require(&#39;papaja&#39;)) remotes::install_github(&quot;crsh/papaja&quot;); library(&#39;papaja&#39;) if (!require(&#39;regexplain&#39;)) remotes::install_github(&quot;gadenbuie/regexplain&quot;); library(&#39;regexplain&#39;) if (!require(&#39;report&#39;)) remotes::install_github(&quot;easystats/report&quot;); library(&#39;report&#39;) Algo más sobre la instalación de paquetes Los paquetes de R son una colección de funciones, datos y documentación que amplían las capacidades básicas de R. Gran parte de las funciones y paquetes que utilizaremos en este workshop se encuentran contenidas en el meta-paquete tidyverse (este es un paquete de paquetes). No lo instalamos en (C), pero si quisieras instalarlo solo tendrías que ejecutar la siguiente linea en la consola de RStudio: install.packages(&quot;tidyverse&quot;) Para instalar otro paquete diferente de “tidyverse,” remplaza su nombre entre comillas dentro de la función: install.packages(&quot;NOMBRE_DE_PAQUETE&quot;) Una vez instalado un paquete, no es necesario volver hacerlo, a menos que reinstales R. Cargar paquetes Las funciones, datos y documentación dentro de nuestros paquetes no podrán ser utilizadas hasta que se carguen en R. Una vez instalados, para cargar los paquetes se usa la función library(): library(ggplot2) En realidad las funciones también pueden ser llamadas usando su referencia absoluta ::, sin necesidad de cargarlas antes. Por ejemplo: dplyr::tibble(columna = 1). En general: nombre_paquete::nombre_de_funcion(parametros)). Todo en uno El siguiente código simplifica lo anterior. Comprueba que el paquete esta instalado; Si no se encuentra instalado, lo instala. Finalmente lo carga. if (!require(&#39;tidyverse&#39;)) install.packages(&#39;tidyverse&#39;); library(&#39;tidyverse&#39;) Para instalar múltiples paquetes, podemos repetir la linea de mas arriba tantas veces como sea necesario, o usar una versión algo mas sofisticada como el código del apartado (C): if (!require(&#39;tidyverse&#39;)) install.packages(&#39;tidyverse&#39;); library(&#39;tidyverse&#39;) if (!require(&#39;bookdown&#39;)) install.packages(&#39;bookdown&#39;); library(&#39;bookdown&#39;) Al principio de cada capítulo, verás una sección llamada Paquetes para este capítulo. Si pegas el contenido de esa sección en un script de R al empezar cada capítulo, te asegurarás de tener disponibles todas las funciones que usaremos. Instalar paquetes de Github En ocasiones querremos instalar directamente la versión en desarrollo del paquete desde Github. Para eso podemos usar la función install_github() del paquete remotes. Por ejemplo, para instalar el paquete {BayesianReasoning} desde su repositorio de Github: if (!require(&#39;remotes&#39;)) install.packages(&#39;remotes&#39;); library(&#39;remotes&#39;) remotes::install_github(&quot;gorkang/BayesianReasoning&quot;) Bibliografía Algunos de los manuales que vamos a usar para el workshop son los siguientes: Wickham, H., &amp; Grolemund, G. (2016). R for data science: import, tidy, transform, visualize, and model data. O’Reilly Media, Inc. https://r4ds.had.co.nz/ Xie, Y., Allaire, J. J., &amp; Grolemund, G. (2018). R Markdown: The Definitive Guide. CRC Press. https://bookdown.org/yihui/rmarkdown/ Bryan, J., &amp; Hester, J. What They Forgot to Teach You About R. https://whattheyforgot.org/ "],["introducción-a-r-y-visualización-de-datos.html", "Capítulo 1 Introducción a R y visualización de datos 1.1 Introducción: porque la visualización de datos es importante 1.2 Por qué R? 1.3 Visualización de datos con ggplot2 1.4 Visualización interactiva Bibliografía", " Capítulo 1 Introducción a R y visualización de datos Paquetes para este capítulo Para poder ejecutar en tu ordenador el código de los ejemplos y ejercicios de este capítulo vas a necesitar los paquetes siguientes. Te recomiendo que abras un script de R, pegues estas líneas al principio y las ejecutes (CNTRL + ENTER para ejecutar linea a linea). if (!require(&#39;cowplot&#39;)) install.packages(&#39;cowplot&#39;); library(&#39;cowplot&#39;) if (!require(&#39;dplyr&#39;)) install.packages(&#39;dplyr&#39;); library(&#39;dplyr&#39;) if (!require(&#39;esquisse&#39;)) install.packages(&#39;esquisse&#39;); library(&#39;esquisse&#39;) if (!require(&#39;gapminder&#39;)) install.packages(&#39;gapminder&#39;); library(&#39;gapminder&#39;) if (!require(&#39;gghighlight&#39;)) install.packages(&#39;gghighlight&#39;); library(&#39;gghighlight&#39;) if (!require(&#39;ggplot2&#39;)) install.packages(&#39;ggplot2&#39;); library(&#39;ggplot2&#39;) if (!require(&#39;ggthemes&#39;)) install.packages(&#39;ggthemes&#39;); library(&#39;ggthemes&#39;) if (!require(&#39;ggridges&#39;)) install.packages(&#39;ggridges&#39;); library(&#39;ggridges&#39;) if (!require(&#39;knitr&#39;)) install.packages(&#39;knitr&#39;); library(&#39;knitr&#39;) if (!require(&#39;plotly&#39;)) install.packages(&#39;plotly&#39;); library(&#39;plotly&#39;) if (!require(&#39;purrr&#39;)) install.packages(&#39;purrr&#39;); library(&#39;purrr&#39;) if (!require(&#39;readr&#39;)) install.packages(&#39;readr&#39;); library(&#39;readr&#39;) if (!require(&#39;sjPlot&#39;)) install.packages(&#39;sjPlot&#39;); library(&#39;sjPlot&#39;) if (!require(&#39;tidyr&#39;)) install.packages(&#39;tidyr&#39;); library(&#39;tidyr&#39;) 1.1 Introducción: porque la visualización de datos es importante “These 13 datasets (the Datasaurus, plus 12 others) each have the same summary statistics (x/y mean, x/y standard deviation, and Pearson’s correlation) to two decimal places, while being drastically different in appearance.” (Matejka, J., &amp; Fitzmaurice, G., 2017) SOURCE: https://www.autodeskresearch.com/publications/samestats 1.1.1 Ejemplo del mundo real: ¿cuantos temas deberia estudiar? Este ejemplo viene de un experimento que realizamos junto con Carlos Santamaría hace algún tiempo. Presentamos una tarea sobre cálculo de probabilidades a personas que estaban entrando a un examen para convertirse en trabajadores del estado. Simplificando algo, digamos que la materia para el examen eran 80 temas. No es posible estudiar con profundidad todos los temas, así que los opositores se concentraban en un subconjunto de esos temas (e.g. 30 de 80). Al empezar el examen, se seleccionaban al azar 5 de los 80 temas, y cada persona elegía uno de ellos para desarrollar. Abajo se puede ver como cambia la probabilidad de que uno de los temas estudiados aparezca dentro de los 5 seleccionados al azar. Con 30 de los 80 temas estudiados, la probabilidad de que uno de ellos salga en la prueba es del 91%. Si estudiáramos 47, subiríamos a una probabilidad del 99%. En el experimento le preguntamos a las personas por la probabilidad de que les apareciera alguno de los temas estudiados en la prueba. Comparamos las siguientes dos preguntas: ¿Cuál es la probabilidad de que salga uno de los temas que has estudiado? ¿Cuál es la probabilidad de que no salga ninguno de los temas que has estudiado? Miramos el error promedio en función de la pregunta (cuanto se han alejado de la probabilidad correcta), y vimos que nuestra manipulación había tenido un efecto considerable: Question Error_promedio SD N p (no salga ninguno) 4.016129 35.82469 31 p (salga uno) -30.741936 20.01494 31 Hay una diferencia notable entre condiciones. Pasamos de un error promedio del -30.7% a tan solo 4%, simplemente cambiando la pregunta. Hagamos un sencillo análisis de regresión para ver si la diferencia es significativa, y cuanta varianza explica nuestro modelo.   Error Predictors Estimates CI p (Intercept) 4.02 -6.41 – 14.44 0.444 Question [p (salga uno)] -34.76 -49.50 – -20.02 &lt;0.001 Observations 62 R2 / R2 adjusted 0.270 / 0.258 ## ## Shapiro-Wilk normality test ## ## data: modelo_regresion$residuals ## W = 0.96215, p-value = 0.0532 Todo es hermoso. Tenemos un efecto claramente significativo de la pregunta (y con un R2-ajustado de .258, no está nada mal), y además, nuestro modelo no incumple el supuesto de normalidad de residuos (por los pelos!). Léeme Las pruebas de normalidad son muy sensibles al n de la muestra Preparamos un plot con promedios y barras con error standard para nuestro paper. Estamos listos para escribir el paper. Preparemos la tabla con descriptivos… Question Error_promedio SD N p (no salga ninguno) 4.016129 35.82469 31 p (salga uno) -30.741936 20.01494 31 Es curioso que la desviación estandard sea mayor en el grupo con menos error promedio… Visualicemos las respuestas de todos los participantes, junto con la distribución de los datos. Como se puede apreciar en la gráfica, cuando usamos la pregunta ¿Cuál es la probabilidad de que no salga ninguno de los temas que has estudiado? no estamos reduciendo el error, sino convirtiendo una distribución de respuestas unimodal en bimodal. TLDR: La manera en la visualizamos la información determina las conclusiones a las que llegamos. En una sola gráfica: Moraleja: es importante mostrar los datos individuales y/o la distribución de los datos SOURCE: https://www.autodeskresearch.com/publications/samestats 1.2 Por qué R? R es uno de los programas para data science mas populares, especialmente usado en la academia. El numero de paquetes que ofrecen funcionalidades de todo tipo no ha dejado de crecer. En 2021 el numero de paquetes en R-cran ha superado los 19,000 (ver este buscador de paquetes), y el ritmo de crecimiento nos acerca a la singularidad… ;) SOURCE: https://gist.github.com/daroczig/3cf06d6db4be2bbe3368 Además de lo anterior, R es un programa de código abierto (algo esencial para poder hacer ciencia reproducible), con una comunidad de usuarios muy acogedora, y con un importante foco en la inclusividad. La importancia de la comunidad es difícil de apreciar. Por ejemplo, es relativamente habitual que uno abra un issue en Github pidiendo una nueva característica en un paquete, y que los creadores la implementen (e.g. correlation, gtsummary, rorcid), que uno reporte un error y lo corrijan (e.g. sjPlot, gtsummary), recibir correcciones y mejoras en tus repositorios (e.g. html2latex, 2019-Chile), o poder contribuir a repositorios de otros (e.g. jsPsych, gtsummary). Sus funciones de visualización son muy potentes (ver la r-graph-gallery para algunos ejemplos), siendo usadas como herramienta principal en medios como la BBC. SOURCE: BBC No menos importante, hay una gran cantidad de cursos, tutoriales, presentaciones y libros de una calidad excelente, con los que podemos aprender de manera autónoma. Por ejemplo: psyTeachR team at the University of Glasgow A Gentle Guide to the Grammar of Graphics with ggplot2 resulumit.com Rmd workshop R for Data Science Advanced R Con R puedes recoger datos interactivamente con shiny, preparar datos (o extraerlos de paginas web con rvest o RSelenium), visualizar datos estáticos con ggplot, animarlos con gganimate, visualizarlos con interactivamente con plotly o shiny. Puedes también analizar los datos con todas las técnicas imaginables, desde anovas con afex a modelos mixtos con lmer y/o afex, pasando por meta-análisis con metafor, SEM, Path analysis, mediación, con lavaan, análisis Bayesianos con brms o bayesfactor, y un larguísimo etc. Puedes llevar tus visualizaciones y análisis a reportes automáticos en múltiples formatos (pdf, html, docx) con Rmarkdown, crear libros como este con bookdown, páginas web con blogdown o distill, e incluso papers completamente reproducibles (preparación y análisis de datos) en formato APA con papaja. 1.2.1 Bienvenida al tidyverse El tidyverse es un conjunto de paquetes que nos permitirán hacer de manera (habitualmente) intuitiva muchas tareas de preparación y visualización de datos. 1.2.1.1 Tidyverse vs Base R Muchas de las funciones que existen en el Tidyverse tienen un equivalente en base-R (la instalación por defecto de R). El Tidyverse tiene ventajas y desventajas. La ventaja fundamental es que el código resulta (habitualmente) más fácil de leer, los nombres de las funciones son mas intuitivos, y las maneras de hacer las cosas tienen a ser consistentes. La desventaja fundamental es que incrementamos el numero de dependencias (paquetes) de nuestro código. Veamos un ejemplo extraído de aqui. La misma operación con base-R o con tidyverse: Filter rows with conditions evaluated within groups: iris flowers with maximum “Petal.Width” for each “Species” Tidyverse iris %&gt;% group_by(Species) %&gt;% filter(Petal.Width == max(Petal.Width)) Base-R # First operate in the data.frame by group (split-apply) widest_petals &lt;- by(iris, INDICES = iris$Species, FUN = function(x){ x[x$Petal.Width == max(x$Petal.Width), ] }) # Then combine the results into a data.frame do.call(rbind, widest_petals) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## setosa 5.0 3.5 1.6 0.6 setosa ## versicolor 5.9 3.2 4.8 1.8 versicolor ## virginica.101 6.3 3.3 6.0 2.5 virginica ## virginica.110 7.2 3.6 6.1 2.5 virginica ## virginica.145 6.7 3.3 5.7 2.5 virginica 1.2.2 Antes de empezar Programar es muy difícil. Todos necesitamos ayuda. Contar con una comunidad robusta con la que compartir, preguntar, contribuir, ayuda muchísimo. SOURCE: http://www.keywordbasket.com/ZWZlY3RvIGR1bm5pbmcta3J1Z2Vy/ Hay algunos recursos que son imprescindibles. Nadie sabe como los antiguos podían programar antes de la llegada de Stackoverflow: Stack overflow Google: text size ggplot Y otros recursos que resultan muy útiles: Comunidad de usuarios de Rstudio Twiter! Por ejemplo: @thomas_mock (#TidyTuesday) @dataandme @rivaquiroga @RLadiesSantiago Webs como R bloggers 1.2.3 R para visualización de datos ggplot2 es el paquete por excelencia para visualización de datos. Su potencia va asociada a un nivel de complejidad considerable, hasta el punto que hay Cheat sheets oficiales, Cheat sheets buscables, y decenas de miles de preguntas en Stack Overflow. 1.2.3.1 Primeros pasos - con training wheels Para empezar a usar ggplot sin tener que preocuparnos de su complejidad, podemos usar la función esquisse:::esquisser() del paquete esquisse. Esta nos permite usar la potencia de ggplot para explorar una base de datos de manera muy sencilla. SOURCE: https://www.williamrchase.com/slides/intro_r_anthropology_2018#93 La manera fácil (1, 2, 3), usando esquisse: # 1) Asegúrate que hemos instalado el paquete esquisse if (!require(&#39;esquisse&#39;)) install.packages(&#39;esquisse&#39;); library(&#39;esquisse&#39;) # 2) Lanza el wizard esquisser esquisse:::esquisser(iris) # 3) Crea el gráfico que quieras, exporta el código... 1.2.3.2 Aprendamos con Garrick Garrick Aden-Buie ( @grrrck) ha creado una excelente introducción a ggplot2 y la gramática de gráficos. Os recomiendo revisarla para familiarizaros con las funcionalidades de ggplot. 1.3 Visualización de datos con ggplot2 1.3.1 Primeros pasos En esta sección vamos a ver algunos de los componentes que usaremos cuando visualicemos datos. Muchos de los ejemplos que usaremos vienen de R for data science. Los ingredientes esenciales de una gráfica son: Aesthetic mappings (aes): Variables, colores, rellenos, formas, … Geoms (geom_): puntos, líneas, boxplots, … Facets (facet_): facet_wrap() y facet_grid() Transformaciones estadísticas: stat_summary, ..prop.., … SOURCE: https://skillgaze.com/2017/10/31/understanding-different-visualization-layers-of-ggplot/ En ggplot, después de indicar los datos y coordenadas (e.g. ggplot(data = mpg, aes(x = displ, y = hwy))), podemos sumar uno o varios geoms con una lógica de capas superpuestas, por ejemplo + geom_point(): 1.3.2 Aesthetic mappings En aes() vamos a indicar las variables que queremos en los ejes x e y, el color de los puntos o líneas, el relleno de las barras, la forma de los puntos, el tipo de linea, la agrupación de los datos, etc. x: x = gdpPercap y: y = lifeExp color: color = continent; color = “red”; color = “#FAA627” fill: fill = continent; fill = “red”; fill = “#FAA627” alpha: alpha = continent; alpha = 0.2 size: size = continent; size = 5 shape: shape = continent; shape = 0 ver codigo de las distintas formas linetype: linetype = continent; linetype = “dashed” group: group = continent 1.3.2.1 x-y Usando los datos de gapminder, visualizamos la relación entre gdpPercap (eje x), y lifeExp (eje y). ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp)) + geom_point() Si respetamos el orden, podemos simplificar nuestro código, evitando el data = y mapping =. En este caso, vemos de nuevo lifeExp y gdpPercap, invirtiendo los ejes. ggplot(gapminder, aes(lifeExp, gdpPercap)) + geom_point() Ejercicio Usando gapminder, ¿podrías crear un gráfico de gdp per cápita por población como éste? 1.3.2.2 Color, alpha, size Para elegir paletas de colores: colorbrewer Codigo HEX de colores # Gráfico inicial ggplot(gapminder, aes(gdpPercap, lifeExp)) + geom_point() # Color &quot;rojo&quot; para los puntos ggplot(gapminder, aes(gdpPercap, lifeExp)) + geom_point(color = &quot;red&quot;) # Color en función de la variable &#39;continent&#39; ggplot(gapminder, aes(gdpPercap, lifeExp, color = continent)) + geom_point() # Color en función de la variable &#39;continent&#39; + size ggplot(gapminder, aes(gdpPercap, lifeExp, color = continent, size = 2)) + geom_point() # Color en función de la variable &#39;continent&#39; + size + alpha ggplot(gapminder, aes(gdpPercap, lifeExp, color = continent, size = 2, alpha = .1)) + geom_point() Imagina que queremos asignar colores manualmente. ggplot(gapminder, aes(gdpPercap, lifeExp)) + geom_point(color = c(&quot;red&quot;, &quot;grey&quot;, &quot;green&quot;, &quot;purple&quot;, &quot;black&quot;)) # Error: Aesthetics must be either length 1 or the same as the data (1704): colour Tenemos que indicar que el color depende de continent, y después usar scale_color_manual() ggplot(gapminder, aes(gdpPercap, lifeExp, color = continent)) + geom_point() + scale_color_manual(values = c(&quot;red&quot;, &quot;grey&quot;, &quot;green&quot;, &quot;purple&quot;, &quot;black&quot;)) Ejercicios Usando como base el plot del ejercicio anterior (GDP x población), ¿podrías hacer lo siguiente? Colorear los puntos por continente Tamaño del punto 4 Alpha 0.5 Cada uno de los siguientes gráficos tiene un error. ¿Sabrias corregirlos? Solucion: color = continent debe ir dentro de aes() ggplot(gapminder, aes(gdpPercap, pop), color = continent) + geom_point(size = 4, alpha = .5) Solución: color = “blue” debe ir fuera de aes() ggplot(gapminder, aes(gdpPercap, pop, color = &quot;blue&quot;)) + geom_point(size = 4, alpha = .5) 1.3.2.3 Shape Códigos para las distintas formas: SOURCE: https://r4ds.had.co.nz/data-visualisation.html#aesthetic-mappings En este ejemplo usamos la variable continent para que cada asignar una forma diferente a cada uno de los continentes. ggplot(gapminder, aes(gdpPercap, lifeExp, shape = continent)) + geom_point() 1.3.2.4 Linetype Códigos para los distintos estilos de linea: SOURCE: http://sape.inf.usi.ch/quick-reference/ggplot2/linetype Podemos definir directamente el tipo de línea que queremos en geom_line(): ggplot(gapminder, aes(year, lifeExp, color = continent)) + stat_summary(fun = mean, geom = &quot;line&quot;, linetype = &quot;dashed&quot;) O que el tipo de línea dependa de una variable: ggplot(gapminder, aes(year, lifeExp, linetype = continent, color = continent)) + stat_summary(fun = mean, geom = &quot;line&quot;) 1.3.3 Geoms Una de las cosas más difíciles cuando nos enfrentamos a nuevos datos es elegir el método más efectivo para visualizarlos. Hay varios recursos interesantes sobre cómo elegir una gráfica. En esta sección veremos distintos tipos de geometría, o geoms_(). Algunos tipos de geoms Para una lista exhaustiva ver el manual de ggplot2. SOURCE: https://nbisweden.github.io/RaukR-2019/ggplot/presentation/ggplot_presentation_assets/geoms.png 1.3.3.1 geom_point y geom_jitter Si queremos un gráfico de dispersión o scatterplot, podemos usar el geom_point() ggplot(mpg, aes(displ, hwy)) + geom_point() En algunos casos, tenemos muchos puntos que se superponen. Si usamos geom_jitter() la posición de los puntos cambia levemente de manera aleatoria para evitar superposiciones. Con las propiedades ´width´ y ´height´ podemos controlar cuando desplazamiento queremos horizontal y verticalmente. ggplot(mpg, aes(displ, hwy)) + geom_jitter() 1.3.3.2 geom_smooth Podemos usar líneas de tendencia con geom_smooth(). El method por defecto es loess, pero podemos usar otras funciones (e.g. geom_smooth(method = \"lm\") para usar una regresión lineal). # Linea de tendencia (default loess) ggplot(gapminder, aes(gdpPercap, lifeExp)) + geom_point() + geom_smooth() # Usamos lm ggplot(gapminder, aes(gdpPercap, lifeExp)) + geom_point() + geom_smooth(method = &quot;lm&quot;) # Un smooth por cada clase ggplot(gapminder, aes(gdpPercap, lifeExp, color = continent)) + geom_point() + geom_smooth() # Coloreamos puntos pero mantenemos un solo smooth ggplot(gapminder, aes(gdpPercap, lifeExp)) + geom_point(aes(color = continent)) + geom_smooth() Ejercicios Usando como base el plot de la sección Shape: Colorear los puntos por continente Mostrar una línea de tendencia por continente (sin el intervalo de confianza) Que el tipo de línea cambie por continente Añadir transparencia para que las líneas destaquen Usando el df mpg, intenta crear los 6 plots que se pueden ver más abajo. Aquí tienes el plot base, para hacer mas fácil la tarea: ggplot(mpg, aes(displ, hwy)) + geom_point() + theme_grey() Además de generar uno a uno los 6 plots, serías capaz de generar la figura que se ve abajo? Esto es, un plot que incluye los 6 plots juntos. Solucion: En la sección Combinando gráficas veras un ejemplo del uso de la función cowplot::plot_grid() 1.3.3.3 geom_boxplot y geom_violin Podemos crear diagramas de cajas (boxplots) con geom_boxplot o violines con geom_violin para visualizar como cambian los datos por grupo. # Boxplot con fill ggplot(gapminder, aes(continent, lifeExp, fill = continent)) + geom_boxplot(alpha = .2) # Violins ggplot(gapminder, aes(continent, lifeExp, fill = continent)) + geom_violin(alpha = .2) # Combinamos ambos ggplot(gapminder, aes(continent, lifeExp)) + geom_boxplot(alpha = .2) + geom_violin(alpha = .2, aes(fill = continent)) 1.3.3.4 geom_histogram y geom_bar Podemos usar histogramas geom_histogram() con variables continuas. ggplot(gapminder, aes(lifeExp)) + geom_histogram() O si tenemos variables categóricas, geom_bar(). ggplot(gapminder, aes(continent, fill = continent)) + geom_bar(alpha = .6) 1.3.3.5 geom_density Para visualizar distribuciones (cuando tenemos muchos datos), podemos usar geom_density(). # Density with fill and alpha ggplot(gapminder, aes(lifeExp, fill = continent)) + geom_density(alpha = .2) Ejercicio Añadiendo un parámetro a la gráfica de arriba, podemos transformarla en las versiones de abajo. ¿Podrías hacerlo? (recuerda que poníendote encima de geom_density() y tecleando F1 puedes ver la ayuda de la función). Solucion: position = \"stack\" y position = \"fill\". 1.3.3.6 geom_density_ridges Uno de mis geoms favoritos para comparar distribuciones es geom_density_ridges: # geom_density_ridges ggplot(gapminder, aes(lifeExp, continent, fill = continent)) + ggridges::geom_density_ridges(alpha = .2) Especialmente porque podemos incluir en el mismo gráfico información sobre distribuciones y puntos individuales. # geom_density_ridges junto con raincloud points y histograma ggplot(gapminder, aes(lifeExp, continent, fill = continent)) + ggridges::geom_density_ridges(stat = &quot;binline&quot;, bins = 20, scale = 0.95, draw_baseline = FALSE) + ggridges::geom_density_ridges(jittered_points = TRUE, position = &quot;raincloud&quot;, alpha = 0.7, scale = 0.9) Ejercicios Usando como base el plot de la seccion geom_histogram(): Colorea los histogramas por continente Sabrias hacer que no se amontonen unos continentes sobre otros? Necesitarás añadir transparencia para ver todos los datos Solucion: geom_histogram(position = \"identity\", alpha = .3). Con el DF diamonds, crea el siguiente plot: Pista: Tienes que usar el geom_bar() y el parámetro fill. 1.3.4 Facets Cuando queremos separar en gráficos independientes distintas categorías dentro de nuestros datos, podemos usar facetas. Hay dos funciones para esto, facet_grid y facet_wrap. 1.3.4.1 facet_grid facet_grid(~ variable) nos devuelve una matriz simétrica de gráficas. # Plot inicial ggplot(gapminder, aes(gdpPercap, lifeExp, color = continent)) + geom_point(alpha = .2) + facet_grid(~ continent) + guides(color = &quot;none&quot;) # Cambiamos ejes ggplot(gapminder, aes(gdpPercap, lifeExp, color = continent)) + geom_point(alpha = .2) + facet_grid(continent ~ .) + guides(color = &quot;none&quot;) # Añadimos una segunda variable ggplot(gapminder, aes(gdpPercap, lifeExp, color = country)) + geom_line(alpha = .2) + facet_grid(continent ~ pop &gt; 5000000) + guides(color = &quot;none&quot;) 1.3.4.2 facet_wrap facet_wrap(~ variable) nos devuelve tantas facetas como niveles de la variable, pudiendo definir el número de filas y columnas que queremos. # Plot base ggplot(gapminder, aes(lifeExp, fill = continent)) + geom_histogram(alpha = .5) + facet_wrap( ~ continent) + guides(fill = &quot;none&quot;) Con la función gghighlight() podemos añadir una capa para facilitar la comparación de cada faceta con los datos completos. ggplot(gapminder, aes(lifeExp, fill = continent)) + geom_histogram(alpha = .5) + facet_wrap( ~ continent, nrow = 1) + guides(color = &quot;none&quot;) + gghighlight::gghighlight() 1.3.5 Transformaciones estadísticas ggplot2 nos permite hacer algunas transformaciones estadísticas al crear los gráficos. Para más detalles, ver r4ds. 1.3.5.1 stat_summary Algunas funciones que podemos usar en los gráficos min(): mínimo max(): máximo mean(): media median(): mediana sd(): desviación estandar Podemos usar funciones simples de manera directa. En este caso, mostramos un punto con la mediana, y barras que muestran el rango completo de los datos: ggplot(gapminder, aes(continent, lifeExp)) + stat_summary( fun.ymin = min, fun.ymax = max, fun = median) Si queremos usar funciones algo mas complejas, la sintaxis es algo diferente. En este caso mostramos media ± desviación estandar: ggplot(gapminder, aes(continent, lifeExp)) + stat_summary( fun.ymin = function(x) mean(x) - sd(x), fun.ymax = function(x) mean(x) + sd(x), fun = mean) 1.3.5.2 Promedios por grupo Lo interesante es que podemos añadir estas transformaciones estadísticas como una capa más en los gráficos. Así que, a este gráfico inicial… ggplot(mpg) + geom_jitter(aes(x = class, y = hwy), width = 0.2) + theme_minimal() Le podemos añadir el promedio por grupo: ggplot(mpg) + geom_jitter(aes(x = class, y = hwy), width = 0.2) + stat_summary(aes(x = class, y = hwy), fun = mean, color = &quot;red&quot;, geom = &quot;point&quot;, size = 4, alpha = .7) + theme_minimal() O cosas aún más elaboradas: # Gapminder ggplot(gapminder, aes(gdpPercap, lifeExp, color = continent)) + geom_point(alpha = .1) + stat_summary(data = gapminder %&gt;% group_by(continent) %&gt;% summarise(gdpPercap = mean(gdpPercap), lifeExp = mean(lifeExp)), fun = mean, geom = &quot;point&quot;, size = 4) Ejercicios Cuando al plot A trato de añadirle líneas para cada class, me aparece algo como lo de B. plotA = ggplot(mpg, aes(displ, hwy, color = class)) + geom_point() + theme(legend.position = &quot;bottom&quot;) plotB = ggplot(mpg, aes(displ, hwy, color = class)) + geom_point() + geom_line() + theme(legend.position = &quot;bottom&quot;) cowplot::plot_grid(plotA, plotB, labels = c(&quot;A&quot;, &quot;B&quot;)) Pero en realidad no quiero que las líneas pasen por todos los puntos, sino que muestren el promedio ¿Podrías reproducir el gráfico de abajo? Podrías crear este gráfico? Mostramos mediana ± sd para cada país, organizado por continente. 1.3.6 Personalización de gráficas Habitualmente, un vez hemos creado la gráfica, querremos personalizar varias cosas, como las escalas, colores, estilos, título, etc. 1.3.6.1 Coordenadas # Gráfico inicial ggplot(gapminder, aes(continent)) + geom_bar() # coord_flip() ggplot(gapminder, aes(continent)) + geom_bar() + coord_flip() # coord_polar() ggplot(gapminder, aes(continent)) + geom_bar() + coord_polar() 1.3.6.2 Scales # Grafico inicial ggplot(gapminder, aes(gdpPercap, lifeExp, color = continent)) + geom_point(alpha = .1) # Añadimos breaks en eje y ggplot(gapminder, aes(gdpPercap, lifeExp, color = continent)) + geom_point(alpha = .1) + scale_y_continuous(breaks = seq(0, 100, 5)) # Definimos cuantos breaks queremos ggplot(gapminder, aes(gdpPercap, lifeExp, color = continent)) + geom_point(alpha = .1) + scale_x_continuous(n.breaks = 20, guide = guide_axis(angle = 90)) + scale_y_continuous(n.breaks = 20) # Separador de miles y breaks en x ggplot(gapminder, aes(gdpPercap, lifeExp, color = continent)) + geom_point(alpha = .1) + scale_y_continuous(breaks = seq(0, 100, 5)) + scale_x_continuous(labels = scales::comma, n.breaks = 10) # Formato de $ ($M) ggplot(gapminder, aes(gdpPercap, lifeExp, color = continent)) + geom_point(alpha = .1) + scale_y_continuous(breaks = seq(0, 100, 5)) + scale_x_continuous(labels = scales::dollar_format(prefix = &quot;$&quot;, suffix = &quot;M&quot;), breaks = seq(0, 100000, 20000)) # Escala log ggplot(gapminder, aes(gdpPercap, lifeExp, color = continent)) + geom_point(alpha = .1) + scale_y_continuous(breaks = seq(0, 100, 5)) + scale_x_log10(labels = scales::dollar_format(prefix = &quot;$&quot;, suffix = &quot;M&quot;)) # Invertimos escala ggplot(gapminder, aes(gdpPercap, lifeExp, color = continent)) + geom_point(alpha = .1) + scale_y_reverse() # No mostramos el texto de los breaks de x ggplot(gapminder, aes(gdpPercap, lifeExp, color = continent)) + geom_point(alpha = .1) + scale_y_reverse() + theme(axis.text.x = element_blank()) # Porcentaje ggplot(gapminder, aes(continent, ..prop.., group = 1)) + geom_bar() + scale_y_continuous(labels = scales::percent) Usar eje secundario (derecho) para mostrar etiquetas: gapminder_last = gapminder %&gt;% group_by(continent) %&gt;% filter(year == max(year)) %&gt;% summarize(lifeExp = mean(lifeExp)) ggplot(gapminder, aes(year, lifeExp, linetype = continent, color = continent)) + stat_summary(fun = mean, geom = &quot;line&quot;) + scale_y_continuous( limits = c(0, max(gapminder$lifeExp)), expand = c(0,0), sec.axis = dup_axis( breaks = gapminder_last$lifeExp, labels = gapminder_last$continent, name = NULL)) + scale_x_continuous(expand = c(0,0)) + guides(color = &quot;none&quot;, linetype = &quot;none&quot;) Ejercicio El plot del panel (A) tiene varios problemas (los años no son enteros o factores, los casos no se muestran con un separador de miles, la leyenda esta a la derecha ocupado un espacio precioso, etc.). Trata de resolverlos e intenta llegar al resultado que se ve en el panel (B). Usa el dataframe table1 del paquete {tidyr}? Si te sobra tiempo, puedes tratar de reproducir la siguiente versión mejorada… 1.3.6.3 Colors and fill scales # Plot inicial ggplot(gapminder, aes(continent, lifeExp, fill = continent)) + geom_violin(alpha = .2) # Relleno usando paleta blues ggplot(gapminder, aes(continent, lifeExp, fill = continent)) + geom_violin(alpha = .2) + scale_fill_brewer(palette = &quot;Blues&quot;) # Color grey ggplot(iris, aes(Petal.Width, Petal.Length, color = Species)) + geom_point() + scale_color_grey(start = 0.2, end = 0.8, na.value = &quot;red&quot;) # Gradient ggplot(iris, aes(Petal.Width, Petal.Length, color = Petal.Width)) + geom_point() + scale_color_gradient(low = &quot;red&quot;, high = &quot;blue&quot;) # Gradient con un numero predefinidos de una paleta ggplot(iris, aes(Petal.Width, Petal.Length, color = Petal.Width)) + geom_point() + scale_colour_gradientn(colours = terrain.colors(3)) 1.3.6.4 Combinando gráficas Con {cowplot} podemos combinar gráficas de manera muy simple. Otro paquete muy interesante es {patchwork}. plot1 = ggplot(gapminder, aes(gdpPercap, lifeExp, color = continent)) + geom_point(alpha = .1) + scale_y_continuous(breaks = seq(0, 100, 5)) + scale_x_log10(labels = scales::dollar_format(prefix = &quot;$&quot;, suffix = &quot;M&quot;)) + theme(legend.position = &quot;top&quot;) plot2 = ggplot(gapminder, aes(continent, ..prop.., group = 1)) + geom_bar() + scale_y_continuous(labels = scales::percent) + coord_flip() cowplot::plot_grid(plot2, plot1, rel_widths = c(.3, 0.7)) O hacer cosas mas complejas como combinar un scatteplot con un par de histogramas: # Set up scatterplot scatterplot &lt;- ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) + geom_point(size = 3, alpha = 0.6) + guides(color = FALSE) + theme(plot.margin = margin()) # Define marginal histogram marginal_distribution &lt;- function(x, var, group) { ggplot(x, aes_string(x = var, fill = group)) + geom_histogram(bins = 30, alpha = 0.4, position = &quot;identity&quot;) + # geom_density(alpha = 0.4, size = 0.1) + guides(fill = FALSE) + theme_void() + theme(plot.margin = margin()) } # Set up marginal histograms x_hist &lt;- marginal_distribution(iris, &quot;Sepal.Length&quot;, &quot;Species&quot;) y_hist &lt;- marginal_distribution(iris, &quot;Sepal.Width&quot;, &quot;Species&quot;) + coord_flip() # Align histograms with scatterplot aligned_x_hist &lt;- align_plots(x_hist, scatterplot, align = &quot;v&quot;)[[1]] aligned_y_hist &lt;- align_plots(y_hist, scatterplot, align = &quot;h&quot;)[[1]] # Arrange plots cowplot::plot_grid( aligned_x_hist, NULL, scatterplot, aligned_y_hist, ncol = 2, nrow = 2, rel_heights = c(0.2, 1), rel_widths = c(1, 0.2) ) 1.3.6.5 Usando estilos Los estilos nos permiten personalizar los gráficos de manera muy sencilla, por ejemplo, usando {ggtheme}. Podéis ver un tutorial aquí. Primero creamos un gráfico sobre el que aplicaremos estilos. p &lt;- ggplot(iris, aes(Petal.Width, Petal.Length, color = Species)) + geom_point() + labs(title = &#39;A ggplot simple graph&#39;, subtitle = &#39;Simple tweaks to improve plots, or not&#39;, x = &#39;&#39;, y = &#39;&#39;, caption = &#39;https://github.com/gorkang / @gorkang&#39;) + theme_gray() # This is the default. Needed here because of the Bookdown theme p Usando el tema fivethirtyeight: p + ggthemes::scale_color_fivethirtyeight() + ggthemes::theme_fivethirtyeight() Usando el tema economist: p + ggthemes::scale_color_economist() + ggthemes::theme_economist() Ejercicios Serías capaz de reproducir este gráfico, usando el dataframe diamonds y el theme_economist? Serías capaz de reproducir este gráfico, usando el dataframe gapminder y la paleta Accent? 1.4 Visualización interactiva 1.4.0.1 Gráficas interactivas plotly::ggplotly( ggplot(gapminder %&gt;% filter(year == 2007), aes(gdpPercap, lifeExp, color = continent, size = country)) + geom_point(alpha = .3, point = 2) + scale_y_continuous(breaks = seq(0, 100, 5)) + scale_x_log10(labels = scales::dollar_format(prefix = &quot;$&quot;, suffix = &quot;M&quot;)) + theme(legend.position = &quot;none&quot;) ) 1.4.0.2 Animando gráficas if (!require(&#39;gganimate&#39;)) remotes::install_github(&#39;thomasp85/gganimate&#39;); library(&#39;gganimate&#39;) #sudo apt-get install ffmpeg p = ggplot(gapminder, aes(gdpPercap, lifeExp, size = pop, colour = country)) + geom_point(alpha = 0.7, show.legend = FALSE) + scale_colour_manual(values = country_colors) + scale_size(range = c(2, 12)) + scale_x_log10() + facet_wrap(~continent) + # Here comes the gganimate specific bits labs(title = &#39;Year: {frame_time}&#39;, x = &#39;GDP per capita&#39;, y = &#39;life expectancy&#39;) + transition_time(year) + ease_aes(&#39;linear&#39;) animate(p, renderer = ffmpeg_renderer(), height = 6, width = 10, units = &quot;in&quot;, res = 300) # anim_save(&quot;name_file.mp4&quot;, animation = last_animation()) Video Bibliografía Matejka, J., &amp; Fitzmaurice, G. (2017, May). Same stats, different graphs: Generating datasets with varied appearance and identical statistics through simulated annealing. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems (pp. 1290-1294). ACM. https://bbc.github.io/rcookbook/ https://github.com/bbc/bbplot https://github.com/dreamRs/esquisse Garrick Aden-Buie. A Gentle Guide to the Grammar of Graphics with ggplot2: https://github.com/gadenbuie/gentle-ggplot2 Michael Toth. You Need to Start Branding Your Graphs. Here’s How, with ggplot!: https://michaeltoth.me/you-need-to-start-branding-your-graphs-heres-how-with-ggplot.html Claus Wilke: https://wilkelab.org/practicalgg/ Thomas Lin Pedersen: Part 1: https://www.youtube.com/watch?v=h29g21z0a68 Part 2: https://www.youtube.com/watch?v=0m4yywqNPVY Big Book or R : https://www.bigbookofr.com/index.html "],["preparación-y-transformación-de-datos.html", "Capítulo 2 Preparación y transformación de datos 2.1 Importar y exportar datos 2.2 Preparación y transformación de datos 2.3 Combinar bases de datos 2.4 Más allá de la manipulación 2.5 Datasets interesantes Bibliografía", " Capítulo 2 Preparación y transformación de datos En este capítulo vamos a aprender a importar y exportar todo tipo de archivos, ademas de pasar de una base de datos no especialmente amigable, a una base de datos tidy, esto es, siguiendo algunas reglas bien sencillas que harán más fácil trabajar con los datos. Paquetes para este capítulo if (!require(&#39;dplyr&#39;)) install.packages(&#39;dplyr&#39;); library(&#39;dplyr&#39;) if (!require(&quot;DT&quot;)) install.packages(&quot;DT&quot;); library(&quot;DT&quot;) if (!require(&quot;janitor&quot;)) install.packages(&quot;janitor&quot;); library(&quot;janitor&quot;) if (!require(&#39;readr&#39;)) install.packages(&#39;readr&#39;); library(&#39;readr&#39;) if (!require(&quot;readxl&quot;)) install.packages(&quot;readxl&quot;); library(&quot;readxl&quot;) if (!require(&quot;haven&quot;)) install.packages(&quot;haven&quot;); library(&quot;haven&quot;) if (!require(&quot;here&quot;)) install.packages(&quot;here&quot;); library(&quot;here&quot;) if (!require(&quot;purrr&quot;)) install.packages(&quot;purrr&quot;); library(&quot;purrr&quot;) if (!require(&quot;readODS&quot;)) install.packages(&quot;readODS&quot;); library(&quot;readODS&quot;) if (!require(&quot;tidyr&quot;)) install.packages(&quot;tidyr&quot;); library(&quot;tidyr&quot;) if (!require(&quot;waldo&quot;)) install.packages(&quot;waldo&quot;); library(&quot;waldo&quot;) if (!require(&quot;writexl&quot;)) install.packages(&quot;writexl&quot;); library(&quot;writexl&quot;) if (!require(&#39;regexplain&#39;)) remotes::install_github(&quot;gadenbuie/regexplain&quot;); library(&#39;regexplain&#39;) if (!require(&#39;FFTrees&#39;)) remotes::install_github(&quot;ndphillips/FFTrees&quot;); library(&#39;FFTrees&#39;) if (!require(&#39;caret&#39;)) install.packages(&#39;caret&#39;); library(&#39;caret&#39;) 2.1 Importar y exportar datos Podemos ver las funciones de esta sección y como usarlas en la Cheatsheet importar datos 2.1.1 Importar un solo archivo Vamos a ver con más detalle los archivos CSV (comma separated values). Las funciones para importar archivos excel, Libreoffice, SPSS, etc. tienen parámetros muy similares. 2.1.1.1 Archivos CSV Usaremos las siguientes funciones del paquete readr: readr::read_csv(): valores separados por coma (“,”) readr::read_csv2(): valores separados por punto y coma (“;”) readr::read_delim( , delim = \"|\"): valores separados por un delimitador arbitrario Leemos el archivo 02-read-csv.csv de la carpeta data/files/: DF_name = read_csv(&quot;data/files/02-read-csv.csv&quot;) Si estamos usando rmarkdown, o similar, es recomendable usar here::here() para evitar problemas con los paths a los archivos. name_of_file = here::here(&quot;data/files/02-read-csv.csv&quot;) DF_name = read_csv(name_of_file) DF_name ## # A tibble: 103 x 8 ## X1 ID Genero Edad Educacion FollowUP condition PPV_DECLARED ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 4 41904 1 47 8 80 PPV_Cond1 99 ## 2 5 95041 2 21 6 90 PPV_Cond1 99 ## 3 6 74594 2 29 6 10 PPV_Cond1 99 ## 4 15 72903 2 27 7 75 PPV_Cond1 1 ## 5 16 21260 1 29 5 35 PPV_Cond1 24 ## 6 18 50315 2 28 6 14 PPV_Cond1 99 ## 7 19 21774 2 27 4 2 PPV_Cond1 99 ## 8 20 20881 2 55 6 89 PPV_Cond1 99 ## 9 21 39751 2 28 6 6 PPV_Cond1 99 ## 10 22 99384 1 46 5 0 PPV_Cond1 1 ## # … with 93 more rows 2.1.1.2 Otros tipos de archivos Archivos excel name_of_file = here::here(&quot;data/files/02-read-xlsx.xlsx&quot;) readxl::read_excel(name_of_file) Archivos SPSS name_of_file = here::here(&quot;data/files/02-read-sav.sav&quot;) haven::read_sav(name_of_file) Archivos Libreoffice name_of_file = here::here(&quot;data/files/02-read-ods.ods&quot;) df_ODS = readODS::read_ods(name_of_file) # Vemos las primeras filas head(df_ODS) Google sheets Para poder leer una gsheet debemos antes crear un enlace para compartirla: \"Share\" -&gt; \"Get shareable link\" if (!require(&quot;googlesheets4&quot;)) install.packages(&quot;googlesheets4&quot;); library(&quot;googlesheets4&quot;) name_of_sheet = &quot;1EK5Be4r3eh4QkFjfLvutPYGNhI_MBa4TyIt3rtkJk2c&quot; googlesheets4::read_sheet(name_of_sheet) Ejercicios - Importar datos En el repositorio R para preparación y visualización de datos - DNSC - UAI de la Open Science Foundation podrás ver una carpeta llamada Capitulo 2. Si no tenéis conexión a internet, podéis encontrar los archivos en data/files/OSF_files. Importa los archivos que ahí aparecen, asegurándote que los nombres de columna se leen adecuadamente: Solucion: La función read_excel() tiene parámetros como skip, que permite no leer las primeras n lineas, o sheet, con la que puedes indicar que pestaña leer. 02-extralines-1.xlsx 02-extralines-2.xlsx 02-extralines-3.xlsx 02-spanish.csv 2.1.2 Importar múltiples archivos En ocasiones tenemos múltiples archivos en una carpeta (e.g. uno por participante) y queremos combinarlos todos en un solo DF. Importamos los archivos que están en la carpeta data/files/02-CSVs # Directorio donde se encuentran los archivos name_of_folder = here::here(&quot;data/files/02-CSVs&quot;) # Listamos los archivos a leer files &lt;- list.files(name_of_folder, full.names = TRUE) # Leemos todos los archivos, combinandolos en un dataframe full &lt;- purrr::map_df(files, read_csv) full ## # A tibble: 1,600 x 9 ## Sex Priming trialN Block Adjective Valence Answer Arrow rT ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 male Collective 1 we ofensivo negative yes left 623 ## 2 male Collective 2 we resentido negative no right 1235 ## 3 male Collective 3 we ego�sta negative yes left 335 ## 4 male Collective 4 we indiscreto negative yes left 355 ## 5 male Collective 5 we sumiso negative yes left 618 ## 6 male Collective 6 we agradable positive yes left 328 ## 7 male Collective 7 we clasista negative yes left 348 ## 8 male Collective 8 we altruista positive yes left 1620 ## 9 male Collective 9 we ansioso negative yes left 346 ## 10 male Collective 10 we presumido negative yes left 778 ## # … with 1,590 more rows 2.1.2.1 Incluir nombres de archivos Incluimos nombres de archivo en una columna: name_of_folder = here::here(&quot;data/files/02-CSVs&quot;) files &lt;- list.files(name_of_folder, full.names = TRUE) %&gt;% set_names(basename(.)) full2 &lt;- map_df(files, read_csv, .id = &quot;file&quot;) full2 ## # A tibble: 1,600 x 10 ## file Sex Priming trialN Block Adjective Valence Answer Arrow rT ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 01.csv male Collective 1 we ofensivo negative yes left 623 ## 2 01.csv male Collective 2 we resentido negative no right 1235 ## 3 01.csv male Collective 3 we ego�sta negative yes left 335 ## 4 01.csv male Collective 4 we indiscreto negative yes left 355 ## 5 01.csv male Collective 5 we sumiso negative yes left 618 ## 6 01.csv male Collective 6 we agradable positive yes left 328 ## 7 01.csv male Collective 7 we clasista negative yes left 348 ## 8 01.csv male Collective 8 we altruista positive yes left 1620 ## 9 01.csv male Collective 9 we ansioso negative yes left 346 ## 10 01.csv male Collective 10 we presumido negative yes left 778 ## # … with 1,590 more rows 2.1.2.2 Con parametros Añadimos parametros a la funcion de lectura. En este caso, definimos el tipo de columna esperado con la función col_types(). Con esto nos aseguraremos que si alguno de los archivos tiene el tipo de datos “incorrecto,” aparecerán warnings en la importación: name_of_folder = here::here(&quot;data/files/02-CSVs&quot;) files &lt;- list.files(name_of_folder, full.names = TRUE) full &lt;- map_df(files, read_csv, col_types = cols( Sex = col_factor(), Priming = col_character(), trialN = col_integer(), Block = col_character(), Adjective = col_character(), Valence = col_factor(), Answer = col_character(), Arrow = col_character(), rT = col_double())) full ## # A tibble: 1,600 x 9 ## Sex Priming trialN Block Adjective Valence Answer Arrow rT ## &lt;fct&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 male Collective 1 we ofensivo negative yes left 623 ## 2 male Collective 2 we resentido negative no right 1235 ## 3 male Collective 3 we ego�sta negative yes left 335 ## 4 male Collective 4 we indiscreto negative yes left 355 ## 5 male Collective 5 we sumiso negative yes left 618 ## 6 male Collective 6 we agradable positive yes left 328 ## 7 male Collective 7 we clasista negative yes left 348 ## 8 male Collective 8 we altruista positive yes left 1620 ## 9 male Collective 9 we ansioso negative yes left 346 ## 10 male Collective 10 we presumido negative yes left 778 ## # … with 1,590 more rows Ejercicios - Importar múltiples archivos Cuando más arriba importamos los archivos que están en la carpeta data/files/02-CSVs: ¿Qué archivos importamos exáctamente? ¿Ves algún problema en lo que hicimos? Revisa el número de filas y la variable files. El resultado final deberia ser así: ## # A tibble: 1,200 x 9 ## Sex Priming trialN Block Adjective Valence Answer Arrow rT ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 male Collective 1 we ofensivo negative yes left 623 ## 2 male Collective 2 we resentido negative no right 1235 ## 3 male Collective 3 we ego�sta negative yes left 335 ## 4 male Collective 4 we indiscreto negative yes left 355 ## 5 male Collective 5 we sumiso negative yes left 618 ## 6 male Collective 6 we agradable positive yes left 328 ## 7 male Collective 7 we clasista negative yes left 348 ## 8 male Collective 8 we altruista positive yes left 1620 ## 9 male Collective 9 we ansioso negative yes left 346 ## 10 male Collective 10 we presumido negative yes left 778 ## # … with 1,190 more rows Leed los archivos .xlsx de la carpeta data/files/02-XLSs, combinándolos en un único DF. El resultado final debería ser como se ve a continuación: ## # A tibble: 1,200 x 9 ## Sex Priming trialN Block Adjective Valence Answer Arrow rT ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 male Collective 1 we ofensivo negative yes left 623 ## 2 male Collective 2 we resentido negative no right 1235 ## 3 male Collective 3 we ego�sta negative yes left 335 ## 4 male Collective 4 we indiscreto negative yes left 355 ## 5 male Collective 5 we sumiso negative yes left 618 ## 6 male Collective 6 we agradable positive yes left 328 ## 7 male Collective 7 we clasista negative yes left 348 ## 8 male Collective 8 we altruista positive yes left 1620 ## 9 male Collective 9 we ansioso negative yes left 346 ## 10 male Collective 10 we presumido negative yes left 778 ## # … with 1,190 more rows 2.1.3 Exportar datos 2.1.3.1 Archivos CSV # Versión simple write_csv(DF_name, &quot;data/files/02-write-csv.csv&quot;) # Versión avanzada name_of_file = here::here(&quot;data/files/02-write-csv.csv&quot;) write_csv(DF_name, name_of_file) 2.1.3.2 Otros Archivos name_of_file = here::here(&quot;data/files/02-write-xlsx.xlsx&quot;) writexl::write_xlsx(DF_name, name_of_file) name_of_file = here::here(&quot;data/files/02-write-sav.sav&quot;) haven::write_sav(DF_name, name_of_file) name_of_file = here::here(&quot;data/files/02-write-ods.ods&quot;) readODS::write_ods(DF_name, name_of_file) 2.2 Preparación y transformación de datos Para la preparación y transformación de datos usaremos fundamentalmente dplyr. Hay otros paquetes más rápidos como data.table. Si trabajas con datos gigantescos (millones de filas), sin duda notarás la diferencia. La desventaja es que la sintaxis es (habitualmente) algo más difícil. 2.2.1 Tidy data Existen tres sencillas reglas que definen la Tidy data: Cada variable tiene su columna propia Cada observacion tiene su fila propia Cada valor tiene su celda propia Las ventajas fundamentales son: Uso de una manera consistente de trabajar, que se alinea con el tidyverse Facilidad para trabajar con la logica vectorizada Por ejemplo. De manera muy sencilla y rápida podemos crear una nueva columna realizando algún cómputo arbitrario con los valores de otra columna. # Compute rate per 100,000 table1 %&gt;% mutate(rate_per_100K = cases / population * 100000) ## # A tibble: 6 x 5 ## country year cases population rate_per_100K ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan 1999 745 19987071 3.73 ## 2 Afghanistan 2000 2666 20595360 12.9 ## 3 Brazil 1999 37737 172006362 21.9 ## 4 Brazil 2000 80488 174504898 46.1 ## 5 China 1999 212258 1272915272 16.7 ## 6 China 2000 213766 1280428583 16.7 O contar el número de casos por valor de una variable. # Compute cases per year table1 %&gt;% count(year, wt = cases) ## # A tibble: 2 x 2 ## year n ## &lt;int&gt; &lt;int&gt; ## 1 1999 250740 ## 2 2000 296920 Y, como no, ggplot funciona con datos tidy, en formato long. # Visualise changes over time ggplot(table1, aes(as.factor(year), cases)) + geom_line(aes(group = country), colour = &quot;grey50&quot;) + geom_point(aes(colour = country)) 2.2.2 Verbos dplyr Usaremos {dplyr}, un paquete muy potente para la manipulación de datos. Su sintaxis, además, es bastante intuitiva (¡son verbos en inglés!). Usando pipes %&gt;% (CONTROL + SHIFT + M) podemos enlazar operaciones de transformación de datos de manera muy sencilla (una vez nos aprendamos los verbos). Podemos ver más detalle y ejemplos en la Cheatsheet de dplyr. Verbos esenciales: filter(): filtrar filas arrange(): ordenar filas select(): seleccionar columnas rename(): renombrar columnas mutate(): crear columnas, modificar columnas, etc. Tabla resumen dplyr 2.2.2.1 Filtrar y ordenar filas # DF original name_of_file = here::here(&quot;data/files/02-read-csv.csv&quot;) DF_name = read_csv(name_of_file) DF_name ## # A tibble: 103 x 8 ## X1 ID Genero Edad Educacion FollowUP condition PPV_DECLARED ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 4 41904 1 47 8 80 PPV_Cond1 99 ## 2 5 95041 2 21 6 90 PPV_Cond1 99 ## 3 6 74594 2 29 6 10 PPV_Cond1 99 ## 4 15 72903 2 27 7 75 PPV_Cond1 1 ## 5 16 21260 1 29 5 35 PPV_Cond1 24 ## 6 18 50315 2 28 6 14 PPV_Cond1 99 ## 7 19 21774 2 27 4 2 PPV_Cond1 99 ## 8 20 20881 2 55 6 89 PPV_Cond1 99 ## 9 21 39751 2 28 6 6 PPV_Cond1 99 ## 10 22 99384 1 46 5 0 PPV_Cond1 1 ## # … with 93 more rows # Filtrar DF_name %&gt;% filter(Educacion &gt; 8) ## # A tibble: 3 x 8 ## X1 ID Genero Edad Educacion FollowUP condition PPV_DECLARED ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 157 12207 1 26 9 57 PPV_Cond2 45 ## 2 287 60873 1 72 10 51 PPV_Cond3 99 ## 3 381 64486 2 19 9 80 PPV_Cond4 92 # Ordenar DF_name %&gt;% arrange(Educacion, desc(Genero)) ## # A tibble: 103 x 8 ## X1 ID Genero Edad Educacion FollowUP condition PPV_DECLARED ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 350 20439 2 41 1 81 PPV_Cond4 92 ## 2 399 81379 1 36 1 90 PPV_Cond4 92 ## 3 42 20361 2 37 2 60 PPV_Cond1 1 ## 4 364 19201 2 21 2 67 PPV_Cond4 10 ## 5 412 60292 1 28 2 90 PPV_Cond4 80 ## 6 44 92735 2 30 3 95 PPV_Cond1 99 ## 7 135 32344 2 34 3 81 PPV_Cond2 46 ## 8 299 33562 2 35 3 95 PPV_Cond3 99 ## 9 333 29837 2 28 3 80 PPV_Cond4 60 ## 10 361 57804 2 40 3 30 PPV_Cond4 90 ## # … with 93 more rows 2.2.2.2 Seleccionar, ordenar y renombrar columnas # Seleccionar columnas DF_name %&gt;% select(Genero, Edad) ## # A tibble: 103 x 2 ## Genero Edad ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 47 ## 2 2 21 ## 3 2 29 ## 4 2 27 ## 5 1 29 ## 6 2 28 ## 7 2 27 ## 8 2 55 ## 9 2 28 ## 10 1 46 ## # … with 93 more rows # Eliminar columnas DF_name %&gt;% select(-X1) ## # A tibble: 103 x 7 ## ID Genero Edad Educacion FollowUP condition PPV_DECLARED ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 41904 1 47 8 80 PPV_Cond1 99 ## 2 95041 2 21 6 90 PPV_Cond1 99 ## 3 74594 2 29 6 10 PPV_Cond1 99 ## 4 72903 2 27 7 75 PPV_Cond1 1 ## 5 21260 1 29 5 35 PPV_Cond1 24 ## 6 50315 2 28 6 14 PPV_Cond1 99 ## 7 21774 2 27 4 2 PPV_Cond1 99 ## 8 20881 2 55 6 89 PPV_Cond1 99 ## 9 39751 2 28 6 6 PPV_Cond1 99 ## 10 99384 1 46 5 0 PPV_Cond1 1 ## # … with 93 more rows # Ordenar y eliminar columnas DF_name %&gt;% select(ID, Edad, Genero, everything(), -X1) ## # A tibble: 103 x 7 ## ID Edad Genero Educacion FollowUP condition PPV_DECLARED ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 41904 47 1 8 80 PPV_Cond1 99 ## 2 95041 21 2 6 90 PPV_Cond1 99 ## 3 74594 29 2 6 10 PPV_Cond1 99 ## 4 72903 27 2 7 75 PPV_Cond1 1 ## 5 21260 29 1 5 35 PPV_Cond1 24 ## 6 50315 28 2 6 14 PPV_Cond1 99 ## 7 21774 27 2 4 2 PPV_Cond1 99 ## 8 20881 55 2 6 89 PPV_Cond1 99 ## 9 39751 28 2 6 6 PPV_Cond1 99 ## 10 99384 46 1 5 0 PPV_Cond1 1 ## # … with 93 more rows # Renombrar columnas DF_name %&gt;% rename(Identificador = ID, Sexo = Genero) ## # A tibble: 103 x 8 ## X1 Identificador Sexo Edad Educacion FollowUP condition PPV_DECLARED ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 4 41904 1 47 8 80 PPV_Cond1 99 ## 2 5 95041 2 21 6 90 PPV_Cond1 99 ## 3 6 74594 2 29 6 10 PPV_Cond1 99 ## 4 15 72903 2 27 7 75 PPV_Cond1 1 ## 5 16 21260 1 29 5 35 PPV_Cond1 24 ## 6 18 50315 2 28 6 14 PPV_Cond1 99 ## 7 19 21774 2 27 4 2 PPV_Cond1 99 ## 8 20 20881 2 55 6 89 PPV_Cond1 99 ## 9 21 39751 2 28 6 6 PPV_Cond1 99 ## 10 22 99384 1 46 5 0 PPV_Cond1 1 ## # … with 93 more rows # Renombrar usando la posicion (DANGER!) DF_name %&gt;% rename(Identificador = 2) ## # A tibble: 103 x 8 ## X1 Identificador Genero Edad Educacion FollowUP condition PPV_DECLARED ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 4 41904 1 47 8 80 PPV_Cond1 99 ## 2 5 95041 2 21 6 90 PPV_Cond1 99 ## 3 6 74594 2 29 6 10 PPV_Cond1 99 ## 4 15 72903 2 27 7 75 PPV_Cond1 1 ## 5 16 21260 1 29 5 35 PPV_Cond1 24 ## 6 18 50315 2 28 6 14 PPV_Cond1 99 ## 7 19 21774 2 27 4 2 PPV_Cond1 99 ## 8 20 20881 2 55 6 89 PPV_Cond1 99 ## 9 21 39751 2 28 6 6 PPV_Cond1 99 ## 10 22 99384 1 46 5 0 PPV_Cond1 1 ## # … with 93 more rows # Renombrar usando vectores oldnames = c(&quot;ID&quot;,&quot;Genero&quot;) newnames = c(&quot;Identificador&quot;,&quot;Sexo&quot;) DF_name %&gt;% rename_at(all_of(oldnames), ~ newnames) ## # A tibble: 103 x 8 ## X1 Identificador Sexo Edad Educacion FollowUP condition PPV_DECLARED ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 4 41904 1 47 8 80 PPV_Cond1 99 ## 2 5 95041 2 21 6 90 PPV_Cond1 99 ## 3 6 74594 2 29 6 10 PPV_Cond1 99 ## 4 15 72903 2 27 7 75 PPV_Cond1 1 ## 5 16 21260 1 29 5 35 PPV_Cond1 24 ## 6 18 50315 2 28 6 14 PPV_Cond1 99 ## 7 19 21774 2 27 4 2 PPV_Cond1 99 ## 8 20 20881 2 55 6 89 PPV_Cond1 99 ## 9 21 39751 2 28 6 6 PPV_Cond1 99 ## 10 22 99384 1 46 5 0 PPV_Cond1 1 ## # … with 93 more rows 2.2.2.3 Selección avanzada con select_helpers() El everything() que usamos dentro de select() más arriba es uno de los select_helpers() existentes. Estos permiten realizar operaciones de selección de variables de manera más sencilla. select_helpers() starts_with(): Empieza con un prefijo (e.g. starts_with(\")) ends_with(): Ends with a suffix contains(): Contains a literal string matches(): Matches a regular expression num_range(): Matches a numerical range like x01, x02, x03 one_of(): Matches variable names in a character vector everything(): Matches all variables last_col(): Select last variable, possibly with an offset Trabajaremos con los datos del paper Cognitive and Socio-affective Predictors of Social Adaptation, de Neely et al. Estos se pueden encontrar en un repositorio público de la OSF. Empezaremos con la base RAW en formato wide. # DF original df_wide = read_csv(&quot;https://raw.githubusercontent.com/gorkang/cognitive-and-socio-affective-predictors-of-social-adaptation/master/data-raw/sa-raw-anonymised.csv&quot;) cat(names(df_wide)) ## ID dem_genero dem_edad dem_nivedu WVOC_01_cod WVOC_02_cod WVOC_03_cod WVOC_04_cod WVOC_05_cod WVOC_06_cod WVOC_07_cod WVOC_08_cod WVOC_09_cod WVOC_10_cod WVOC_11_cod WVOC_12_cod WVOC_13_cod WVOC_14_cod WVOC_15_cod WVOC_16_cod WVOC_17_cod WVOC_18_cod WVOC_19_cod WVOC_20_cod WVOC_21_cod WVOC_22_cod WVOC_23_cod WVOC_24_cod WVOC_25_cod WVOC_26_cod WVOC_27_cod WVOC_28_cod WVOC_29_cod WVOC_30_cod WVOC_31_cod WVOC_32_cod WVOC_33_cod WVOC_TOTAL WVOC_TOTAL_STD WMAT_01_cod WMAT_01_raw WMAT_02_cod WMAT_02_raw WMAT_03_cod WMAT_03_raw WMAT_04_cod WMAT_04_raw WMAT_05_cod WMAT_05_raw WMAT_06_cod WMAT_06_raw WMAT_07_cod WMAT_07_raw WMAT_08_cod WMAT_08_raw WMAT_09_cod WMAT_09_raw WMAT_10_cod WMAT_10_raw WMAT_11_cod WMAT_11_raw WMAT_12_cod WMAT_12_raw WMAT_13_cod WMAT_13_raw WMAT_14_cod WMAT_14_raw WMAT_15_cod WMAT_15_raw WMAT_16_cod WMAT_16_raw WMAT_17_cod WMAT_17_raw WMAT_18_cod WMAT_18_raw WMAT_19_cod WMAT_19_raw WMAT_20_cod WMAT_20_raw WMAT_21_cod WMAT_21_raw WMAT_22_cod WMAT_22_raw WMAT_23_cod WMAT_23_raw WMAT_24_cod WMAT_24_raw WMAT_25_cod WMAT_25_raw WMAT_26_cod WMAT_26_raw WMAT_A WMAT_B WMAT_C wmat_total wmat_total_std bfbs_01_cod bfbs_01_conf bfbs_01_raw bfbs_03_cod bfbs_03_conf bfbs_03_raw bfbs_04_cod bfbs_04_conf bfbs_04_raw bfbs_10_cod bfbs_10_conf bfbs_10_raw bfbs_12_cod bfbs_12_conf bfbs_12_raw bfbs_14_cod bfbs_14_conf bfbs_14_raw bfbs_17_cod bfbs_17_conf bfbs_17_raw bfbs_23_cod bfbs_23_conf bfbs_23_raw bfbs_conf_total bfbs_cong_conf bfbs_cong_total bfbs_creib_conf bfbs_creib_total bfbs_incong_conf bfbs_incon_total bfbs_increib_conf bfbs_increib_total bfbs_invalid_conf bfbs_invalid_total bfbs_total bfbs_valid_conf bfbs_valid_total EA_01_raw EA_02_raw EA_03_raw EA_04_raw EA_05_raw EA_06_raw EA_07_raw EA_08_raw EA_09_raw EA_10_raw EA_11_raw EA_12_raw EA_13_raw EA_14_raw EA_15_raw EA_16_raw EA_17_raw EA_18_raw EA_19_raw EA_20_raw EA_21_raw EA_22_raw EA_23_raw EA_24_raw EA_azar_TOTAL EA_control_interno_TOTAL EA_otros_poderosos_TOTAL EAR_01_raw EAR_02_raw EAR_03_raw EAR_04_raw EAR_05_raw EAR_06_raw EAR_07_raw EAR_08_raw EAR_09_raw EAR_10_raw EAR_TOTAL ECRRS_ansiedad_TOTAL ECRRS_evitacion_TOTAL ECRRS_madre_01_raw ECRRS_madre_02_raw ECRRS_madre_03_raw ECRRS_madre_04_raw ECRRS_madre_05_raw ECRRS_madre_06_raw ECRRS_madre_07_raw ECRRS_madre_08_raw ECRRS_madre_09_raw ECRRS_madre_ansiedad_TOTAL ECRRS_madre_evitacion_TOTAL ECRRS_mejoramig_01_raw ECRRS_mejoramig_02_raw ECRRS_mejoramig_03_raw ECRRS_mejoramig_04_raw ECRRS_mejoramig_05_raw ECRRS_mejoramig_06_raw ECRRS_mejoramig_07_raw ECRRS_mejoramig_08_raw ECRRS_mejoramig_09_raw ECRRS_mejoramigo_ansiedad_TOTAL ECRRS_mejoramigo_evitacion_TOTAL ECRRS_padre_01_raw ECRRS_padre_02_raw ECRRS_padre_03_raw ECRRS_padre_04_raw ECRRS_padre_05_raw ECRRS_padre_06_raw ECRRS_padre_07_raw ECRRS_padre_08_raw ECRRS_padre_09_raw ECRRS_padre_ansiedad_TOTAL ECRRS_padre_evitacion_TOTAL ECRRS_pareja_01_raw ECRRS_pareja_02_raw ECRRS_pareja_03_raw ECRRS_pareja_04_raw ECRRS_pareja_05_raw ECRRS_pareja_06_raw ECRRS_pareja_07_raw ECRRS_pareja_08_raw ECRRS_pareja_09_raw ECRRS_pareja_ansiedad_TOTAL ECRRS_pareja_evitacion_TOTAL GHQ_01 GHQ_02 GHQ_03 GHQ_04 GHQ_05 GHQ_06 GHQ_07 GHQ_08 GHQ_09 GHQ_10 GHQ_11 GHQ_12 GHQ_autoestima_TOTAL GHQ_estres_TOTAL GHQ_exito_afrontamiento_TOTAL GHQ_TOTAL wdig_dir_total wdig_inv_total WDIGSIMB_TOTAL wdig_total wdig_total_std lkns_01_cod lkns_01_raw lkns_02_cod lkns_02_raw lkns_03_cod lkns_03_raw lkns_04_cod lkns_04_raw lkns_05_cod lkns_05_raw lkns_06_cod lkns_06_raw lkns_07_cod lkns_07_raw lkns_08_cod lkns_08_raw lkns_09_cod lkns_09_raw lkns_10_cod lkns_10_raw lkns_11_cod lkns_11_raw lkns_total SASS_01_raw SASS_02_raw SASS_03_raw SASS_04_raw SASS_05_raw SASS_06_raw SASS_07_raw SASS_08_raw SASS_09_raw SASS_10_raw SASS_11_raw SASS_12_raw SASS_13_raw SASS_14_raw SASS_15_raw SASS_16_raw SASS_17_raw SASS_18_raw SASS_19_raw SASS_20_raw SASS_21_raw SASS_TOTAL SASS_trabajo bayes_all_accuracy bayes_all_confidence bayes_pictorial_qualitative_accuracy bayes_pictorial_quantitative_accuracy bayes_text_qualitative_accuracy bayes_text_quantitative_accuracy # Seleccionamos variables que contienen la cadena de texto &quot;dem&quot; df_wide %&gt;% select(contains(&quot;dem&quot;)) ## # A tibble: 232 x 3 ## dem_genero dem_edad dem_nivedu ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 38 4 ## 2 0 67 2 ## 3 0 24 4 ## 4 0 30 4 ## 5 0 38 3 ## 6 0 45 4 ## 7 1 58 3 ## 8 1 47 4 ## 9 1 52 3 ## 10 1 49 4 ## # … with 222 more rows # Seleccionamos variables que acacan con la cadena de texto &quot;cod&quot; df_wide %&gt;% select(ID, ends_with(&quot;cod&quot;)) ## # A tibble: 232 x 79 ## ID WVOC_01_cod WVOC_02_cod WVOC_03_cod WVOC_04_cod WVOC_05_cod WVOC_06_cod ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 2 2 2 1 2 2 ## 2 2 2 2 2 1 0 0 ## 3 3 2 2 2 1 2 2 ## 4 4 2 1 1 1 2 2 ## 5 5 2 2 1 1 0 0 ## 6 6 1 1 2 1 2 2 ## 7 7 1 0 2 1 0 0 ## 8 8 2 2 2 1 2 2 ## 9 9 2 2 2 2 2 1 ## 10 10 2 2 2 2 2 0 ## # … with 222 more rows, and 72 more variables: WVOC_07_cod &lt;dbl&gt;, ## # WVOC_08_cod &lt;dbl&gt;, WVOC_09_cod &lt;dbl&gt;, WVOC_10_cod &lt;dbl&gt;, WVOC_11_cod &lt;dbl&gt;, ## # WVOC_12_cod &lt;dbl&gt;, WVOC_13_cod &lt;dbl&gt;, WVOC_14_cod &lt;dbl&gt;, WVOC_15_cod &lt;dbl&gt;, ## # WVOC_16_cod &lt;dbl&gt;, WVOC_17_cod &lt;dbl&gt;, WVOC_18_cod &lt;dbl&gt;, WVOC_19_cod &lt;dbl&gt;, ## # WVOC_20_cod &lt;dbl&gt;, WVOC_21_cod &lt;dbl&gt;, WVOC_22_cod &lt;dbl&gt;, WVOC_23_cod &lt;dbl&gt;, ## # WVOC_24_cod &lt;dbl&gt;, WVOC_25_cod &lt;dbl&gt;, WVOC_26_cod &lt;dbl&gt;, WVOC_27_cod &lt;dbl&gt;, ## # WVOC_28_cod &lt;dbl&gt;, WVOC_29_cod &lt;dbl&gt;, WVOC_30_cod &lt;dbl&gt;, WVOC_31_cod &lt;dbl&gt;, ## # WVOC_32_cod &lt;dbl&gt;, WVOC_33_cod &lt;dbl&gt;, WMAT_01_cod &lt;dbl&gt;, WMAT_02_cod &lt;dbl&gt;, ## # WMAT_03_cod &lt;dbl&gt;, WMAT_04_cod &lt;dbl&gt;, WMAT_05_cod &lt;dbl&gt;, WMAT_06_cod &lt;dbl&gt;, ## # WMAT_07_cod &lt;dbl&gt;, WMAT_08_cod &lt;dbl&gt;, WMAT_09_cod &lt;dbl&gt;, WMAT_10_cod &lt;dbl&gt;, ## # WMAT_11_cod &lt;dbl&gt;, WMAT_12_cod &lt;dbl&gt;, WMAT_13_cod &lt;dbl&gt;, WMAT_14_cod &lt;dbl&gt;, ## # WMAT_15_cod &lt;dbl&gt;, WMAT_16_cod &lt;dbl&gt;, WMAT_17_cod &lt;dbl&gt;, WMAT_18_cod &lt;dbl&gt;, ## # WMAT_19_cod &lt;dbl&gt;, WMAT_20_cod &lt;dbl&gt;, WMAT_21_cod &lt;dbl&gt;, WMAT_22_cod &lt;dbl&gt;, ## # WMAT_23_cod &lt;dbl&gt;, WMAT_24_cod &lt;dbl&gt;, WMAT_25_cod &lt;dbl&gt;, WMAT_26_cod &lt;dbl&gt;, ## # bfbs_01_cod &lt;dbl&gt;, bfbs_03_cod &lt;dbl&gt;, bfbs_04_cod &lt;dbl&gt;, bfbs_10_cod &lt;dbl&gt;, ## # bfbs_12_cod &lt;dbl&gt;, bfbs_14_cod &lt;dbl&gt;, bfbs_17_cod &lt;dbl&gt;, bfbs_23_cod &lt;dbl&gt;, ## # lkns_01_cod &lt;dbl&gt;, lkns_02_cod &lt;dbl&gt;, lkns_03_cod &lt;dbl&gt;, lkns_04_cod &lt;dbl&gt;, ## # lkns_05_cod &lt;dbl&gt;, lkns_06_cod &lt;dbl&gt;, lkns_07_cod &lt;dbl&gt;, lkns_08_cod &lt;dbl&gt;, ## # lkns_09_cod &lt;dbl&gt;, lkns_10_cod &lt;dbl&gt;, lkns_11_cod &lt;dbl&gt; # Lo mismo, pero usando expresiones regulares df_wide %&gt;% select(ID, matches(&quot;cod$&quot;)) # $: fin de la cadena de texto ## # A tibble: 232 x 79 ## ID WVOC_01_cod WVOC_02_cod WVOC_03_cod WVOC_04_cod WVOC_05_cod WVOC_06_cod ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 2 2 2 1 2 2 ## 2 2 2 2 2 1 0 0 ## 3 3 2 2 2 1 2 2 ## 4 4 2 1 1 1 2 2 ## 5 5 2 2 1 1 0 0 ## 6 6 1 1 2 1 2 2 ## 7 7 1 0 2 1 0 0 ## 8 8 2 2 2 1 2 2 ## 9 9 2 2 2 2 2 1 ## 10 10 2 2 2 2 2 0 ## # … with 222 more rows, and 72 more variables: WVOC_07_cod &lt;dbl&gt;, ## # WVOC_08_cod &lt;dbl&gt;, WVOC_09_cod &lt;dbl&gt;, WVOC_10_cod &lt;dbl&gt;, WVOC_11_cod &lt;dbl&gt;, ## # WVOC_12_cod &lt;dbl&gt;, WVOC_13_cod &lt;dbl&gt;, WVOC_14_cod &lt;dbl&gt;, WVOC_15_cod &lt;dbl&gt;, ## # WVOC_16_cod &lt;dbl&gt;, WVOC_17_cod &lt;dbl&gt;, WVOC_18_cod &lt;dbl&gt;, WVOC_19_cod &lt;dbl&gt;, ## # WVOC_20_cod &lt;dbl&gt;, WVOC_21_cod &lt;dbl&gt;, WVOC_22_cod &lt;dbl&gt;, WVOC_23_cod &lt;dbl&gt;, ## # WVOC_24_cod &lt;dbl&gt;, WVOC_25_cod &lt;dbl&gt;, WVOC_26_cod &lt;dbl&gt;, WVOC_27_cod &lt;dbl&gt;, ## # WVOC_28_cod &lt;dbl&gt;, WVOC_29_cod &lt;dbl&gt;, WVOC_30_cod &lt;dbl&gt;, WVOC_31_cod &lt;dbl&gt;, ## # WVOC_32_cod &lt;dbl&gt;, WVOC_33_cod &lt;dbl&gt;, WMAT_01_cod &lt;dbl&gt;, WMAT_02_cod &lt;dbl&gt;, ## # WMAT_03_cod &lt;dbl&gt;, WMAT_04_cod &lt;dbl&gt;, WMAT_05_cod &lt;dbl&gt;, WMAT_06_cod &lt;dbl&gt;, ## # WMAT_07_cod &lt;dbl&gt;, WMAT_08_cod &lt;dbl&gt;, WMAT_09_cod &lt;dbl&gt;, WMAT_10_cod &lt;dbl&gt;, ## # WMAT_11_cod &lt;dbl&gt;, WMAT_12_cod &lt;dbl&gt;, WMAT_13_cod &lt;dbl&gt;, WMAT_14_cod &lt;dbl&gt;, ## # WMAT_15_cod &lt;dbl&gt;, WMAT_16_cod &lt;dbl&gt;, WMAT_17_cod &lt;dbl&gt;, WMAT_18_cod &lt;dbl&gt;, ## # WMAT_19_cod &lt;dbl&gt;, WMAT_20_cod &lt;dbl&gt;, WMAT_21_cod &lt;dbl&gt;, WMAT_22_cod &lt;dbl&gt;, ## # WMAT_23_cod &lt;dbl&gt;, WMAT_24_cod &lt;dbl&gt;, WMAT_25_cod &lt;dbl&gt;, WMAT_26_cod &lt;dbl&gt;, ## # bfbs_01_cod &lt;dbl&gt;, bfbs_03_cod &lt;dbl&gt;, bfbs_04_cod &lt;dbl&gt;, bfbs_10_cod &lt;dbl&gt;, ## # bfbs_12_cod &lt;dbl&gt;, bfbs_14_cod &lt;dbl&gt;, bfbs_17_cod &lt;dbl&gt;, bfbs_23_cod &lt;dbl&gt;, ## # lkns_01_cod &lt;dbl&gt;, lkns_02_cod &lt;dbl&gt;, lkns_03_cod &lt;dbl&gt;, lkns_04_cod &lt;dbl&gt;, ## # lkns_05_cod &lt;dbl&gt;, lkns_06_cod &lt;dbl&gt;, lkns_07_cod &lt;dbl&gt;, lkns_08_cod &lt;dbl&gt;, ## # lkns_09_cod &lt;dbl&gt;, lkns_10_cod &lt;dbl&gt;, lkns_11_cod &lt;dbl&gt; 2.2.2.4 Modificar y añadir variables # DF original DF_name ## # A tibble: 103 x 8 ## X1 ID Genero Edad Educacion FollowUP condition PPV_DECLARED ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 4 41904 1 47 8 80 PPV_Cond1 99 ## 2 5 95041 2 21 6 90 PPV_Cond1 99 ## 3 6 74594 2 29 6 10 PPV_Cond1 99 ## 4 15 72903 2 27 7 75 PPV_Cond1 1 ## 5 16 21260 1 29 5 35 PPV_Cond1 24 ## 6 18 50315 2 28 6 14 PPV_Cond1 99 ## 7 19 21774 2 27 4 2 PPV_Cond1 99 ## 8 20 20881 2 55 6 89 PPV_Cond1 99 ## 9 21 39751 2 28 6 6 PPV_Cond1 99 ## 10 22 99384 1 46 5 0 PPV_Cond1 1 ## # … with 93 more rows # Modificar variable reemplazando valor DF_name %&gt;% mutate(PPV_DECLARED = PPV_DECLARED/100) ## # A tibble: 103 x 8 ## X1 ID Genero Edad Educacion FollowUP condition PPV_DECLARED ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 4 41904 1 47 8 80 PPV_Cond1 0.99 ## 2 5 95041 2 21 6 90 PPV_Cond1 0.99 ## 3 6 74594 2 29 6 10 PPV_Cond1 0.99 ## 4 15 72903 2 27 7 75 PPV_Cond1 0.01 ## 5 16 21260 1 29 5 35 PPV_Cond1 0.24 ## 6 18 50315 2 28 6 14 PPV_Cond1 0.99 ## 7 19 21774 2 27 4 2 PPV_Cond1 0.99 ## 8 20 20881 2 55 6 89 PPV_Cond1 0.99 ## 9 21 39751 2 28 6 6 PPV_Cond1 0.99 ## 10 22 99384 1 46 5 0 PPV_Cond1 0.01 ## # … with 93 more rows # Añadir variable DF_name %&gt;% mutate(PPV_DECLARED_PCT = PPV_DECLARED/100) ## # A tibble: 103 x 9 ## X1 ID Genero Edad Educacion FollowUP condition PPV_DECLARED ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 4 41904 1 47 8 80 PPV_Cond1 99 ## 2 5 95041 2 21 6 90 PPV_Cond1 99 ## 3 6 74594 2 29 6 10 PPV_Cond1 99 ## 4 15 72903 2 27 7 75 PPV_Cond1 1 ## 5 16 21260 1 29 5 35 PPV_Cond1 24 ## 6 18 50315 2 28 6 14 PPV_Cond1 99 ## 7 19 21774 2 27 4 2 PPV_Cond1 99 ## 8 20 20881 2 55 6 89 PPV_Cond1 99 ## 9 21 39751 2 28 6 6 PPV_Cond1 99 ## 10 22 99384 1 46 5 0 PPV_Cond1 1 ## # … with 93 more rows, and 1 more variable: PPV_DECLARED_PCT &lt;dbl&gt; # Añadir variable destruyendo el resto del DF DF_name %&gt;% transmute(PPV_DECLARED_PCT = PPV_DECLARED/100) ## # A tibble: 103 x 1 ## PPV_DECLARED_PCT ## &lt;dbl&gt; ## 1 0.99 ## 2 0.99 ## 3 0.99 ## 4 0.01 ## 5 0.24 ## 6 0.99 ## 7 0.99 ## 8 0.99 ## 9 0.99 ## 10 0.01 ## # … with 93 more rows # Limpiar nombres DF_name %&gt;% janitor::clean_names() ## # A tibble: 103 x 8 ## x1 id genero edad educacion follow_up condition ppv_declared ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 4 41904 1 47 8 80 PPV_Cond1 99 ## 2 5 95041 2 21 6 90 PPV_Cond1 99 ## 3 6 74594 2 29 6 10 PPV_Cond1 99 ## 4 15 72903 2 27 7 75 PPV_Cond1 1 ## 5 16 21260 1 29 5 35 PPV_Cond1 24 ## 6 18 50315 2 28 6 14 PPV_Cond1 99 ## 7 19 21774 2 27 4 2 PPV_Cond1 99 ## 8 20 20881 2 55 6 89 PPV_Cond1 99 ## 9 21 39751 2 28 6 6 PPV_Cond1 99 ## 10 22 99384 1 46 5 0 PPV_Cond1 1 ## # … with 93 more rows 2.2.2.5 Resúmenes agrupados La combinación de verbos group_by() y summarise() es una de las más usadas. Con esta podemos calcular promedios, medianas, etc. por condición de manera sencilla. # Resumen DF_name %&gt;% summarise(Promedio_PPV = mean(PPV_DECLARED), N = n()) ## # A tibble: 1 x 2 ## Promedio_PPV N ## &lt;dbl&gt; &lt;int&gt; ## 1 68.5 103 # Resumen agrupado DF_name %&gt;% group_by(Genero) %&gt;% summarise(Promedio_PPV = mean(PPV_DECLARED), N = n()) ## # A tibble: 2 x 3 ## Genero Promedio_PPV N ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1 66.4 40 ## 2 2 69.9 63 # Resumen agrupando por multiples variables, y calculando varias cosas DF_name %&gt;% group_by(Genero, condition) %&gt;% summarise(promedio_PPV = mean(PPV_DECLARED), mediana_PPV = median(PPV_DECLARED), SD = sd(PPV_DECLARED), N = n()) ## # A tibble: 8 x 6 ## # Groups: Genero [2] ## Genero condition promedio_PPV mediana_PPV SD N ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1 PPV_Cond1 63.8 88 43.1 9 ## 2 1 PPV_Cond2 51 46 10.3 13 ## 3 1 PPV_Cond3 75.6 80 32.5 5 ## 4 1 PPV_Cond4 80.1 92 19.4 13 ## 5 2 PPV_Cond1 74 99 43.0 19 ## 6 2 PPV_Cond2 49.4 46 16.3 8 ## 7 2 PPV_Cond3 69.2 98.5 38.9 16 ## 8 2 PPV_Cond4 74.7 90 27.3 20 Ejercicios - verbos dplyr Usando la base df_wide, haz las siguientes cosas, una a una: Importa los datos (ver código abajo) Filtra el DF para quedarnos solo con edades entre 18 y 50 años Ordena los datos por genero y edad, esta última decreciente Selecciona las columnas para quedarnos solo con ID, variables demograficas, y respuestas crudas (raw) Crea una nueva variable que sea niv_edu_porc, en la que calcules cual es el porcentaje de nivel educativo al que han llegado relativo al máximo de la base de datos (nivel educativo persona / nivel educativo maximo; en porcentaje) Ahora combina el resultado de todas las operaciones anteriores en un DF Calcula el promedio y desviación típica de edad para cada género df_wide = read_csv(&quot;https://raw.githubusercontent.com/gorkang/cognitive-and-socio-affective-predictors-of-social-adaptation/master/data-raw/sa-raw-anonymised.csv&quot;) 2.2.3 Verbos avanzados y otras criaturas indómitas 2.2.3.1 Wide to long Empecemos con un ejemplo muy sencillo. 3 participantes, 2 items. # Creamos un DF df_simple_wide = data.frame(ID = c(&quot;Participante1&quot;, &quot;Participante2&quot;, &quot;Participante3&quot;), Item1 = c(22, 33, 44), Item2 = c(88, 99, 77)) df_simple_wide ## ID Item1 Item2 ## 1 Participante1 22 88 ## 2 Participante2 33 99 ## 3 Participante3 44 77 # Wide to long df_simple_long = df_simple_wide %&gt;% pivot_longer(Item1:Item2, names_to = &quot;Item&quot;, values_to = &quot;Response&quot;) df_simple_long ## # A tibble: 6 x 3 ## ID Item Response ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Participante1 Item1 22 ## 2 Participante1 Item2 88 ## 3 Participante2 Item1 33 ## 4 Participante2 Item2 99 ## 5 Participante3 Item1 44 ## 6 Participante3 Item2 77 Ahora pasemos a un ejemplo mas complejo. Tenemos las puntuaciones a los 11 items de la lipkus numeracy scale de 232 participantes, ademas de datos demográficos. # Leemos documento en formato WIDE df_wide = read_csv(&quot;https://raw.githubusercontent.com/gorkang/cognitive-and-socio-affective-predictors-of-social-adaptation/master/data-raw/sa-raw-anonymised.csv&quot;) %&gt;% # Seleccionamos solo algunas de las filas select(ID, dem_genero, dem_edad, dem_nivedu, matches(&quot;lkns_[0-9]{2}_raw&quot;)) df_wide ## # A tibble: 232 x 15 ## ID dem_genero dem_edad dem_nivedu lkns_01_raw lkns_02_raw lkns_03_raw ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 38 4 500 10 10 ## 2 2 0 67 2 0 0 0 ## 3 3 0 24 4 700 100 0.1 ## 4 4 0 30 4 500 30 1 ## 5 5 0 38 3 6 2 3 ## 6 6 0 45 4 40 200 2 ## 7 7 1 58 3 0 0 0 ## 8 8 1 47 4 500 100 0.1 ## 9 9 1 52 3 600 1 1 ## 10 10 1 49 4 600 500 70 ## # … with 222 more rows, and 8 more variables: lkns_04_raw &lt;chr&gt;, ## # lkns_05_raw &lt;chr&gt;, lkns_06_raw &lt;dbl&gt;, lkns_07_raw &lt;chr&gt;, lkns_08_raw &lt;dbl&gt;, ## # lkns_09_raw &lt;dbl&gt;, lkns_10_raw &lt;dbl&gt;, lkns_11_raw &lt;dbl&gt; # Wide to long df_wide %&gt;% pivot_longer(cols = lkns_01_raw:lkns_11_raw, names_to = &quot;Item&quot;, values_to = &quot;Response&quot;, values_transform = list(Response = as.character)) ## # A tibble: 2,552 x 6 ## ID dem_genero dem_edad dem_nivedu Item Response ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 1 38 4 lkns_01_raw 500 ## 2 1 1 38 4 lkns_02_raw 10 ## 3 1 1 38 4 lkns_03_raw 10 ## 4 1 1 38 4 lkns_04_raw b) 1 de cada 1000 ## 5 1 1 38 4 lkns_05_raw b)10% ## 6 1 1 38 4 lkns_06_raw 2 ## 7 1 1 38 4 lkns_07_raw 0 ## 8 1 1 38 4 lkns_08_raw 0 ## 9 1 1 38 4 lkns_09_raw 0 ## 10 1 1 38 4 lkns_10_raw 0 ## # … with 2,542 more rows df_wide %&gt;% pivot_longer(5:15, names_to = &quot;Item&quot;, values_to = &quot;Response&quot;, values_transform = list(Response = as.character)) ## # A tibble: 2,552 x 6 ## ID dem_genero dem_edad dem_nivedu Item Response ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 1 38 4 lkns_01_raw 500 ## 2 1 1 38 4 lkns_02_raw 10 ## 3 1 1 38 4 lkns_03_raw 10 ## 4 1 1 38 4 lkns_04_raw b) 1 de cada 1000 ## 5 1 1 38 4 lkns_05_raw b)10% ## 6 1 1 38 4 lkns_06_raw 2 ## 7 1 1 38 4 lkns_07_raw 0 ## 8 1 1 38 4 lkns_08_raw 0 ## 9 1 1 38 4 lkns_09_raw 0 ## 10 1 1 38 4 lkns_10_raw 0 ## # … with 2,542 more rows df_long = df_wide %&gt;% pivot_longer(matches(&quot;lkns&quot;), names_to = &quot;Item&quot;, values_to = &quot;Response&quot;, values_transform = list(Response = as.character)) DT::datatable(df_long) 2.2.3.2 Long to wide Retomamos el ejemplo simple de antes: # Long to wide simple df_simple_long %&gt;% pivot_wider(names_from = Item, values_from = Response) ## # A tibble: 3 x 3 ## ID Item1 Item2 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Participante1 22 88 ## 2 Participante2 33 99 ## 3 Participante3 44 77 Y lo mismo con el ejemplo mas complejo: # Long to wide df_long %&gt;% pivot_wider(names_from = Item, values_from = Response) ## # A tibble: 232 x 15 ## ID dem_genero dem_edad dem_nivedu lkns_01_raw lkns_02_raw lkns_03_raw ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 1 38 4 500 10 10 ## 2 2 0 67 2 0 0 0 ## 3 3 0 24 4 700 100 0.1 ## 4 4 0 30 4 500 30 1 ## 5 5 0 38 3 6 2 3 ## 6 6 0 45 4 40 200 2 ## 7 7 1 58 3 0 0 0 ## 8 8 1 47 4 500 100 0.1 ## 9 9 1 52 3 600 1 1 ## 10 10 1 49 4 600 500 70 ## # … with 222 more rows, and 8 more variables: lkns_04_raw &lt;chr&gt;, ## # lkns_05_raw &lt;chr&gt;, lkns_06_raw &lt;chr&gt;, lkns_07_raw &lt;chr&gt;, lkns_08_raw &lt;chr&gt;, ## # lkns_09_raw &lt;chr&gt;, lkns_10_raw &lt;chr&gt;, lkns_11_raw &lt;chr&gt; 2.2.3.2.1 ¿Para que sirve tener los datos en formato long? DF_plot = df_long %&gt;% mutate(Response_num = as.numeric(Response)) DF_plot %&gt;% drop_na(Response_num) %&gt;% ggplot(aes(Item, Response_num, color = Item)) + geom_jitter(alpha = .5) + geom_violin(alpha = .4) + scale_y_log10() + coord_flip() DF_plot %&gt;% filter(is.na(Response_num)) %&gt;% ggplot(aes(Item, Response, color = Item)) + geom_jitter(alpha = .5, height = .2) + facet_wrap(~ Item, scales = &quot;free&quot;) Ejercicios - wide to long Trabajaremos con los datos procesados del paper Cognitive and Socio-affective Predictors of Social Adaptation, de Neely et al. Estos se pueden encontrar en un repositorio público de la OSF. Empezaremos con la base final en formato wide (Dentro de https://osf.io/egxy5/, ver archivo: /outputs/data/sa-prepared.csv). Cambia el orden de las variables para que ID sea la primera columna. Transforma la base a formato long (eso sí, mantén las variables demográficas en formato wide). Pista: Tendras que usar la función select() y el select helper everything() para el primer paso. Importamos datos, y limpiamos nombres de variables: DF_wide = read_csv(&quot;https://raw.githubusercontent.com/gorkang/cognitive-and-socio-affective-predictors-of-social-adaptation/master/outputs/data/sa-prepared.csv&quot;) %&gt;% janitor::clean_names() 2.2.4 Separate, omit, ifelse, case_when, tipos de variables… # Base original name_of_file = here::here(&quot;data/files/02-read-csv.csv&quot;) DF_name = read_csv(name_of_file) # Separate DF_name %&gt;% separate(condition, c(&quot;primer_chunk&quot;, &quot;segundo_chunk&quot;), sep = &quot;_&quot;) ## # A tibble: 103 x 9 ## X1 ID Genero Edad Educacion FollowUP primer_chunk segundo_chunk ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 4 41904 1 47 8 80 PPV Cond1 ## 2 5 95041 2 21 6 90 PPV Cond1 ## 3 6 74594 2 29 6 10 PPV Cond1 ## 4 15 72903 2 27 7 75 PPV Cond1 ## 5 16 21260 1 29 5 35 PPV Cond1 ## 6 18 50315 2 28 6 14 PPV Cond1 ## 7 19 21774 2 27 4 2 PPV Cond1 ## 8 20 20881 2 55 6 89 PPV Cond1 ## 9 21 39751 2 28 6 6 PPV Cond1 ## 10 22 99384 1 46 5 0 PPV Cond1 ## # … with 93 more rows, and 1 more variable: PPV_DECLARED &lt;dbl&gt; # Separate in rows DF_name %&gt;% separate_rows(condition, sep = &quot;_&quot;) ## # A tibble: 206 x 8 ## X1 ID Genero Edad Educacion FollowUP condition PPV_DECLARED ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 4 41904 1 47 8 80 PPV 99 ## 2 4 41904 1 47 8 80 Cond1 99 ## 3 5 95041 2 21 6 90 PPV 99 ## 4 5 95041 2 21 6 90 Cond1 99 ## 5 6 74594 2 29 6 10 PPV 99 ## 6 6 74594 2 29 6 10 Cond1 99 ## 7 15 72903 2 27 7 75 PPV 1 ## 8 15 72903 2 27 7 75 Cond1 1 ## 9 16 21260 1 29 5 35 PPV 24 ## 10 16 21260 1 29 5 35 Cond1 24 ## # … with 196 more rows # Drop NAs DF_name %&gt;% drop_na(PPV_DECLARED) ## # A tibble: 103 x 8 ## X1 ID Genero Edad Educacion FollowUP condition PPV_DECLARED ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 4 41904 1 47 8 80 PPV_Cond1 99 ## 2 5 95041 2 21 6 90 PPV_Cond1 99 ## 3 6 74594 2 29 6 10 PPV_Cond1 99 ## 4 15 72903 2 27 7 75 PPV_Cond1 1 ## 5 16 21260 1 29 5 35 PPV_Cond1 24 ## 6 18 50315 2 28 6 14 PPV_Cond1 99 ## 7 19 21774 2 27 4 2 PPV_Cond1 99 ## 8 20 20881 2 55 6 89 PPV_Cond1 99 ## 9 21 39751 2 28 6 6 PPV_Cond1 99 ## 10 22 99384 1 46 5 0 PPV_Cond1 1 ## # … with 93 more rows # If else DF_name %&gt;% mutate(Genero = ifelse(Genero == 1, &quot;Hombre&quot;, &quot;Mujer&quot;)) ## # A tibble: 103 x 8 ## X1 ID Genero Edad Educacion FollowUP condition PPV_DECLARED ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 4 41904 Hombre 47 8 80 PPV_Cond1 99 ## 2 5 95041 Mujer 21 6 90 PPV_Cond1 99 ## 3 6 74594 Mujer 29 6 10 PPV_Cond1 99 ## 4 15 72903 Mujer 27 7 75 PPV_Cond1 1 ## 5 16 21260 Hombre 29 5 35 PPV_Cond1 24 ## 6 18 50315 Mujer 28 6 14 PPV_Cond1 99 ## 7 19 21774 Mujer 27 4 2 PPV_Cond1 99 ## 8 20 20881 Mujer 55 6 89 PPV_Cond1 99 ## 9 21 39751 Mujer 28 6 6 PPV_Cond1 99 ## 10 22 99384 Hombre 46 5 0 PPV_Cond1 1 ## # … with 93 more rows # Case when DF_name %&gt;% mutate(Genero = case_when( Genero == 1 ~ &quot;Hombre&quot;, Genero == 2 ~ &quot;Mujer&quot;, TRUE ~ &quot;Otros&quot;)) ## # A tibble: 103 x 8 ## X1 ID Genero Edad Educacion FollowUP condition PPV_DECLARED ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 4 41904 Hombre 47 8 80 PPV_Cond1 99 ## 2 5 95041 Mujer 21 6 90 PPV_Cond1 99 ## 3 6 74594 Mujer 29 6 10 PPV_Cond1 99 ## 4 15 72903 Mujer 27 7 75 PPV_Cond1 1 ## 5 16 21260 Hombre 29 5 35 PPV_Cond1 24 ## 6 18 50315 Mujer 28 6 14 PPV_Cond1 99 ## 7 19 21774 Mujer 27 4 2 PPV_Cond1 99 ## 8 20 20881 Mujer 55 6 89 PPV_Cond1 99 ## 9 21 39751 Mujer 28 6 6 PPV_Cond1 99 ## 10 22 99384 Hombre 46 5 0 PPV_Cond1 1 ## # … with 93 more rows # Unite DF_separated = DF_name %&gt;% separate(condition, c(&quot;primer_chunk&quot;, &quot;segundo_chunk&quot;), sep = &quot;_&quot;) DF_separated %&gt;% unite(condition, c(primer_chunk, segundo_chunk), sep = &quot;_&quot;) ## # A tibble: 103 x 8 ## X1 ID Genero Edad Educacion FollowUP condition PPV_DECLARED ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 4 41904 1 47 8 80 PPV_Cond1 99 ## 2 5 95041 2 21 6 90 PPV_Cond1 99 ## 3 6 74594 2 29 6 10 PPV_Cond1 99 ## 4 15 72903 2 27 7 75 PPV_Cond1 1 ## 5 16 21260 1 29 5 35 PPV_Cond1 24 ## 6 18 50315 2 28 6 14 PPV_Cond1 99 ## 7 19 21774 2 27 4 2 PPV_Cond1 99 ## 8 20 20881 2 55 6 89 PPV_Cond1 99 ## 9 21 39751 2 28 6 6 PPV_Cond1 99 ## 10 22 99384 1 46 5 0 PPV_Cond1 1 ## # … with 93 more rows # Pull DF_name %&gt;% pull(PPV_DECLARED) %&gt;% mean(.) ## [1] 68.52427 Ejercicios - verbos avanzados dplyr Importa los datos y limpia los nombres de columna: # Leemos los datos y usamos janitor::clean_names() para limpiar los nombres de las columnas DF_wide = read_csv(&quot;https://raw.githubusercontent.com/gorkang/cognitive-and-socio-affective-predictors-of-social-adaptation/master/outputs/data/sa-prepared.csv&quot;) %&gt;% janitor::clean_names() En un nuevo DF (DF_split), crea una variable llamada social_adaptation_split con la median split para la variable social_adaptation. La mitad superior se llamará high_social_adaptation y la mitad inferior low_social_adaptation. Asegúrate que no hay valores NA. Pista: Necesitarás extraer la mediana a una variable a parte, y después usar case_when() para crear la nueva variable social_adaptation_split. La función drop_na() será necesaria para el paso 3. El resultado final debería ser: 2.2.5 Regular expressions Basic Regular Expressions Cheatsheet SOURCE: https://xkcd.com/208/ Usando este dataframe: DF_regexp = DF_name %&gt;% select(-X1); DF_regexp ## # A tibble: 103 x 7 ## ID Genero Edad Educacion FollowUP condition PPV_DECLARED ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 41904 1 47 8 80 PPV_Cond1 99 ## 2 95041 2 21 6 90 PPV_Cond1 99 ## 3 74594 2 29 6 10 PPV_Cond1 99 ## 4 72903 2 27 7 75 PPV_Cond1 1 ## 5 21260 1 29 5 35 PPV_Cond1 24 ## 6 50315 2 28 6 14 PPV_Cond1 99 ## 7 21774 2 27 4 2 PPV_Cond1 99 ## 8 20881 2 55 6 89 PPV_Cond1 99 ## 9 39751 2 28 6 6 PPV_Cond1 99 ## 10 99384 1 46 5 0 PPV_Cond1 1 ## # … with 93 more rows Podemos usar la función gsub() para eliminar partes de una cadena de texto, o para extraer un número: # gsub() DF_regexp %&gt;% mutate(condition = gsub(&quot;PPV_&quot;, &quot;&quot;, condition)) %&gt;% # Buscamos PPV_ y lo reemplazamos por nada (lo eliminamos!) mutate(condition_N = gsub(&quot;.*([[:digit:]]$)&quot;, &quot;\\\\1&quot;, condition)) # Busca cualquier dígito al final de la cadena de texto y elimina el resto ## # A tibble: 103 x 8 ## ID Genero Edad Educacion FollowUP condition PPV_DECLARED condition_N ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 41904 1 47 8 80 Cond1 99 1 ## 2 95041 2 21 6 90 Cond1 99 1 ## 3 74594 2 29 6 10 Cond1 99 1 ## 4 72903 2 27 7 75 Cond1 1 1 ## 5 21260 1 29 5 35 Cond1 24 1 ## 6 50315 2 28 6 14 Cond1 99 1 ## 7 21774 2 27 4 2 Cond1 99 1 ## 8 20881 2 55 6 89 Cond1 99 1 ## 9 39751 2 28 6 6 Cond1 99 1 ## 10 99384 1 46 5 0 Cond1 1 1 ## # … with 93 more rows Con select() y matches() seleccionamos columnas usando la siguiente regular expression lkns_[0-9]{2}_raw: lkns_ contiene esta cadena de texto [0-9]{2} a continuación, contiene cualquier dígito del 0 al 9, dos veces. _rawa continuación, contiene esta cadena de texto read_csv(&quot;https://raw.githubusercontent.com/gorkang/cognitive-and-socio-affective-predictors-of-social-adaptation/master/data-raw/sa-raw-anonymised.csv&quot;) %&gt;% # Seleccionamos solo algunas de las filas select(ID, dem_genero, dem_edad, dem_nivedu, matches(&quot;lkns_[0-9]{2}_raw&quot;)) ## # A tibble: 232 x 15 ## ID dem_genero dem_edad dem_nivedu lkns_01_raw lkns_02_raw lkns_03_raw ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 38 4 500 10 10 ## 2 2 0 67 2 0 0 0 ## 3 3 0 24 4 700 100 0.1 ## 4 4 0 30 4 500 30 1 ## 5 5 0 38 3 6 2 3 ## 6 6 0 45 4 40 200 2 ## 7 7 1 58 3 0 0 0 ## 8 8 1 47 4 500 100 0.1 ## 9 9 1 52 3 600 1 1 ## 10 10 1 49 4 600 500 70 ## # … with 222 more rows, and 8 more variables: lkns_04_raw &lt;chr&gt;, ## # lkns_05_raw &lt;chr&gt;, lkns_06_raw &lt;dbl&gt;, lkns_07_raw &lt;chr&gt;, lkns_08_raw &lt;dbl&gt;, ## # lkns_09_raw &lt;dbl&gt;, lkns_10_raw &lt;dbl&gt;, lkns_11_raw &lt;dbl&gt; Una aplicación Shiny para ayudar a construir Regular Expressions: regexplain::regexplain_gadget() Ejercicios - Calcular puntajes de escalas usando regular expressions Ahora volvemos a usar con los datos brutos (sa-raw-anonymised.csv) del paper Cognitive and Socio-affective Predictors of Social Adaptation, de Neely et al.  En estos datos tenemos las puntuaciones crudas (e.g. WMAT_01_raw) y ya codificadas/corregidas (WMAT_01_cod) para los ítems de varias escalas Para preparar los datos de cara al análisis final, necesitamos calcular el puntaje para cada participante y escala. Empezaremos con la prueba de Matrices de WAIS (WMAT_). **Extrae la suma para cada participante de los ítems WMAT_[NUMEROS]_cod** Hay al menos dos estrategias posibles: Selecciona las columnas relevantes y haz la suma de columnas Convierte a long, filtra para quedarte con las filas correspondientes a la prueba relevante, y haz una suma agrupada Pista para seleccionar o filtrar columnas: Recuerda que usamos select() para seleccionar columnas, o filter() para filtrar. Pista para seleccionar columnas: Podemos usar matches(\"WMAT_[0-9]{2}_cod\") para seleccionar o filtrar todas las columnas o ítems que contienen: WMAT_, 2 numeros del 0 al 9, y acaban en _cod. Pista para suma de columnas: rowSums() es la función que podemos usar, pero su sintaxis es algo complicada. Pista para suma agrupada: Usamos group_by() %&gt;% summarise() poniendo parámetros dentro de cada función. Importar datos: df_wide_raw = read_csv(&quot;https://raw.githubusercontent.com/gorkang/cognitive-and-socio-affective-predictors-of-social-adaptation/master/data-raw/sa-raw-anonymised.csv&quot;) 2.3 Combinar bases de datos 2.3.1 Bind rows or columns El método más sencillo. Símplemente unimos las filas o columnas de los dataframes. # Importar CSVs DF1 = read_csv(here::here(&quot;data/files/02-CSVs/01.csv&quot;)) DF2 = read_csv(here::here(&quot;data/files/02-CSVs/02.csv&quot;)) # Bind DFs añadiendo las *filas* de DF2 a DF1 DF1 %&gt;% bind_rows(DF2) ## # A tibble: 800 x 9 ## Sex Priming trialN Block Adjective Valence Answer Arrow rT ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 male Collective 1 we ofensivo negative yes left 623 ## 2 male Collective 2 we resentido negative no right 1235 ## 3 male Collective 3 we ego�sta negative yes left 335 ## 4 male Collective 4 we indiscreto negative yes left 355 ## 5 male Collective 5 we sumiso negative yes left 618 ## 6 male Collective 6 we agradable positive yes left 328 ## 7 male Collective 7 we clasista negative yes left 348 ## 8 male Collective 8 we altruista positive yes left 1620 ## 9 male Collective 9 we ansioso negative yes left 346 ## 10 male Collective 10 we presumido negative yes left 778 ## # … with 790 more rows # Bind DFs añadiendo las *columnas* de DF2 a DF1 # bind_cols renombra automaticamente los nombres de las columnas para que no haya coincidencias DF1 %&gt;% bind_cols(DF2) ## # A tibble: 400 x 18 ## Sex...1 Priming...2 trialN...3 Block...4 Adjective...5 Valence...6 Answer...7 ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 male Collective 1 we ofensivo negative yes ## 2 male Collective 2 we resentido negative no ## 3 male Collective 3 we ego�sta negative yes ## 4 male Collective 4 we indiscreto negative yes ## 5 male Collective 5 we sumiso negative yes ## 6 male Collective 6 we agradable positive yes ## 7 male Collective 7 we clasista negative yes ## 8 male Collective 8 we altruista positive yes ## 9 male Collective 9 we ansioso negative yes ## 10 male Collective 10 we presumido negative yes ## # … with 390 more rows, and 11 more variables: Arrow...8 &lt;chr&gt;, rT...9 &lt;dbl&gt;, ## # Sex...10 &lt;chr&gt;, Priming...11 &lt;chr&gt;, trialN...12 &lt;dbl&gt;, Block...13 &lt;chr&gt;, ## # Adjective...14 &lt;chr&gt;, Valence...15 &lt;chr&gt;, Answer...16 &lt;chr&gt;, ## # Arrow...17 &lt;chr&gt;, rT...18 &lt;dbl&gt; 2.3.2 Joins El paquete {dplyr} tiene funciones que permiten trabajar combinando, filtrando, etc. distintos dataframes. Podéis ver más detalle y algunas ilustraciones fantásticas (como la de abajo; inner_join()) en el capítulo relational data de r4ds. SOURCE: https://r4ds.had.co.nz/relational-data.html#mutating-joins En https://github.com/gadenbuie/tidyexplain se pueden ver animaciones mostrando estas operaciones. Tipos de Join Estas operaciones tendrán la forma: DF_x %&gt;% WHATEVER_join(DF_y) Mutating joins: inner_join(): preserva pares de observaciones de DF_x y de DF_y con claves iguales left_join(): preserva las observaciones de DF_x, añadiendo las de DF_y con claves iguales right_join(): preserva las observaciones de DF_y, añadiendo las de DF_x con claves iguales full_join(): preserva todas las observaciones de DF_x y DF_y, alineándolas cuando tengan claves iguales Filtering joins: semi_join(): preserva solo aquellas observaciones de DF_x cuyas claves aparezcan en DF_y anti_join(): preserva solo aquellas observaciones de DF_x cuyas claves NO aparezcan en DF_y Nesting joins: nest_join(): preserva las observaciones de DF_x, añadiendo las de DF_y con claves iguales 2.3.2.1 Mutating joins Importamos datos Tenemos los siguientes dataframes: DF_IDs: Variables demográficas de participantes DF_results: Resultados en variables de interés de participantes DF_BAD: Grupo de participantes “selectos” # Importar CSVs para los joins DF_IDs = read_csv(here::here(&quot;data/files/02-join-IDs.csv&quot;)) DF_results = read_csv(here::here(&quot;data/files/02-join-results.csv&quot;)) DF_BAD = read_csv(here::here(&quot;data/files/02-join-BAD.csv&quot;)) # DT::datatable(DF_IDs) # DT::datatable(DF_results) # DT::datatable(DF_BAD) 2.3.2.1.1 Inner join Preserva pares de observaciones de DF_x y de DF_y con claves iguales. SOURCE: https://github.com/gadenbuie/tidyexplain DF_inner_joined = DF_IDs %&gt;% inner_join(DF_results) #nrow(DF_inner_joined) DT::datatable(DF_inner_joined) 2.3.2.1.2 Left join Preserva las observaciones de DF_x, añadiendo las de DF_y con claves iguales. SOURCE: https://github.com/gadenbuie/tidyexplain DF_left_joined = DF_IDs %&gt;% left_join(DF_results) #nrow(DF_left_joined) DT::datatable(DF_left_joined) 2.3.2.1.3 Full join Preserva todas las observaciones de DF_x y DF_y, alineándolas cuando tengan claves iguales. SOURCE: https://github.com/gadenbuie/tidyexplain DF_full_joined = DF_IDs %&gt;% full_join(DF_results) #nrow(DF_full_joined) DT::datatable(DF_full_joined) 2.3.2.2 Filtering joins 2.3.2.2.1 Anti join Preserva solo aquellas observaciones de DF_x cuyas claves NO aparezcan en DF_y. SOURCE: https://github.com/gadenbuie/tidyexplain # AVOID the people present in DF_BAD DF_anti_joined = DF_IDs %&gt;% anti_join(DF_BAD, by = &quot;ID&quot;) %&gt;% left_join(DF_results) DT::datatable(DF_anti_joined) 2.3.2.2.2 Semi join Preserva solo aquellas observaciones de DF_x cuyas claves aparezcan en DF_y. SOURCE: https://github.com/gadenbuie/tidyexplain # INCLUDE ONLY the people present in DF_BAD DF_semi_joined = DF_IDs %&gt;% semi_join(DF_BAD, by = &quot;ID&quot;) %&gt;% left_join(DF_results) DT::datatable(DF_semi_joined) 2.3.2.3 Nesting joins DF_nest_joined = DF_IDs %&gt;% nest_join(DF_results, by = &quot;ID&quot;) DT::datatable(DF_nest_joined) Ejercicios JOINS Con los DFs de abajo, haz las siguientes operaciones: DF_IDs = read_csv(here::here(&quot;data/files/02-join-IDs2.csv&quot;)) DF_results = read_csv(here::here(&quot;data/files/02-join-results.csv&quot;)) DF_BAD = read_csv(here::here(&quot;data/files/02-join-BAD.csv&quot;)) Une los datos demográficos con los resultados A la base resultante, quítale los sujetos descartados de DF_BAD Crea una nueva base con datos demográficos y resultados para los sujetos descartados Comprueba si el promedio para Crystallized Intelligence de los participantes descartados difiere de la de los no descartados Haz una gráfica donde se puedan ver las diferencias En el ejercicio 3 de verbos avanzados creaste un DF llamado DF_split con la median split a partir de la variable Social.Adaptation. Uno ese DF al DF_long que habías creado en el ejercicio 2 de la misma sección. El DF final se vera así: Haz un plot donde se vea la distribución para todas las variables de resultados de los dos niveles de social_adaptation_split. El plot que vimos en el tema anterior tiene el problema de que los datos de tuberculosis son en números absolutos. Serias capaz de convertir estos a % de la población, como se ve en el plot de abajo? 2.4 Más allá de la manipulación 2.4.1 Fast and Frugal trees Hay un paquete fantástico de Nathaniel Phillips llamado {FFTrees}. Este permite crear, visualizar y evaluar fast-and-frugal decision trees DF_wide = read.csv(&quot;https://raw.githubusercontent.com/gorkang/cognitive-and-socio-affective-predictors-of-social-adaptation/master/outputs/data/sa-prepared.csv&quot;) %&gt;% mutate(Anxious.Attachment = round(Anxious.Attachment, 2)) median_social_adaptation = DF_wide %&gt;% pull(Social.Adaptation) %&gt;% median(., na.rm = TRUE) DF_split = DF_wide %&gt;% mutate(high_social_adaptation = case_when( Social.Adaptation &gt;= median_social_adaptation ~ TRUE, Social.Adaptation &lt; median_social_adaptation ~ FALSE)) %&gt;% select(-ID, -Social.Adaptation) %&gt;% drop_na() sa.fft &lt;- FFTrees(formula = high_social_adaptation ~., data = DF_split) # Plotear el arbol de decision plot(sa.fft, main = &quot;Social adaptation&quot;, decision.labels = c(&quot;Low&quot;, &quot;High&quot;)) # Describe el algoritmo sa.fft$inwords ## NULL 2.4.2 Machine learning Con el paquete {caret} podemos usar alguno de los 238 distintos métodos de machine learning. Por ejemplo, Backprop: # ensure the results are repeatable set.seed(7) DF_wide = read_csv(&quot;https://raw.githubusercontent.com/gorkang/cognitive-and-socio-affective-predictors-of-social-adaptation/master/outputs/data/sa-prepared.csv&quot;) %&gt;% janitor::clean_names() %&gt;% as.data.frame() median_social_adaptation = DF_wide %&gt;% pull(social_adaptation) %&gt;% median(., na.rm = TRUE) DB_x = DF_wide %&gt;% mutate(high_social_adaptation = as.factor( case_when( social_adaptation &gt;= median_social_adaptation ~ &quot;high&quot;, social_adaptation &lt; median_social_adaptation ~ &quot;low&quot;))) %&gt;% select(-id, -social_adaptation) %&gt;% drop_na() %&gt;% select(high_social_adaptation, everything()) # calculate correlation matrix correlationMatrix &lt;- cor(DB_x[,2:ncol(DB_x)]) # print(correlationMatrix) # find attributes that are highly corrected (ideally &gt;0.75) highlyCorrelated &lt;- findCorrelation(correlationMatrix, cutoff = 0.25) # print(highlyCorrelated) DB_final = DB_x %&gt;% select(all_of(highlyCorrelated)) %&gt;% select(high_social_adaptation, everything()) %&gt;% as.data.frame() # prepare training scheme control &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 10, repeats = 3) # train the model model &lt;- train(high_social_adaptation ~ ., data = DB_final, method = &quot;lvq&quot;, preProcess = &quot;scale&quot;, trControl = control) # estimate variable importance importance &lt;- varImp(model, scale = FALSE) # print(importance) # plot importance plot(importance) # define the control using a random forest selection function control &lt;- rfeControl(functions = rfFuncs, method = &quot;cv&quot;, number = 10) # run the RFE algorithm results &lt;- rfe(DB_final[,2:ncol(DB_final)], DB_final[,1], sizes = c(1:ncol(DB_final)), rfeControl = control) # summarize the results print(results) ## ## Recursive feature selection ## ## Outer resampling method: Cross-Validated (10 fold) ## ## Resampling performance over subset size: ## ## Variables Accuracy Kappa AccuracySD KappaSD Selected ## 1 0.5444 0.06947 0.04637 0.09913 ## 2 0.5877 0.16723 0.08935 0.17367 ## 3 0.6537 0.30008 0.07950 0.15638 ## 4 0.6299 0.25436 0.05316 0.10521 ## 5 0.6535 0.30535 0.06075 0.11840 ## 6 0.6578 0.31455 0.03172 0.06010 * ## ## The top 5 variables (out of 6): ## stress, internal_locus, crystallized_intelligence, fluid_intelligence, age # list the chosen features predictors(results) ## [1] &quot;stress&quot; &quot;internal_locus&quot; ## [3] &quot;crystallized_intelligence&quot; &quot;fluid_intelligence&quot; ## [5] &quot;age&quot; &quot;sex&quot; # plot the results plot(results, type = c(&quot;g&quot;, &quot;o&quot;)) Caret ha sido desplazado por {tidymodels}, un conjunto de paquetes que emplean la filosofía del {tidyverse}. 2.5 Datasets interesantes En los siguientes repositorios podréis encontrar datasets interesantes para jugar. fivethirtyeight Our World in Data TidyTuesday Bibliografía Cheatsheets RStudio Cheatsheet dplyr data-carpentry-week lesson_joins R4ds - Joins Tidyexplain "],["análisis-de-datos-exploratorio.html", "Capítulo 3 Análisis de datos exploratorio 3.1 Visualizando distribuciones 3.2 Covariación 3.3 Ejercicios finales Bibliografía", " Capítulo 3 Análisis de datos exploratorio Paquetes para este capítulo if (!require(&#39;cowplot&#39;)) install.packages(&#39;cowplot&#39;); library(&#39;cowplot&#39;) if (!require(&#39;dplyr&#39;)) install.packages(&#39;dplyr&#39;); library(&#39;dplyr&#39;) if (!require(&#39;gapminder&#39;)) install.packages(&#39;gapminder&#39;); library(&#39;gapminder&#39;) if (!require(&#39;ggplot2&#39;)) install.packages(&#39;ggplot2&#39;); library(&#39;ggplot2&#39;) if (!require(&#39;ggridges&#39;)) install.packages(&#39;ggridges&#39;); library(&#39;ggridges&#39;) if (!require(&#39;haven&#39;)) install.packages(&#39;haven&#39;); library(&#39;haven&#39;) if (!require(&#39;inspectdf&#39;)) install.packages(&#39;inspectdf&#39;); library(&#39;inspectdf&#39;) if (!require(&#39;tidyr&#39;)) install.packages(&#39;tidyr&#39;); library(&#39;tidyr&#39;) En este capítulo vamos a aplicar lo que hemos aprendido en los dos capítulos anteriores, combinando transformación de datos con visualización para entender nuestras bases de datos, buscar patrones interesantes, etc. Podéis encontrar una introducción más completa en el manual R 4 data science - exploratory data analysis. 3.1 Visualizando distribuciones Para visualizar la distribución de nuestras variables, tendremos que seguir estrategias diferentes dependiendo de si se trata de variables categóricas o continuas. 3.1.1 Variables categóricas ggplot(gapminder, aes(continent)) + geom_bar() gapminder %&gt;% count(continent) ## # A tibble: 5 x 2 ## continent n ## &lt;fct&gt; &lt;int&gt; ## 1 Africa 624 ## 2 Americas 300 ## 3 Asia 396 ## 4 Europe 360 ## 5 Oceania 24 3.1.2 Variables continuas ggplot(gapminder, aes(lifeExp)) + geom_histogram(binwidth = 1) gapminder %&gt;% summarise(MEAN = mean(lifeExp), MEDIAN = median(lifeExp), SD = sd(lifeExp), MAX = max(lifeExp), MIN = min(lifeExp)) ## # A tibble: 1 x 5 ## MEAN MEDIAN SD MAX MIN ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 59.5 60.7 12.9 82.6 23.6 Ejercicios ¿Podrías replicar la visualización de abajo? Queremos mostrar un histograma por continente. ¿Como podemos añadir el histograma general para poder entender donde se ubica cada continente? También queremos ver los descriptivos por continente, ordenados por el promedio: ## # A tibble: 5 x 6 ## continent MEAN MEDIAN SD MAX MIN ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Africa 48.9 47.8 9.15 76.4 23.6 ## 2 Asia 60.1 61.8 11.9 82.6 28.8 ## 3 Americas 64.7 67.0 9.35 80.7 37.6 ## 4 Europe 71.9 72.2 5.43 81.8 43.6 ## 5 Oceania 74.3 73.7 3.80 81.2 69.1 Ejercicio variables individuales Usando el DF mpg, visualiza la distribucion de las variables manufacturer, class, trans, hwy y cty 3.1.3 Visualizando datasets completos Cuando nos llega una nueva base de datos, una de las primeras cosas que haremos será familiarizarnos con los datos. Cómo se distribuyen, cual es la relación entre distintas variables, etc. # Wide to long d &lt;- gapminder %&gt;% pivot_longer(everything(), values_transform = list(value = as.character)) %&gt;% filter(value != 999) %&gt;% # Si existiera algun codigo para missing values, filtrar mutate(value_NUM = as.numeric(value)) # Plot numeric variables d %&gt;% drop_na(value_NUM) %&gt;% ggplot(aes(value_NUM)) + facet_wrap(~ name, scales = &quot;free&quot;) + geom_histogram(bins = 15) #+ scale_x_log10() # Plot non-numeric variables d %&gt;% drop_na(value) %&gt;% filter(is.na(value_NUM)) %&gt;% ggplot(aes(value)) + facet_wrap(~ name, scales = &quot;free&quot;) + geom_bar() + coord_flip() 3.1.3.1 inspectdf gapminder %&gt;% inspectdf::inspect_na() ## # A tibble: 6 x 3 ## col_name cnt pcnt ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 country 0 0 ## 2 continent 0 0 ## 3 year 0 0 ## 4 lifeExp 0 0 ## 5 pop 0 0 ## 6 gdpPercap 0 0 gapminder %&gt;% inspectdf::inspect_na() %&gt;% show_plot + coord_flip() gapminder_cat &lt;- gapminder %&gt;% inspectdf::inspect_cat() gapminder_cat %&gt;% inspectdf::show_plot() gapminder_num &lt;- gapminder %&gt;% inspectdf::inspect_num() gapminder_num %&gt;% inspectdf::show_plot() 3.2 Covariación 3.2.1 Variable categórica y continua Podemos contar el numero de elementos por nivel de la variable o ver densidad, etc. ggplot(gapminder, aes(lifeExp, colour = continent)) + geom_freqpoly(binwidth = 2) Podemos usar geom_density_ridges() para combinar puntos con distribuciones: ggplot(gapminder, aes(lifeExp, continent, fill = continent)) + ggridges::geom_density_ridges(stat = &quot;binline&quot;, bins = 20, scale = 0.95, draw_baseline = FALSE, alpha = .3) + ggridges::geom_density_ridges(jittered_points = TRUE, position = &quot;raincloud&quot;, alpha = 0.5, scale = 0.9) ¿Qué estamos viendo exáctamente arriba? Hay un punto por cada pais, y por cada año, lo que da lugar aalgo bien dificil de interpretar. Podemos ver los datos únicamente del último año: gapminder %&gt;% group_by(year) %&gt;% summarise(n()) ## # A tibble: 12 x 2 ## year `n()` ## &lt;int&gt; &lt;int&gt; ## 1 1952 142 ## 2 1957 142 ## 3 1962 142 ## 4 1967 142 ## 5 1972 142 ## 6 1977 142 ## 7 1982 142 ## 8 1987 142 ## 9 1992 142 ## 10 1997 142 ## 11 2002 142 ## 12 2007 142 ggplot(gapminder %&gt;% filter(year &gt; 1995), aes(lifeExp, continent, fill = continent)) + ggridges::geom_density_ridges(stat = &quot;binline&quot;, bins = 20, scale = 0.95, draw_baseline = FALSE, alpha = .3) + ggridges::geom_density_ridges(jittered_points = TRUE, position = &quot;raincloud&quot;, alpha = 0.5, scale = 0.9) 3.2.2 Ejercicio ¿Podríais mostrar la diferencia entre los extremos en la base de datos, 1952 y 2007?: Mira el plot donde mostramos la diferencia entre los extremos en la base de datos, 1952 y 2007 con dos gráficas, una al lado de la otra. ¿Ves algún problema? Trata de resolver el problema en las escalas. El resultado final debería ser: Podemos hacer algo que clarifica mucho las cosas. Como visualizar ver el avance por país de una manera más directa: # Cálculo DF_gapminder_max_min = gapminder %&gt;% group_by(continent, country) %&gt;% summarise(lifeExp = max(lifeExp) - min(lifeExp)) ggplot(DF_gapminder_max_min, aes(lifeExp, continent, fill = continent)) + ggridges::geom_density_ridges(stat = &quot;binline&quot;, bins = 20, scale = 0.95, draw_baseline = FALSE, alpha = .3) + ggridges::geom_density_ridges(jittered_points = TRUE, position = &quot;raincloud&quot;, alpha = 0.5, scale = 0.9) + theme(legend.position = &quot;none&quot;) + ggtitle(&quot;Diferencia entre max y min por país&quot;) 3.2.3 Ejercicio Arriba estamos asumiendo que el valor máximo de esperanza de vida corresponde al más actualizado, pero probablemente no sea así. ¿Podrías rehacer el cálculo para mostrar la diferencia entre 1952 y 2007? Pista 1. Crear un DF para cada año, renombrando la variable lifeExp2. Usando la funcion full_join(), juntamos ambas bases (tendras que usar el parametro by). 3. Con mutate() calculamos la diferencia. 3.2.4 Dos variables categóricas ggplot(diamonds, aes(cut, color)) + geom_count() diamonds %&gt;% count(color, cut) ## # A tibble: 35 x 3 ## color cut n ## &lt;ord&gt; &lt;ord&gt; &lt;int&gt; ## 1 D Fair 163 ## 2 D Good 662 ## 3 D Very Good 1513 ## 4 D Premium 1603 ## 5 D Ideal 2834 ## 6 E Fair 224 ## 7 E Good 933 ## 8 E Very Good 2400 ## 9 E Premium 2337 ## 10 E Ideal 3903 ## # … with 25 more rows diamonds %&gt;% count(color, cut) %&gt;% ggplot(aes(color, cut, fill = n)) + geom_tile() 3.2.5 Dos variables continuas ggplot(gapminder, aes(lifeExp, gdpPercap)) + geom_point() ggplot(gapminder, aes(lifeExp, gdpPercap, color = continent)) + geom_point(alpha = 1 / 2) + scale_y_log10() ggplot(gapminder, aes(lifeExp, gdpPercap)) + geom_hex() ggplot(gapminder, aes(lifeExp, gdpPercap)) + geom_boxplot(mapping = aes(group = cut_width(lifeExp, 10))) + scale_y_log10() 3.2.6 Ejercicio covariación 2 Usando el DF mpg, visualiza la covariación entre: manufacturer y hwy class y hwy hwy y cty 3.3 Ejercicios finales 3.3.1 Ejercicio exploración base nueva Usando la base del paper Cancer Screening Risk Literacy of Physicians in Training, haz un primer análisis exploratorio que incluya: histogramas de todas las variables numéricas y no-numéricas scatterplots de la relación entre comprensión y numeracy, y entre comprensión y screenbeliefs Bibliografía Wickham, H., &amp; Grolemund, G. (2016). R for data science: import, tidy, transform, visualize, and model data. O’Reilly Media, Inc. https://r4ds.had.co.nz/ "],["trabajo-con-rmarkdown-para-reportes-reproducibles.html", "Capítulo 4 Trabajo con RMarkdown para reportes reproducibles Dependencias 4.1 Que es la reproducibilidad 4.2 Proyectos de R-Studio 4.3 Control de cambios con Git y Github 4.4 Workflow 4.5 RMarkdown, openscience y análisis reproducibles 4.6 Sintaxis, chunks de código, tipos de archivo 4.7 De los datos al reporte final: Una historia de amor con R 4.8 Avanzado 4.9 Mas allá de Rmarkdown 4.10 Varios Bibliografía", " Capítulo 4 Trabajo con RMarkdown para reportes reproducibles Paquetes para este capítulo if (!require(&#39;afex&#39;)) install.packages(&#39;afex&#39;); library(&#39;afex&#39;) if (!require(&#39;corrr&#39;)) install.packages(&#39;corrr&#39;); library(&#39;corrr&#39;) if (!require(&#39;dplyr&#39;)) install.packages(&#39;dplyr&#39;); library(&#39;dplyr&#39;) if (!require(&#39;DT&#39;)) install.packages(&#39;DT&#39;); library(&#39;DT&#39;) if (!require(&#39;ggraph&#39;)) install.packages(&#39;ggraph&#39;); library(&#39;ggraph&#39;) if (!require(&#39;here&#39;)) install.packages(&#39;here&#39;); library(&#39;here&#39;) if (!require(&#39;gtsummary&#39;)) install.packages(&#39;gtsummary&#39;); library(&#39;gtsummary&#39;) if (!require(&#39;knitr&#39;)) install.packages(&#39;knitr&#39;); library(&#39;knitr&#39;) if (!require(&#39;parameters&#39;)) install.packages(&#39;parameters&#39;); library(&#39;parameters&#39;) if (!require(&#39;remotes&#39;)) install.packages(&#39;remotes&#39;); library(&#39;remotes&#39;) if (!require(&#39;renv&#39;)) install.packages(&quot;renv&quot;); library(&#39;renv&#39;) if (!require(&#39;rticles&#39;)) install.packages(&#39;rticles&#39;); library(&#39;rticles&#39;) if (!require(&#39;see&#39;)) install.packages(&quot;see&quot;); library(&#39;see&#39;) if (!require(&#39;sjPlot&#39;)) install.packages(&#39;sjPlot&#39;); library(&#39;sjPlot&#39;) # if (!require(&#39;stargazer&#39;)) install.packages(&#39;stargazer&#39;); library(&#39;stargazer&#39;) if (!require(&#39;stringi&#39;)) install.packages(&#39;stringi&#39;); library(&#39;stringi&#39;) if (!require(&#39;tinytex&#39;)) install.packages(&#39;tinytex&#39;); library(&#39;tinytex&#39;) if (!require(&#39;usethis&#39;)) install.packages(&#39;usethis&#39;); library(&#39;usethis&#39;) if (!require(&#39;correlation&#39;)) remotes::install_github(&quot;easystats/correlation&quot;); library(&#39;correlation&#39;) # if (!require(&#39;correlation&#39;)) install.packages(&quot;correlation&quot;); library(&#39;correlation&#39;) # if (!require(&#39;grateful&#39;)) remotes::install_github(&quot;Pakillo/grateful&quot;); library(&#39;grateful&#39;) if (!require(&#39;papaja&#39;)) remotes::install_github(&quot;crsh/papaja&quot;); library(&#39;papaja&#39;) if (!require(&#39;report&#39;)) remotes::install_github(&quot;easystats/report&quot;); library(&#39;report&#39;) Dependencias Vamos a necesitar Git y Latex para poder trabajar: Instalar Git Ver instrucciones para Windows, Mac y Linux.   Importante: en el paso Adjusting your PATH environment en en Windows, selecciona Git from the command line and also from 3rd-party software Instalar latex: tinytex::install_tinytex() # Llevará un rato 4.1 Que es la reproducibilidad La crisis de replicación (replication crisis) se inició con un paper que trató de replicar los resultados de 100 investigaciones clásicas. Esta crisis ha generado un movimiento muy interesante dentro de las Ciencias Sociales y la Psicología en particular. Cada vez es más común aplicar algunos principios de buenas prácticas como compartir materiales, datos y scripts de análisis, para que tanto los revisores como otros investigadores puedan entender, reanalizar, etc. nuestras investigaciones. Hay algunas organizaciones que han surgido para tratar de mejorar la colaboración, transparencia, y manera de trabajar, como el Psychological Science Accelerator, la Peer Reviewer’s Openness Initiative (PRO), o la Open Science Foundation. Una de las soluciones propuestas para resolver muchos de los problemas actuales pasa por los Registered reports. En estos se da una restructured submission timeline: Before collecting data, authors submit a study protocol containing their hypotheses, planned methods, and analysis pipeline, which undergoes peer review. Además de los motivos científicos para trabajar de manera más transparente y reproducible, hay también motivos prácticos. Si trabajamos de manera reproducible, las modificaciones en tablas, gráficas, número de participantes o reanálisis son triviales. En este capítulo vamos a ver algunos pasos fundamentales para tender un workflow que permita y ayude a la reproducibilidad. 4.2 Proyectos de R-Studio El primer paso empieza por crear un proyecto de RStudio. Al usar proyectos, simplificamos varias cosas, haciendo automáticamente más fácil compartir nuestro trabajo con otras personas. Podéis leer algo más sobre esto aquí. 4.3 Control de cambios con Git y Github SOURCE: https://xkcd.com/1597/ 4.3.1 Git Un segundo elemento que nos va a ayudar a trabajar en equipo, y a evitar problemas en proyectos relativamente complejos es el uso de un sistema de control de versiones como Git. Los proyectos de RStudio hacen especialmente sencillo usar algunas funcionalidades básicas de Git. Algunas referencias útiles: OhshitGit website Git in practice happygitwithr 4.3.2 Github SOURCE: github.githubassets.com Github es una plataforma web muy popular donde almacenar proyectos de programación que usa como motor. Muchos de los paquetes de R, el mismo RStudio, etc, tienen repositorios abiertos en Github. Una de las ventajas fundamentales de usar Github es que esta plataforma integra algunas herramientas para hacer más sencillo el control de versiones, como el pull request, que nos permite combinar ramas de proyectos sin apenas problemas. Github tiene un programa especial para estudiantes: https://education.github.com/ 4.3.3 Clonar un repositorio existente Algo que podemos hacer con todos los repositorios de Github es clonarlos localmente: Primero, copiamos la repository URL del repo de Github (ver imagen de abajo). Será algo similar a https://github.com/VUESTRO_NOMBRE_DE_USUARIO/NOMBRE_REPOSITORIO.git Segundo, en RStudio: File &gt; New Project &gt; Version Control &gt; Git 4.3.4 Crear un proyecto en RStudio asociado a Github Podemos empezar creando un repositorio en Github, para despues clonarlo localmente. Versión simple [recomendado] En Github: Creamos repositorio nuevo Initialize this repository with a README Clonar repositorio Alternativamente, si ya tenemos un proyecto de RStudio, podemos crear un repositorio de Github asociado automágicamente. Usando el terminal Crear local git repo (solo si no lo tenemos aún): usethis::use_git() (se crea una carpeta oculta llamada .git) Crear Github Token: usethis::browse_github_token() Insertar token en archivo .Renviron: usethis::edit_r_environ() Crear Github repo: usethis::use_github() Empujar el repositorio local a Github: git push --set-upstream origin master Ejercicio Git-Github Crea un proyecto de RStudio Abre una cuenta en Github y/o haz login Sigue los pasos de arriba para crear un repositorio público y asociarlo a un repositorio local 4.4 Workflow SOURCE: nvie.com Hay diferentes filosofias sobre cual es la mejor manera de trabajar con Git. En este post por Vincent Driessen podeis ver una explicación bien detallada, complementada con imagenes como la que se ve a continuación. El modelo básico implica la existencia de dos ramas. Una master (“producción”), que siempre debe funcionar, y una develop (para desarrollo), donde experimentamos, rompemos cosas, etc. Podeis ver un manual super completo llamado Happy Git and GitHub for the useR elaborado por Jenny Bryan, Jim Hester, entre otros. 4.4.1 Modelo básico En RStudio podemos trabajar gráficamente, Usando el panel Git. Usando el entorno gráfico Empezamos en la rama master: Pull : nos aseguramos que nuestro repositorio local esta actualizado Branch : Creamos nueva rama llamada development Hacemos cambios en nuestros scripts Commit : Commiteamos los cambios Push : subimos la rama a Github Pull request (En Github): Compare &amp; Pull request Pull : nos aseguramos que nuestro repositorio local esta actualizado Como hacerlo usando el terminal Pull: nos aseguramos que nuestro repositorio local esta actualizado: git pull Branch: Creamos nueva rama llamada development: git checkout -b development Hacemos cambios en nuestros scripts Commit: Commiteamos los cambios Añadimos archivos: git add foo.txt Hacemos el commit: git commit --message \"A commit message\" Push: subimos la rama a Github: git push origin development Pull request (En Github): Compare &amp; Pull request Pull: nos aseguramos que nuestro repositorio local esta actualizado: git pull 4.4.2 Pull request en 3 + 1 sencillos pasos Después de hacer el push de arriba, al entrar en nuestro repositorio deberíamos ver algo parecido a lo siguiente (si no lo vemos, ir a branches). La única dificultad es saber cual de los botones verdes apretar: Paso 1. Compare &amp; pull request Paso 2. Create pull request Paso 3. Merge pull request Borrar rama antigua Ejercicio Nuestro primer commit Usando el proyecto de RStudio de antes, crea una rama nueva llamada development Crea un nuevo archivo en formato .Rmd: Haz un commit de ese archivo y subelo (push) a Github (asegurate que esta allá!). No olvides hacer un pull! Ahora haz cambios en el archivo, commitealos, súbelos, y sincroniza tu repo local 4.5 RMarkdown, openscience y análisis reproducibles RMarkdown es un tipo de archivo que nos permite combinar texto formateado con código y resultados en un mismo documento (HTML, PDF, WORD…). Aprovechando la potencia de este tipo de archivo, algunas personas han creado paquetes para preparar artículos en formato APA, o con las plantillas de decenas de editoriales. 4.6 Sintaxis, chunks de código, tipos de archivo La sintáxis básica de RMarkdown es sorprendentemente sencilla, como se puede ver más abajo. Eso si, lo que hay detrás es toda la potencia de latex, así que el cielo es el límite. Y como no, tenemos mucha ayuda: R Markdown cheatsheet R Markdown: The Definitive Guide Web oficial de Rmarkdown dentro de RStudio Resumiendo, tienes tres elementos básicos: 4.6.1 Cabecera YAML Cuando creas un documento .Rmd nuevo verás algo similar a lo siguiente en las primeras lineas: --- title: &quot;Untitled&quot; author: &quot;G&quot; date: &quot;6/1/2019&quot; output: pdf_document --- Esta es la cabecera YAML, en la cual se le pueden pasar parámetros para añadir un índice, cambiar formato, y muchas otras cosas. 4.6.2 Rmarkdown En el resto del documento (con la excepción de los chunks de código), el formato que usaremos será Rmarkdown. Su sintaxis es muy sencilla pero nada tolerante. Podéis ver las bases en la R Markdown cheatsheet. IMPORTANTE. Si algo no funciona como esperas: Añade saltos de linea entre párrafos. Añade dos espacios al final de las líneas. Añade un espacio después de #: MAL: #Título grande BIEN: # Título grande 4.6.3 Chunks de código Los chunks de código están delimitados por: En su interior, puedes usar código R como si estuvieras en un script de R normal. En la cabecera puedes añadir opciones. Hay una cantidad apabullante de opciones. Por ejemplo, en el siguiente chunk: {r nombre_chunk, eval=TRUE, include=TRUE, fig.height=10, fig.width=12, message=FALSE, warning=FALSE, cache=TRUE, results='asis'} eval=TRUE: Muestra el código include=TRUE: Corre el código fig.height=10: altura de los plots (en inches) fig.width=12: ancho de los plots (en inches) message=FALSE: NO muestres mensajes warning=FALSE: NO muestres warnings cache=TRUE: cachea el output del plot results='asis': muestra el output tal cual (importante cuando el output es en latex/pdf) Haciendo click en la herramienta de la derecha del chunk puedes controlar varios parámetros esenciales. TRUCO: Si tienes un chunk al principio llamado setup, cada vez que reinicies RStudio y ejecutes código en cualquier parte de tu documento, ese bloque se ejecutara automaticamente. Esto es ideal para poner tus librerias, lectura de datos… Ejercicio básico RMarkdown Volvamos al archivo .Rmd que creamos antes. Hagamos lo siguiente: Dale formato de artículo científico, creando las siguientes secciones: Title Abstract Introducción Materials and Methods Participants Materials Results Experiment 1 Experiment 2 Discussion Bibliography Pon texto de relleno dentro de cada sección. Para ello puedes usar la función stringi::stri_rand_lipsum() del paquete {stringi}. Renderiza tu documento en formato PDF. Pull, Commit, Push, Pull… 4.6.4 Herramientas básicas para investigadoras/es De manera relativamente sencilla podemos incluir tablas bonitas en los reportes. 4.6.4.1 Descriptivos gtsummary::tbl_summary(gapminder %&gt;% select(-country), by = continent, missing = &quot;ifany&quot;) %&gt;% gtsummary::add_n() html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #mnfuvswbia .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #mnfuvswbia .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #mnfuvswbia .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #mnfuvswbia .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #mnfuvswbia .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #mnfuvswbia .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #mnfuvswbia .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #mnfuvswbia .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #mnfuvswbia .gt_column_spanner_outer:first-child { padding-left: 0; } #mnfuvswbia .gt_column_spanner_outer:last-child { padding-right: 0; } #mnfuvswbia .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #mnfuvswbia .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #mnfuvswbia .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #mnfuvswbia .gt_from_md > :first-child { margin-top: 0; } #mnfuvswbia .gt_from_md > :last-child { margin-bottom: 0; } #mnfuvswbia .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #mnfuvswbia .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #mnfuvswbia .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #mnfuvswbia .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #mnfuvswbia .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #mnfuvswbia .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #mnfuvswbia .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #mnfuvswbia .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #mnfuvswbia .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #mnfuvswbia .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #mnfuvswbia .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #mnfuvswbia .gt_sourcenote { font-size: 90%; padding: 4px; } #mnfuvswbia .gt_left { text-align: left; } #mnfuvswbia .gt_center { text-align: center; } #mnfuvswbia .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #mnfuvswbia .gt_font_normal { font-weight: normal; } #mnfuvswbia .gt_font_bold { font-weight: bold; } #mnfuvswbia .gt_font_italic { font-style: italic; } #mnfuvswbia .gt_super { font-size: 65%; } #mnfuvswbia .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 65%; } Characteristic N Africa, N = 6241 Americas, N = 3001 Asia, N = 3961 Europe, N = 3601 Oceania, N = 241 year 1,704 1,980 (1,966, 1,993) 1,980 (1,966, 1,993) 1,980 (1,966, 1,993) 1,980 (1,966, 1,993) 1,980 (1,966, 1,993) lifeExp 1,704 48 (42, 54) 67 (58, 72) 62 (51, 70) 72 (70, 75) 74 (71, 78) pop 1,704 4,579,311 (1,342,075, 10,801,490) 6,227,510 (2,962,359, 18,340,309) 14,530,830 (3,844,393, 46,300,348) 8,551,125 (4,331,500, 21,802,867) 6,403,492 (3,199,212, 14,351,625) gdpPercap 1,704 1,192 (761, 2,377) 5,466 (3,428, 7,830) 2,647 (1,057, 8,549) 12,082 (7,213, 20,461) 17,983 (14,142, 22,214) 1 Median (IQR) 4.6.4.2 Inferenciales Que test estadístico debería usar, con código en R 4.6.4.2.1 Test de correlación corr_test = cor.test(iris$Sepal.Width, iris$Sepal.Length, method = &quot;spearman&quot;) table_easystats = report::report(corr_test); table_easystats ## Effect sizes were labelled following Funder&#39;s (2019) recommendations. ## ## The Spearman&#39;s rank correlation rho between iris$Sepal.Width and iris$Sepal.Length is negative, statistically significant, and small (rho = -0.17, S = 6.56e+05, p &lt; .05) # table_easystats %&gt;% parameters::print_md() knitr::kable(table_easystats, align = &quot;c&quot;) Parameter1 Parameter2 rho S p Method Alternative iris\\(Sepal.Width | iris\\)Sepal.Length -0.17 6.56e+05 0.041 Spearman’s rank correlation rho two.sided iris %&gt;% correlation(partial = TRUE) %&gt;% plot() 4.6.4.2.2 Tabla de correlaciones table_correlations &lt;- iris %&gt;% correlation::correlation() TABLE_CORR = table_correlations %&gt;% summary(stars = FALSE, include_significance = TRUE, p_digits = 3) %&gt;% parameters::print_md() TABLE_CORR Correlation Matrix (pearson-method) Parameter Petal.Width Petal.Length Sepal.Width Sepal.Length 0.82 (p &lt; .001) 0.87 (p &lt; .001) -0.12 (p = 0.152) Sepal.Width -0.37 (p &lt; .001) -0.43 (p &lt; .001) Petal.Length 0.96 (p &lt; .001) p-value adjustment method: Holm (1979) gsub(&quot;p = |p &quot;, &quot;&quot;, TABLE_CORR) Correlation Matrix (pearson-method) Parameter Petal.Width Petal.Length Sepal.Width Sepal.Length 0.82 (&lt; .001) 0.87 (&lt; .001) -0.12 (0.152) Sepal.Width -0.37 (&lt; .001) -0.43 (&lt; .001) Petal.Length 0.96 (&lt; .001) p-value adjustment method: Holm (1979) gsub(&quot;\\\\(p &quot;, &quot;&lt;BR&gt;(p&quot;, TABLE_CORR) Correlation Matrix (pearson-method) Parameter Petal.Width Petal.Length Sepal.Width Sepal.Length 0.82 (p&lt; .001) 0.87 (p&lt; .001) -0.12 (p= 0.152) Sepal.Width -0.37 (p&lt; .001) -0.43 (p&lt; .001) Petal.Length 0.96 (p&lt; .001) p-value adjustment method: Holm (1979) 4.6.4.3 LM Usaremos un análisis de regresión sencillo tratando de predecir la longitud de los sépalos a partir de los efectos principales y la interacción entre la longitud y ancho de los pétalos. model_lm &lt;- lm(Sepal.Length ~ Petal.Length * Petal.Width, data = iris) #summary(model_lm) report::report(model_lm) ## We fitted a linear model (estimated using OLS) to predict Sepal.Length with Petal.Length and Petal.Width (formula: Sepal.Length ~ Petal.Length * Petal.Width). The model explains a statistically significant and substantial proportion of variance (R2 = 0.81, F(3, 146) = 204.54, p &lt; .001, adj. R2 = 0.80). The model&#39;s intercept, corresponding to Petal.Length = 0 and Petal.Width = 0, is at 4.58 (95% CI [4.36, 4.80], t(146) = 40.89, p &lt; .001). Within this model: ## ## - The effect of Petal.Length is statistically significant and positive (beta = 0.44, 95% CI [0.31, 0.57], t(146) = 6.74, p &lt; .001; Std. beta = 1.42, 95% CI [1.14, 1.71]) ## - The effect of Petal.Width is statistically significant and negative (beta = -1.24, 95% CI [-1.67, -0.81], t(146) = -5.65, p &lt; .001; Std. beta = -0.49, 95% CI [-0.76, -0.21]) ## - The interaction effect of Petal.Width on Petal.Length is statistically significant and positive (beta = 0.19, 95% CI [0.12, 0.25], t(146) = 5.62, p &lt; .001; Std. beta = 0.31, 95% CI [0.20, 0.41]) ## ## Standardized parameters were obtained by fitting the model on a standardized version of the dataset. parameters::model_parameters(model_lm) %&gt;% parameters::print_md() Parameter Coefficient SE 95% CI t(146) p (Intercept) 4.58 0.11 (4.36, 4.80) 40.89 &lt; .001 Petal.Length 0.44 0.07 (0.31, 0.57) 6.74 &lt; .001 Petal.Width -1.24 0.22 (-1.67, -0.81) -5.65 &lt; .001 Petal.Length * Petal.Width 0.19 0.03 (0.12, 0.25) 5.62 &lt; .001 Hay varias maneras de mostrar tablas en Rmarkdown. Arriba usamos el paquete {parameters}, pero también se puede hacer con {stargazer}, papaja, kable, etc. sjPlot es especialmente potente. El único detalle es que no genera outputs en pdf. Para usar las tablas generadas por sjPlot en pdf’s ver el paquete {html2latex} sjPlot::tab_model(model_lm)   Sepal Length Predictors Estimates CI p (Intercept) 4.58 4.36 – 4.80 &lt;0.001 Petal.Length 0.44 0.31 – 0.57 &lt;0.001 Petal.Width -1.24 -1.67 – -0.81 &lt;0.001 Petal.Length *Petal.Width 0.19 0.12 – 0.25 &lt;0.001 Observations 150 R2 / R2 adjusted 0.808 / 0.804 4.6.4.4 Anova Ver paquete {afex} data(obk.long, package = &quot;afex&quot;) head(obk.long) ## id treatment gender age phase hour value ## 1 1 control M -4.75 pre 1 1 ## 2 1 control M -4.75 pre 2 2 ## 3 1 control M -4.75 pre 3 4 ## 4 1 control M -4.75 pre 4 2 ## 5 1 control M -4.75 pre 5 1 ## 6 1 control M -4.75 post 1 3 # estimate mixed ANOVA on the full design: model = afex::aov_ez(id = &quot;id&quot;, dv = &quot;value&quot;, data = obk.long, between = c(&quot;treatment&quot;), within = c(&quot;phase&quot;, &quot;hour&quot;)) table_afex = papaja::apa_print(model)$table knitr::kable(table_afex) Effect F df1 df2 MSE p ges 1 Treatment 2.91 2 13 32.04 .090 .211 2 Phase 19.29 1.74 22.64 4.07 &lt; .001 .164 4 Hour 18.44 1.95 25.41 2.87 &lt; .001 .129 3 Treatment \\(\\times\\) Phase 5.43 3.48 22.64 4.07 .004 .099 5 Treatment \\(\\times\\) Hour 0.08 3.91 25.41 2.87 .987 .001 6 Phase \\(\\times\\) Hour 1.35 4.02 52.29 2.24 .265 .017 7 Treatment \\(\\times\\) Phase \\(\\times\\) Hour 0.33 8.05 52.29 2.24 .951 .008 Ejercicio avanzado Bajad la base de Cancer Screening Risk Literacy of Physicians in Training: https://osf.io/qn9a2/ y el preprint del artículo: En el documento .Rmd de antes: Cread algo parecido a la tabla de correlaciones (Tabla 3) que se ve en el artículo. Tratad de emular el tipo de análisis que se ve en la Tabla 4. 4.7 De los datos al reporte final: Una historia de amor con R Preparar artículos en formato APA remotes::install_github(&quot;crsh/papaja&quot;) # Create new R Markdown file rmarkdown::draft( here::here(&quot;data&quot;, &quot;output&quot;, &quot;mymanuscript.Rmd&quot;), &quot;apa6&quot;, package = &quot;papaja&quot;, create_dir = FALSE, edit = FALSE) # Render manuscript rmarkdown::render( here::here(&quot;data&quot;, &quot;output&quot;, &quot;mymanuscript.Rmd&quot;), quiet = TRUE, clean = TRUE) Y no olvidemos el paquete {rticles}, que contiene plantillas de decenas de editoriales 4.8 Avanzado Para evitar problemas con los paths de los archivos, usar here::here() Para evitar problemas con instalación de Latex: tinytex::install_tinytex() Corregir ortografía en Rmarkdown (F7) https://github.com/ropensci/spelling#readme 4.8.1 Usar bibliografía Bibliografía en Rmarkdown https://blog.rstudio.com/2020/11/09/rstudio-1-4-preview-citations/ https://rmarkdown.rstudio.com/authoring_bibliographies_and_citations.html https://www.r-bloggers.com/bibliography-with-knitr-cite-your-references-and-packages/ 4.8.2 Citar los paquetes que usamos ¿Debemos citar los paquetes que usamos? Respuesta corta, si Respuesta larga, la mayoría de los paquetes # grateful::cite_packages(all.pkg = FALSE, # out.format = &quot;rmd&quot;, # out.dir = &quot;dev&quot;) 4.8.3 Manejo de dependencias Usando un sistema de manejo de dependencias renv creamos un snapshot de las librerías usadas actualmente. Es muy importante para garantizar que nuestros scripts correran en el futuro. Instalamos renv: install.packages(\"remotes\") remotes::install_github(\"rstudio/renv\") Inicializamos el entorno local de un nuevo proyecto, con una librería privada de R renv::init() Trabajamos en el proyecto, instalando los paquetes que necesitemos Guardamos el estado de las librerías usadas en el proyecto en un lockfile (llamado renv.lock), renv::snapshot() Restauramos el estado de las librerías a partir del lockfile generado por renv::snapshot(). renv::restore() 4.9 Mas allá de Rmarkdown Aplicaciones web interactivas con R: Shiny Bayesian reasoning 4.10 Varios 4.10.1 Shortcuts! Alt+SHIFT+K: Ver shortcuts! CTRL+SHIFT+M: Pipe CTRL+SHIFT+A: Reformat code CTRL+I: Reindent lines 4.10.2 Estilo Es recomendable ser consistente en la manera de escribir código. Habitualmente se recomienda seguir una guía de estilo. Por ejemplo, Hadley Wickham’s Style guide o la guia de estilo del tidyverse. 4.10.3 Algunos paquetes interesantes Descargar datos suplementarios de papers publicados usando DOI https://github.com/easystats/easystats https://usethis.r-lib.org/ https://github.com/karthik/holepunch Bibliografía Guia de estilo del tidyverse Hadley Wickham’s Style guide Happy Git and GitHub for the useR targets Scheel, A. M., Schijen, M., &amp; Lakens, D. (in press). An excess of positive results: Comparing the standard Psychology literature with Registered Reports. Advances in Methods and Practices in Psychological Science. Xie, Y., Allaire, J. J., &amp; Grolemund, G. (2018). R Markdown: The Definitive Guide. CRC Press. https://bookdown.org/yihui/rmarkdown/ Yihui Xie (2018). bookdown: Authoring Books and Technical Documents with R Markdown https://bookdown.org/yihui/bookdown/markdown-syntax.html Mas cosas sobre reproducibilidad: Reproducibility project: Psychology Many labs 2 "],["reporte-final.html", "Capítulo 5 Reporte final 5.1 Base de datos a usar 5.2 Paso a paso 5.3 Hint 5.4 Nota final", " Capítulo 5 Reporte final El objetivo evaluable de este workshop es escribir CONJUNTAMENTE un mini-paper (tan sólo título, abstract, método y resultados) en inglés, en formato APA usando {papaja} o algún formato de {rticles}. 5.1 Base de datos a usar SOURCE: https://twitter.com/richarddmorey/status/690680901760606209 Usaremos la base de datos https://osf.io/qn9a2/ asociada al paper Cancer screening risk literacy of physicians in training: An experimental study de Petrova et al. En ese paper podréis ver que hay 1 experimento. Vuestra tarea será crear conjuntamente un paper en Rmarkdown (PDF) donde hagáis un reanálisis de los datos de ese paper. Cada uno de vosotros elegirá un subconjunto de variables y un análisis, e incluirá un “Experimento” dentro del paper conjunto, mostrando y describiendo sus resultados. 5.2 Paso a paso Tendréis que seguir los siguientes pasos para completar el trabajo: Paso 1: Crear un Repositorio en Github Paso 2: Clonar el repositorio localmente en un computador Paso 3: Crear una rama development y trabajar conjuntamente en la preparación de datos (importar, renombrar, seleccionar variables…) Paso 4: Usando {papaja}, o alguna de las plantillas de {rticles} cread juntos (en un solo computador) un documento Rmd e incluid la estructura (secciones) del paper Title Abstract: describir brevemente que se hace Introduction Materials and Methods Participants Materials Results Experiment 1 Experiment 2 Experiment 3 Discussion Paso 5: Cada uno clonara el repositorio localmente en su computador, creará su rama propia y, en la sección adecuada (e.g. Experiment 2), completará las tareas de abajo (commit y push cambios a la rama propia de Github al finalizar). NO borrar la rama propia de Github: Describir el análisis realizado Tabla APA con descriptivos de las variables seleccionadas Descripción de resultados en formato APA Tabla de resultados APA Plot APA que represente adecuadamente estos resultados Paso 6: Combinar los cambios de las 3 ramas individuales en development Paso 7: Cada uno debería revisar el paper final, corregir fallos y combinarlos en la rama development Paso 8: Mover los cambios a master y celebrar! 5.3 Hint En la carpeta Data and results de https://osf.io/qn9a2/ hay varios archivos que os ayudarán a entender cuales son las variables de interés. Analysis script R1.sps: script de SPSS usado para los análisis del paper Variables R1.sps: descripción de variables … Es muy recomendable ubicar las variables de interés, y renombrarlas para que sean fácilmente reconocibles. 5.4 Nota final La nota final se definirá de la siguiente manera: Paper final: 60% Tarea individual: 40% El formato, el estilo, y los acabados tienen que ser con el estándar de calidad esperado en un paper científico. Se evaluará específicamente lo siguiente: Paper final (60% total): (90%) El paper tiene que ser reproducible: El profesor descargará el repositorio completo en un computador sin ninguna librería instalada y kniteara el archivo .Rmd del paper. La expectativa es que todo funcione, y que el paper que se cree automáticamente sea idéntico al entregado por los alumnos. (10%) Calidad general de la redacción, ausencia de errores gramaticales y ortográficos graves. Tarea individual (40% total): (20%) Describir el análisis realizado (20%) Tabla APA con descriptivos de las variables seleccionadas (20%) Descripción de resultados en formato APA (20%) Tabla de resultados APA (20%) Plot APA que represente adecuadamente estos resultados La historia completa de commits de todas las ramas (master, development, y las 3 individuales) deberá estar disponible. "],["ejercicios-6.html", "Capítulo 6 Ejercicios 6.1 Ejercicio 1 6.2 Ejercicio 2", " Capítulo 6 Ejercicios 6.1 Ejercicio 1 Crea un script en el que hagas lo siguiente: Cargar librerias necesarias Importar datos Preparar datos Crear gráfica Crear tabla Tendras que mostrar como corre el script con una sesion limpia de RStudio (CONTROL + SHIFT + F10) Notas (importar datos): Los datos deberían ser, idealmente, de un proyecto en el que estés trabajando. Si no tienes datos disponibles, puedes encontrar bases de datos abiertas de varias maneras: Tidytuesday: https://github.com/rfordatascience/tidytuesday Buscando en google scholar papers que comparten sus datos: https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=data+available+%22osf.io%22+psychology&amp;btnG= Buscando directamente en Open Science Foundation: https://osf.io/ (preparación de datos): Algunos ejemplos de preparación de datos: renombrar variables, filtrar, computar nuevas variables, combinar bases… (gráfica): Algunos ejemplos de gráficas: distribución de variable/s, relación entre variables, … (tabla): Algunos ejemplos de tablas: promedios por condición, media, mediana, max y min para algunas variables claves… 6.2 Ejercicio 2 Diseña un sencillo experimento: Debes usar algunas de las escalas/tareas que aparecen en: https://github.com/gorkang/jsPsychMaker (ver carpeta canonical_protocol, archivo canonical_protocol_details.csv) Además, debes diseñar una nueva tarea muy sencilla o adaptar una nueva escala La duración total del “experimento” no deberia superar los 5 minutos Tendrás que hacer una breve presentación contándonos el diseño experimental. Notas (tareas jsPsychMaker): Usa un máximo de 2 tareas (tarea nueva): Puedes diseñar una tarea con una sola pregunta para dos condiciones experimentales, o adaptar un cuestionario breve ya existente (que no tengamos en jsPsychMaker). "],["paquetes-usados.html", "Paquetes usados References", " Paquetes usados En la documentación y ejercicios de este workshop se usaron los paquetes que se pueden ver abajo. Este listado se creó automáticamente usando {grateful}: base (R Core Team 2019) knitr (Xie 2014) stargazer (Hlavac 2018) rticles (Allaire et al. 2019) renv (Ushey 2019) remotes (Hester et al. 2019) papaja (Aust and Barth 2018) easystats (Lüdecke and Makowski 2019) estimate (Makowski and Lüdecke 2019b) see (Lüdecke et al. 2019) report (Makowski et al. 2019) correlation (Makowski 2019) parameters (Makowski and Lüdecke 2019a) bayestestR (Makowski, Ben-Shachar, and Lüdecke 2019) performance (Lüdecke, Makowski, and Waggoner 2019) insight (Lüdecke, Waggoner, and Makowski 2019) corrr (Jackson, Cimentada, and Ruiz 2019) afex (Singmann et al. 2019) lme4 (Bates et al. 2015) Matrix (Bates and Maechler 2019) hexbin (Carr et al. 2019) FFTrees (Phillips et al. 2018) caret (Jed Wing et al. 2019) lattice (Sarkar 2008) janitor (Firke 2019) gsheet (Conway 2016) DT (Xie, Cheng, and Tan 2019) writexl (Ooms 2018) readODS (Schutten et al. 2018) here (Müller 2017) haven (Wickham and Miller 2019) readxl (Wickham and Bryan 2019) plotly (Sievert 2018) ggridges (Wilke 2018) ggthemes (Arnold 2019) gganimate (Pedersen and Robinson 2019) gapminder (Bryan 2017) esquisse (Meyer and Perrier 2019) cowplot (Wilke 2019) forcats (Wickham 2019a) stringr (Wickham 2019b) dplyr (Wickham et al. 2019) purrr (Henry and Wickham 2019) readr (Wickham, Hester, and Francois 2018) tidyr (Wickham and Henry 2019) tibble (Müller and Wickham 2019) ggplot2 (Wickham 2016) tidyverse (Wickham 2017) References "]]
